
每周分类神经辐射场 - fast ![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)
=================================================================================================================================
## 按类别筛选: 
 [全部](../weekly_nerf_cn.md) | [动态](./dynamic.md) | [编辑](./editing.md) | [快速](./fast.md) | [泛化](./generalization.md) | [人体](./human.md) | [视频](./video.md) | [光照](./lighting.md) | [重建](./reconstruction.md) | [纹理](./texture.md) | [语义](./semantic.md) | [姿态-SLAM](./pose-slam.md) | [其他](./others.md) 
## Dec27 - Jan3, 2023
## Dec25 - Dec31, 2022
## Dec18 - Dec24, 2022
## Dec11 - Dec17, 2022
## Dec4 - Dec10, 2022
  - [GARF：几何感知广义神经辐射场](https://arxiv.org/abs/2212.02280) | [code]
    > 神经辐射场 (NeRF) 彻底改变了自由视点渲染任务，并取得了令人瞩目的成果。 然而，效率和准确性问题阻碍了其广泛应用。 为了解决这些问题，我们提出了几何感知广义神经辐射场 (GARF) 和几何感知动态采样 (GADS) 策略，以在不进行逐场景优化的情况下对未见场景执行实时新颖视图渲染和无监督深度估计。 与大多数现有的广义 NeRF 不同，我们的框架仅使用少量输入图像就可以在像素尺度和几何尺度上推断出看不见的场景。 更具体地说，我们的方法通过编码器-解码器结构和有助于避免遮挡的点级可学习多视图特征融合模块来学习新视图合成的共同属性。 为了在广义模型中保留场景特征，我们引入了一个无监督深度估计模块来推导粗几何，将光线采样间隔缩小到估计表面的邻近空间，并在期望最大位置采样，构成几何感知动态采样策略（ GADS）。 此外，我们引入了多级语义一致性损失 (MSC) 来帮助提供更多信息的表示学习。 对室内和室外数据集的大量实验表明，与最先进的广义 NeRF 方法相比，GARF 将样本减少了 25% 以上，同时提高了渲染质量和 3D 几何估计。
## Nov27 - Dec3, 2022
  - [QFF：神经场表示的量化傅立叶特征](https://arxiv.org/abs/2212.00914) | [code]
    > 多层感知器 (MLP) 学习高频的速度很慢。 最近的方法对空间箱中的特征进行编码以提高学习细节的速度，但是以更大的模型尺寸和连续性损失为代价。 相反，我们建议在通常用于位置编码的傅里叶特征的容器中对特征进行编码。 我们称这些为量化傅立叶特征 (QFF)。 作为一种自然的多分辨率和周期性表示，我们的实验表明，使用 QFF 可以为多种应用带来更小的模型尺寸、更快的训练和更高质量的输出，包括神经图像表示 (NIR)、神经辐射场 (NeRF) 和符号距离函数 (SDF) 建模。 QFF 易于编码，计算速度快，并且可以作为许多神经场表示之外的简单补充。
  - [用于快速多视图视频合成的混合神经体素](https://arxiv.org/abs/2212.00190) | [code]
    > 由于现实世界环境的复杂性和高度动态的运动，从现实世界的多视图输入合成高保真视频具有挑战性。 以前基于神经辐射场的作品已经展示了动态场景的高质量重建。 但是，在真实场景中训练此类模型非常耗时，通常需要数天或数周。 在本文中，我们提出了一种名为 MixVoxels 的新方法，以更好地表示具有快速训练速度和有竞争力的渲染质量的动态场景。 拟议的 MixVoxels 将 4D 动态场景表示为静态和动态体素的混合，并使用不同的网络对其进行处理。 这样，静态体素所需模态的计算可以由轻量级模型处理，这从本质上减少了计算量，特别是对于许多以静态背景为主的日常动态场景。 为了分离这两种体素，我们提出了一个新的变化场来估计每个体素的时间方差。 对于动态体素，我们设计了一种内积时间查询方法来有效地查询多个时间步长，这对于恢复高动态运动至关重要。 因此，通过对输入 300 帧视频的动态场景进行 15 分钟的训练，MixVoxels 实现了比以前的方法更好的 PSNR。 此 https 网址提供代码和训练模型
  - [使用 RGBXY 导数和最佳传输的可微分渲染, ToG2022](https://dl.acm.org/doi/abs/10.1145/3550454.3555479) | [code]
    > 传统的可微分渲染方法通常很难在逆渲染优化中收敛，尤其是当初始对象和目标对象位置不太接近时。 受拉格朗日流体模拟的启发，我们提出了一种新颖的可微分渲染方法来解决这个问题。 我们将每个屏幕空间像素与像素中心覆盖的可见 3D 几何点相关联，并计算几何点而不是像素的导数。 我们将关联的几何点称为像素的点代理。 对于每个点代理，我们计算其 5D RGBXY 导数，测量其 3D RGB 颜色和 2D 投影屏幕空间位置如何相对于场景参数发生变化。 此外，为了捕获全局和远程对象运动，我们利用基于最佳传输的像素匹配来设计更复杂的损失函数。 我们已经进行了实验来评估我们提出的方法在各种逆向渲染应用程序中的有效性，并证明了与最先进的基线相比更优越的收敛行为。
  - [QuadStream：一种用于新视点重建的基于 Quad 的场景流架构, ToG2022](https://dl.acm.org/doi/abs/10.1145/3550454.3555524) | [code]
    > 通过网络将渲染的 3D 内容流式传输到手机或 VR/AR 耳机等瘦客户端设备，将高保真图形带到通常由于热量、功率或成本限制而无法实现的平台。 流式 3D 内容必须以对延迟和潜在网络丢失都具有鲁棒性的表示形式进行传输。 在存在遮挡事件的情况下，传输视频流并重新投影以纠正不断变化的视点失败； 在功率有限的移动 GPU 上无法在客户端流式传输场景几何体和执行高质量渲染。 为了平衡消除遮挡稳健性和最小客户端工作量这两个相互竞争的目标，我们引入了 QuadStream，这是一种新的流媒体内容表示，它通过允许客户端有效地渲染新颖的视图而没有由消除遮挡事件引起的伪影来减少运动到光子的延迟。 受视频编解码器设计的传统宏块方法的启发，我们将从视图单元中的位置看到的场景分解为一系列四边形代理，或来自多个视图的视图对齐四边形。 通过在光栅化 G-Buffer 上操作，我们的方法独立于场景本身的表示； 生成的 QuadStream 是场景的近似几何表示，可以由瘦客户端重建以呈现当前视图和附近的相邻视图。 我们的技术贡献是一种有效的并行四边形生成、合并和打包策略，用于覆盖场景中潜在客户移动的代理视图； 一种打包和编码策略，允许将具有深度信息的掩码四边形作为帧相干流传输； 以及一种高效的渲染方法，用于将我们的 QuadStream 表示渲染为瘦客户端上的全新视图。 我们表明，与视频数据流方法和基于几何的流媒体相比，我们的方法实现了卓越的质量。
  - [用于全频着色的轻量级神经基函数, SIGGRAPH-Asia2022](https://dl.acm.org/doi/abs/10.1145/3550469.3555386) | [code]
    > 基函数既提供了紧凑表示的能力，又提供了高效计算的特性。 因此，它们普遍用于渲染以执行全频着色。 然而，包括球谐函数 (SH)、小波和球面高斯函数 (SG) 在内的常用基函数都有其自身的局限性，例如 SH 的低频、小波的旋转不变性以及 SG 不支持多乘积等。 在本文中，我们提出了神经基函数，这是一组隐式和数据驱动的基函数，它规避了所有所需属性的限制。 我们首先引入了一个表示神经网络，它将任何一般的 2D 球面函数（例如环境光照、BRDF 和可见性）作为输入并将其投影到潜在空间上作为我们的神经基函数的系数。 然后，我们设计了几个执行不同类型计算的轻量级神经网络，为我们的基函数提供了不同的计算属性，例如双/三乘积积分和旋转。 我们通过将神经基函数集成到全频着色应用程序中来展示我们的神经基函数的实用性，表明我们的方法不仅在同等质量下实现了比小波高 10 × -40 × 的压缩率，而且还渲染了全频 实时照明效果，没有上述经典基础功能的限制。
## Nov20 - Nov26, 2022
  - [ScanNeRF：神经辐射场的可扩展基准, WACV2023](https://arxiv.org/abs/2211.13762) | [code]
    > 在本文中，我们提出了第一个用于评估神经辐射场 (NeRF) 和一般情况下的神经渲染 (NR) 框架的真实基准思想。我们设计并实施了一个有效的管道，可以毫不费力地扫描大量真实物体。我们的扫描站的硬件预算不到 500 美元，仅需 5 分钟即可收集大约 4000 张扫描对象的图像。这样的平台用于构建 ScanNeRF，这是一个以多个训练/验证/测试拆分为特征的数据集，旨在对现代 NeRF 方法在不同条件下的性能进行基准测试。因此，我们评估了三个尖端的 NeRF 变体，以突出它们的优点和缺点。该数据集可在我们的项目页面上找到，还有一个在线基准，以促进开发越来越好的 NeRF。
  - [沉浸式神经图形基元](https://arxiv.org/abs/2211.13494) | [code]
    > 神经辐射场 (NeRF)，特别是它通过即时神经图形基元的扩展，是一种用于视图合成的新型渲染方法，它使用真实世界的图像来构建照片般逼真的沉浸式虚拟场景。尽管有潜力，但关于 NeRF 和虚拟现实 (VR) 结合的研究仍然很少。目前，还没有集成到可用的典型 VR 系统中，并且尚未评估 NeRF 实现的 VR 性能和适用性，例如，针对不同的场景复杂性或屏幕分辨率。在本文中，我们提出并评估了一个基于 NeRF 的框架，该框架能够在沉浸式 VR 中渲染场景，允许用户自由移动头部来探索复杂的现实世界场景。我们通过对三个不同的 NeRF 场景进行基准测试来评估我们的框架，这些场景涉及它们在不同场景复杂性和分辨率下的渲染性能。利用超分辨率，我们的方法可以产生每秒 30 帧的帧速率，每只眼睛的分辨率为 1280x720 像素。我们讨论了我们框架的潜在应用，并在线提供了一个开源实现。
  - [通过 Bootstrapped Radiance Field Inversion 从单个图像中获取形状、姿势和外观](https://arxiv.org/abs/2211.11674) | [code]
    > 神经辐射场 (NeRF) 与 GAN 相结合代表了从单一视图进行 3D 重建领域的一个有前途的方向，因为它们能够有效地对任意拓扑进行建模。然而，该领域最近的工作主要集中在已知确切地面真实姿势的合成数据集上，而忽略了姿势估计，这对于某些下游应用程序（例如增强现实 (AR) 和机器人技术）很重要。我们为自然图像引入了一个有原则的端到端重建框架，其中没有准确的地面真实姿势。我们的方法从对象的单个图像中恢复 SDF 参数化的 3D 形状、姿势和外观，而无需在训练期间利用多个视图。更具体地说，我们利用无条件 3D 感知生成器，我们对其应用混合反演方案，在该方案中，模型会产生对解决方案的初步猜测，然后通过优化对其进行细化。我们的框架可以在短短 10 步内对图像进行反渲染，使其能够在实际场景中使用。我们在各种真实和综合基准测试中展示了最先进的结果。
## Nov13 - Nov19, 2022
  - [DINER：无序不变的隐式神经表征](https://arxiv.org/abs/2211.07871) | [code]
    > 隐式神经表示 (INR) 将信号的属性表征为相应坐标的函数，它成为解决逆问题的利器。然而，INR 的容量受到网络训练中频谱偏差的限制。在本文中，我们发现通过重新排列输入信号的坐标可以在很大程度上解决这种与频率相关的问题，为此我们提出了无序不变的隐式神经表示 (DINER)，方法是将哈希表扩充为传统的 INR 骨架。鉴于离散信号共享相同的属性直方图和不同的排列顺序，哈希表可以将坐标投影到相同的分布中，映射信号可以使用后续的 INR 网络更好地建模，从而显着减轻频谱偏差。实验不仅揭示了 DINER 对不同 INR 主干（MLP 与 SIREN）和各种任务（图像/视频表示、相位检索和折射率恢复）的泛化，而且还显示了优于最先进技术的优势算法的质量和速度。
## Nov6 - Nov12, 2022
  - [基于时间相干性的大规模场景分布式光线追踪, ToG2022](https://ieeexplore.ieee.org/abstract/document/9940545) | [code]
    > 分布式光线追踪算法在渲染海量场景时被广泛使用，其中数据利用率和负载均衡是提高性能的关键。一项基本观察是射线在时间上是相干的，这表明时间信息可用于提高计算效率。在本文中，我们使用时间相干性来优化分布式光线追踪的性能。首先，我们提出了一种基于时间一致性的调度算法来指导任务/数据分配和调度。然后，我们提出了一个虚拟门户结构来预测基于前一帧的光线辐射率，并将辐射率低的光线发送到预先计算的简化模型进行进一步追踪，这可以大大降低遍历复杂度和网络数据传输的开销.该方法在大小高达 355 GB 的场景中得到验证。与以前的算法相比，我们的算法可以实现高达 81% 的加速，并且均方误差非常小。
  - [QRF：具有量子辐射场的隐式神经表示](https://arxiv.org/abs/2211.03418) | [code]
    > 现实世界场景的逼真渲染对于包括混合现实 (MR) 和虚拟现实 (VR) 在内的广泛应用来说是一项巨大的挑战。神经网络长期以来一直在求解微分方程的背景下进行研究，之前已被引入作为照片级渲染的隐式表示。然而，使用经典计算的逼真渲染具有挑战性，因为它需要耗时的光线行进，并且由于维数灾难而遭受计算瓶颈。在本文中，我们提出了量子辐射场 (QRF)，它集成了量子电路、量子激活函数和量子体积渲染，用于隐式场景表示。结果表明，QRF不仅发挥了量子计算速度快、收敛快、并行度高等优势，而且保证了体绘制的高质量。
## Oct30 - Nov5, 2022
## Oct23 - Oct29, 2022
  - [NeX360：基于神经基础扩展的实时全方位视图合成, TPAMI2022](https://ieeexplore.ieee.org/abstract/document/9931981) | [code]
    > 我们介绍了 NeX，这是一种基于多平面图像 (MPI) 增强的新颖视图合成的新方法，可以实时再现视图相关的效果。与传统的 MPI 不同，我们的技术将每个像素参数化为从神经网络学习的球形基函数的线性组合，以对视图相关的效果进行建模，并使用混合隐式-显式建模策略来改进精细细节。此外，我们还展示了 NeX 的扩展，它利用知识蒸馏来为无限 360 ∘ 场景训练多个 MPI。我们的方法在几个基准数据集上进行了评估：NeRF-Synthetic 数据集、Light Field 数据集、Real Forward-Facing 数据集、Space 数据集以及 Shiny，我们的新数据集包含更具挑战性的视图相关效果，例如彩虹反射在 CD 上。我们的方法在 PSNR、SSIM 和 LPIPS 上优于其他实时渲染方法，可以实时渲染无界 360 ∘ 场景。
  - [NeRFPlayer：具有分解神经辐射场的可流式动态场景表示](https://arxiv.org/abs/2210.15947) | [code]
    > 在 VR 中自由地在真实世界的 4D 时空空间中进行视觉探索一直是一项长期的追求。当仅使用几个甚至单个 RGB 相机来捕捉动态场景时，这项任务特别有吸引力。为此，我们提出了一个能够快速重建、紧凑建模和流式渲染的高效框架。首先，我们建议根据时间特征分解 4D 时空空间。 4D 空间中的点与属于三个类别的概率相关联：静态区域、变形区域和新区域。每个区域都由一个单独的神经场表示和规范化。其次，我们提出了一种基于混合表示的特征流方案，用于有效地对神经场进行建模。我们的方法，创造了 NeRFPlayer，在单手持相机和多相机阵列捕获的动态场景上进行评估，在质量和速度方面实现与最近最先进的方法相当或更优的渲染性能，实现重建每帧 10 秒，实时渲染。
  - [用于 3D 视频合成的流式辐射场, NeurIPS2022](https://arxiv.org/abs/2210.14831) | [code]
    > 我们提出了一种基于显式网格的方法，用于有效地重建流辐射场，用于真实世界动态场景的新视图合成。我们不是训练一个结合所有帧的单一模型，而是用增量学习范式来制定动态建模问题，其中训练每帧模型差异以补充当前帧上基础模型的适应性。通过利用简单而有效的窄带调整策略，所提出的方法实现了一个可行的框架，用于处理高训练效率的动态视频序列。通过使用基于模型差异的压缩，可以显着减少使用显式网格表示引起的存储开销。我们还引入了一种有效的策略来进一步加速每一帧的模型优化。对具有挑战性的视频序列的实验表明，我们的方法能够以具有竞争力的渲染质量实现每帧 15 秒的训练速度，比最先进的隐式方法实现 1000 倍的加速。此 https 网址提供了代码。
## Oct16 - Oct22, 2022
## Oct9 - Oct15, 2022
  - [基于显着性感知动态路由策略的遥感图像轻量级无级超分辨率](https://arxiv.org/abs/2210.07598) | [***``[code]``***](https://github.com/hanlinwu/SalDRN)
    > 基于深度学习的算法极大地提高了遥感图像（RSI）超分辨率（SR）的性能。然而，增加网络深度和参数会导致计算和存储的巨大负担。直接减少现有模型的深度或宽度会导致性能大幅下降。我们观察到，一个 RSI 中不同区域的 SR 难度差异很大，现有方法使用相同的深度网络处理图像中的所有区域，造成计算资源的浪费。此外，现有的 SR 方法通常预先定义整数尺度因子，不能进行无级 SR，即单个模型可以处理任何潜在的尺度因子。在每个比例因子上重新训练模型会浪费大量的计算资源和模型存储空间。为了解决上述问题，我们提出了一种显着性感知动态路由网络（SalDRN），用于 RSI 的轻量级和无级 SR。首先，我们引入视觉显着性作为区域级 SR 难度的指标，并将轻量级显着性检测器集成到 SalDRN 中以捕获像素级视觉特征。然后，我们设计了一种显着性感知动态路由策略，该策略采用路径选择开关根据子图像块的 SR 难度自适应地选择适当深度的特征提取路径。最后，我们提出了一种新颖的轻量级无级上采样模块，其核心是隐式特征函数，用于实现从低分辨率特征空间到高分辨率特征空间的映射。综合实验验证，SalDRN 可以在性能和复杂性之间取得良好的折衷。代码位于 \url{this https URL}。
  - [具有可学习位置特征的可扩展神经视频表示, NeurIPS2022](https://arxiv.org/abs/2210.06823) | [***``[code]``***](https://github.com/subin-kim-cv/NVP)
    > 使用基于坐标的神经表示 (CNR) 的复杂信号的简洁表示已经取得了很大进展，最近的几项工作集中在扩展它们以处理视频。在这里，主要挑战是如何（a）减轻训练 CNR 时的计算效率低下，以（b）实现高质量的视频编码，同时（c）保持参数效率。为了同时满足 (a)、(b) 和 (c) 的所有要求，我们提出了具有可学习位置特征 (NVP) 的神经视频表示，这是一种新颖的 CNR，通过引入“可学习位置特征”可以有效地将视频摊销为潜在代码。具体来说，我们首先提出了一种基于设计 2D 潜在关键帧的 CNR 架构，以学习每个时空轴上的常见视频内容，这极大地改善了所有这三个要求。然后，我们建议利用现有强大的图像和视频编解码器作为潜在代码的计算/内存高效压缩过程。我们展示了 NVP 在流行的 UVG 基准上的优越性；与现有技术相比，NVP 不仅训练速度快 2 倍（不到 5 分钟），而且编码质量也超过了 34.07→34.57（用 PSNR 指标衡量），即使使用的参数减少了 8 倍以上。我们还展示了 NVP 的有趣属性，例如视频修复、视频帧插值等。
  - [CUF：连续上采样滤波器](https://arxiv.org/abs/2210.06965) | [code]
    > 神经领域已迅速被用于表示 3D 信号，但它们在更经典的 2D 图像处理中的应用相对有限。在本文中，我们考虑了图像处理中最重要的操作之一：上采样。在深度学习中，可学习的上采样层已广泛用于单图像超分辨率。我们建议将上采样内核参数化为神经域。这种参数化导致了一个紧凑的架构，与竞争的任意尺度超分辨率架构相比，参数数量减少了 40 倍。当对大小为 256x256 的图像进行上采样时，我们表明我们的架构比竞争的任意尺度超分辨率架构效率高 2x-10 倍，并且在实例化为单尺度模型时比亚像素卷积更有效。在一般情况下，这些增益随目标规模的平方呈多项式增长。我们在标准基准上验证了我们的方法，表明可以在不牺牲超分辨率性能的情况下实现这种效率提升。
  - [NerfAcc：一个通用的 NeRF 加速工具箱](https://arxiv.org/abs/2210.04847) | [***``[code]``***](https://github.com/KAIR-BAIR/nerfacc)
    > 我们提出了 NerfAcc，一个用于高效体积渲染辐射场的工具箱。我们以 Instant-NGP 中提出的技术为基础，并将这些技术扩展为不仅支持有界静态场景，还支持动态场景和无界场景。 NerfAcc 带有一个用户友好的 Python API，并为大多数 NeRF 的即插即用加速做好了准备。提供了各种示例来展示如何使用此工具箱。可在此处找到代码：此 https 网址。
## Oct2 - Oct8, 2022
  - [在杂乱的环境中学习感知感知敏捷飞行](https://arxiv.org/abs/2210.01841) | [code]
    > 最近，神经控制策略的性能优于现有的基于模型的规划和控制方法，可在最短的时间内通过杂乱的环境自主导航四旋翼飞行器。然而，它们没有感知意识，这是基于视觉的导航的关键要求，因为相机的视野有限和四旋翼的驱动不足。我们提出了一种学习神经网络策略的方法，该策略可在杂乱的环境中实现感知感知、最短时间飞行。我们的方法通过利用特权学习作弊框架结合了模仿学习和强化学习 (RL)。使用 RL，我们首先训练具有全状态信息的感知感知教师策略，以便在最短时间内通过杂乱的环境。然后，我们使用模仿学习将其知识提炼成基于视觉的学生策略，该策略仅通过相机感知环境。我们的方法将感知和控制紧密结合，在计算速度（快 10 倍）和成功率方面显示出显着优势。我们使用物理四旋翼和硬件在环仿真以高达 50 公里/小时的速度展示了闭环控制性能。
## Sep25 - Oct1, 2022
  - [了解体素网格 NeRF 模型的纯 CLIP 指导](https://arxiv.org/abs/2209.15172) | [code]
    > 我们使用 CLIP 探索文本到 3D 对象生成的任务。具体来说，我们在不访问任何数据集的情况下使用 CLIP 进行指导，我们将这种设置称为纯 CLIP 指导。虽然之前的工作采用了这种设置，但没有系统研究防止 CLIP 中产生对抗性生成的机制。我们说明了不同的基于图像的增强如何防止对抗性生成问题，以及生成的结果如何受到影响。我们测试了不同的 CLIP 模型架构，并表明集成不同的模型进行指导可以防止更大模型中的对抗性生成并产生更清晰的结果。此外，我们实现了一个隐式体素网格模型，以展示神经网络如何提供额外的正则化层，从而产生更好的几何结构和生成对象的连贯性。与之前的工作相比，我们以更高的记忆效率和更快的训练速度获得了更连贯的结果。
## Sep18 - Sep24, 2022
  - [来自单个压缩光场测量的快速视差估计](https://arxiv.org/abs/2209.11342) | [code]
    > 来自光场的丰富空间和角度信息允许开发多种视差估计方法。然而，光场的获取需要较高的存储和处理成本，限制了该技术在实际应用中的使用。为了克服这些缺点，压缩传感 (CS) 理论允许开发光学架构来获取单个编码光场测量。该测量使用需要高计算成本的优化算法或深度神经网络进行解码。从压缩光场进行视差估计的传统方法需要首先恢复整个光场，然后进行后处理步骤，因此需要很长时间。相比之下，这项工作通过省略传统方法中所需的恢复步骤，从单个压缩测量中提出了一种快速的视差估计。具体来说，我们建议联合优化用于获取单个编码光场快照的光学架构和用于估计视差图的卷积神经网络 (CNN)。在实验上，所提出的方法估计的视差图与使用深度学习方法重建的光场获得的视差图相当。此外，所提出的方法在训练和推理方面比从重建光场估计视差的最佳方法快 20 倍。
  - [wildNeRF：使用稀疏单目数据捕获的野外动态场景的完整视图合成](https://arxiv.org/abs/2209.10399) | [code]
    > 我们提出了一种新的神经辐射模型，该模型可以以自我监督的方式进行训练，用于动态非结构化场景的新视图合成。我们的端到端可训练算法可在几秒钟内学习高度复杂的真实静态场景，并在几分钟内学习具有刚性和非刚性运动的动态场景。通过区分静态像素和以运动为中心的像素，我们从一组稀疏的图像中创建高质量的表示。我们对现有基准进行了广泛的定性和定量评估，并在具有挑战性的 NVIDIA 动态场景数据集上设置了最先进的性能指标。此外，我们在具有挑战性的现实世界数据集（例如 Cholec80 和 SurgicalActions160）上评估我们的模型性能。
## Sep11 - Sep17, 2022
## Sep4 - Sep10, 2022
## Aug28 - Sep3, 2022
  - [FoV-NeRF：虚拟现实的中心凹神经辐射场, TVCG2022](https://ieeexplore.ieee.org/abstract/document/9872532) | [code]
    > 随着消费者显示器和商业 VR 平台的兴起，虚拟现实 (VR) 正变得无处不在。这种显示需要低延迟和高质量的合成图像渲染，同时减少计算开销。神经渲染的最新进展表明，有望通过基于图像的虚拟或物理环境表示来解锁 3D 计算机图形的新可能性。具体来说，神经辐射场 (NeRF) 表明，可以在不损失与视图相关的效果的情况下实现 3D 场景的照片般逼真的质量和连续视图变化。虽然 NeRF 可以显着受益于 VR 应用的渲染，但它面临着由高视场、高分辨率和立体/以自我为中心的观看带来的独特挑战，通常会导致渲染图像的低质量和高延迟。在 VR 中，这不仅会损害交互体验，还可能导致疾病。为了解决 VR 中的六自由度、以自我为中心和立体 NeRF 的这些问题，我们提出了第一个注视条件 3D 神经表示和视图合成方法。我们将视觉和立体敏锐度的人类心理物理学纳入 3D 风景的以自我为中心的神经表示中。然后，我们共同优化延迟/性能和视觉质量，同时相互桥接人类感知和神经场景合成，以实现感知上高质量的沉浸式交互。我们进行了客观分析和主观研究，以评估我们方法的有效性。我们发现我们的方法显着减少了延迟（与 NeRF 相比减少了高达 99% 的时间），而不会损失高保真渲染（在感知上与全分辨率地面实况相同）。所提出的方法可能是迈向未来实时捕捉、传送和可视化远程环境的 VR/AR 系统的第一步。
  - [克隆：用于占用网格辅助神经表示的相机-激光雷达融合](https://arxiv.org/abs/2209.01194) | [code]
    > 本文提出了 CLONeR，它通过允许对从稀疏输入传感器视图观察到的大型户外驾驶场景进行建模，显着改进了 NeRF。这是通过将 NeRF 框架内的占用和颜色学习解耦为分别使用 LiDAR 和相机数据训练的单独的多层感知器 (MLP) 来实现的。此外，本文提出了一种在 NeRF 模型旁边构建可微分 3D 占用网格图 (OGM) 的新方法，并利用此占用网格改进沿射线的点采样，以在度量空间中进行体积渲染。
## Aug21 - Aug27, 2022
  - [Voxurf：基于体素的高效准确的神经表面重建](https://arxiv.org/abs/2208.12697) | [code]
    > 神经表面重建旨在基于多视图图像重建准确的 3D 表面。以前基于神经体绘制的方法大多训练完全隐式模型，并且它们需要对单个场景进行数小时的训练。最近的努力探索了显式体积表示，它通过在可学习的体素网格中记忆重要信息来大大加速优化过程。然而，这些基于体素的方法通常难以重建细粒度几何。通过实证研究，我们发现高质量的表面重建取决于两个关键因素：构建连贯形状的能力和颜色几何依赖性的精确建模。特别是后者是精细细节准确重建的关键。受这些发现的启发，我们开发了 Voxurf，这是一种基于体素的高效和准确的神经表面重建方法，它包括两个阶段：1）利用可学习的特征网格来构建色场并获得连贯的粗略形状，以及 2）使用捕获精确的颜色几何依赖性的双色网络优化详细的几何图形。我们进一步引入了分层几何特征，以实现跨体素的信息共享。我们的实验表明，Voxurf 同时实现了高效率和高质量。在 DTU 基准上，与最先进的方法相比，Voxurf 实现了更高的重建质量，训练速度提高了 20 倍。
  - [E-NeRF：来自移动事件相机的神经辐射场](https://arxiv.org/abs/2208.11300) | [code]
    > 从理想图像估计神经辐射场 (NeRFs) 已在计算机视觉领域得到广泛研究。大多数方法假设最佳照明和缓慢的相机运动。这些假设在机器人应用中经常被违反，其中图像包含运动模糊并且场景可能没有合适的照明。这可能会导致下游任务（例如场景的导航、检查或可视化）出现重大问题。为了缓解这些问题，我们提出了 E-NeRF，这是第一种从快速移动的事件摄像机中以 NeRF 形式估计体积场景表示的方法。我们的方法可以在非常快速的运动和高动态范围条件下恢复 NeRF，在这种情况下，基于帧的方法会失败。我们展示了仅通过提供事件流作为输入来渲染高质量帧是可能的。此外，通过结合事件和帧，我们可以估计在严重运动模糊下比最先进的方法质量更高的 NeRF。我们还表明，在只有很少的输入视图可用的情况下，结合事件和帧可以克服 NeRF 估计的失败情况，而无需额外的正则化。
## Aug14 - Aug20, 2022
  - [PDRF：渐进式去模糊辐射场，用于从模糊图像中快速、稳健地重建场景](https://arxiv.org/abs/2208.08049) | [code]
    > 我们提出了渐进式去模糊辐射场 (PDRF)，这是一种从模糊图像中有效重建高质量辐射场的新方法。虽然当前最先进的 (SoTA) 场景重建方法从干净的源视图实现照片般逼真的渲染结果，但当源视图受到模糊影响时，它们的性能会受到影响，这在野外图像中很常见。以前的去模糊方法要么不考虑 3D 几何，要么计算量很大。为了解决这些问题，PDRF 是辐射场建模中的一种渐进式去模糊方案，它通过结合 3D 场景上下文准确地模拟模糊。 PDRF 进一步使用有效的重要性采样方案，从而实现快速的场景优化。具体来说，PDRF 提出了一种 Coarse Ray Renderer 来快速估计体素密度和特征；然后使用 Fine Voxel Renderer 来实现高质量的光线追踪。我们进行了广泛的实验，结果表明 PDRF 比以前的 SoTA 快 15 倍，同时在合成场景和真实场景上都取得了更好的性能。
  - [HDR-Plenoxels：自校准高动态范围辐射场, ECCV2022](https://arxiv.org/abs/2208.06787) | [code]
    > 我们提出了高动态范围辐射 (HDR) 场 HDR-Plenoxels，它学习 3D HDR 辐射场、几何信息和 2D 低动态范围 (LDR) 图像中固有的不同相机设置的全光函数。我们基于体素的体素渲染管道仅使用从不同相机设置中以端到端方式拍摄的多视图 LDR 图像来重建 HDR 辐射场，并且具有快速的收敛速度。为了处理现实世界场景中的各种相机，我们引入了一个色调映射模块，该模块对相机内的数字成像管道 (ISP) 进行建模并解开辐射设置。我们的色调映射模块允许我们通过控制每个新视图的辐射设置来进行渲染。最后，我们构建了一个具有不同相机条件的多视图数据集，这符合我们的问题设置。我们的实验表明，HDR-Plenoxels 可以仅从带有各种相机的 LDR 图像中表达细节和高质量的 HDR 新颖视图。
## Aug7 - Aug13, 2022
  - [OmniVoxel：一种快速精确的全向神经辐射场重建方法, GCCE 2022](https://arxiv.org/abs/2208.06335) | [code]
    > 本文提出了一种利用等矩形全向图像重建神经辐射场的方法。具有辐射场的隐式神经场景表示可以在有限的空间区域内连续重建场景的 3D 形状。然而，在商用 PC 硬件上训练完全隐式表示需要大量时间和计算资源（每个场景 15 ~ 20 小时）。因此，我们提出了一种显着加速这一过程的方法（每个场景 20 ∼ 40 分钟）。我们没有使用完全隐式的光线表示来重建辐射场，而是采用包含张量中的密度和颜色特征的特征体素。考虑到全向 equirectangular 输入和相机布局，我们使用球面体素化来表示，而不是三次表示。我们的体素化方法可以平衡内景和外景的重建质量。此外，我们对颜色特征采用轴对齐位置编码方法来提高整体图像质量。我们的方法在具有随机相机姿势的合成数据集上实现了令人满意的经验性能。此外，我们在包含复杂几何形状的真实场景中测试了我们的方法，并实现了最先进的性能。我们的代码和完整的数据集将与论文发表的同时发布。
  - [HRF-Net：来自稀疏输入的整体辐射场](https://arxiv.org/abs/2208.04717) | [code]
    > 我们提出了 HRF-Net，这是一种基于整体辐射场的新型视图合成方法，它使用一组稀疏输入来渲染新颖的视图。最近的泛化视图合成方法也利用了辐射场，但渲染速度不是实时的。现有的方法可以有效地训练和渲染新颖的视图，但它们不能推广到看不见的场景。我们的方法解决了用于泛化视图合成的实时渲染问题，包括两个主要阶段：整体辐射场预测器和基于卷积的神经渲染器。这种架构不仅可以基于隐式神经场推断出一致的场景几何，还可以使用单个 GPU 有效地渲染新视图。我们首先在 DTU 数据集的多个 3D 场景上训练 HRF-Net，并且该网络可以仅使用光度损失对看不见的真实和合成数据产生似是而非的新颖视图。此外，我们的方法可以利用单个场景的一组更密集的参考图像来生成准确的新颖视图，而无需依赖额外的显式表示，并且仍然保持预训练模型的高速渲染。实验结果表明，HRF-Net 在各种合成和真实数据集上优于最先进的可泛化神经渲染方法。
## Jul31 - Aug6, 2022
  - [全息显示3D相位全息图的端到端学习](https://www.nature.com/articles/s41377-022-00894-6) | [code]
    > 计算机生成的全息术 (CGH) 提供相干波前的体积控制，是体积 3D 显示器、光刻、神经光刺激和光/声捕获等应用的基础。最近，基于深度学习的方法作为 CGH 合成的有前途的计算范式出现，克服了传统基于模拟/优化的方法中的质量-运行时权衡。然而，预测全息图的质量本质上受数据集质量的限制。在这里，我们介绍了一个新的全息图数据集 MIT-CGH-4K-V2，它使用分层深度图像作为数据高效的体积 3D 输入和用于直接合成高质量 3D 相位的两阶段监督+无监督训练协议-只有全息图。所提出的系统还可以校正视觉像差，从而允许为最终用户定制。我们通过实验展示了逼真的 3D 全息投影并讨论了相关的空间光调制器校准程序。我们的方法在消费级 GPU 上实时运行，在 iPhone 13 Pro 上以 5 FPS 运行，有望显着提高上述应用程序的性能。
## Jul24 - Jul30, 2022
  - [MobileNeRF：利用多边形光栅化管道在移动架构上进行高效的神经场渲染](https://arxiv.org/abs/2208.00277) | [***``[code]``***](https://github.com/google-research/jax3d/tree/main/jax3d/projects/mobilenerf)
    > 神经辐射场 (NeRFs) 展示了从新颖视图合成 3D 场景图像的惊人能力。但是，它们依赖于基于光线行进的专用体积渲染算法，这些算法与广泛部署的 g 的功能不匹配图形硬件。本文介绍了一种基于纹理多边形的新 NeRF 表示，它可以使用标准渲染管道有效地合成新图像。 NeRF 表示为一组多边形，其纹理表示二进制不透明度和特征向量。使用 z 缓冲区对多边形进行传统渲染会生成每个像素都有特征的图像，这些图像由在片段着色器中运行的小型、依赖于视图的 MLP 进行解释，以产生最终的像素颜色。这种方法使 NeRF 能够使用传统的多边形光栅化管道进行渲染，该管道提供大规模的像素级并行性，在包括手机在内的各种计算平台上实现交互式帧速率。
  - [通过 NeRF Attention 进行端到端视图合成](https://arxiv.org/abs/2207.14741) | [code]
    > 在本文中，我们提出了一个用于视图合成的简单 seq2seq 公式，其中我们将一组光线点作为输入和输出与光线相对应的颜色。在这个 seq2seq 公式上直接应用标准转换器有两个限制。首先，标准注意力不能成功地适应体积渲染过程，因此合成视图中缺少高频分量。其次，将全局注意力应用于所有光线和像素是非常低效的。受神经辐射场 (NeRF) 的启发，我们提出了 NeRF 注意力 (NeRFA) 来解决上述问题。一方面，NeRFA 将体积渲染方程视为软特征调制过程。通过这种方式，特征调制增强了具有类似 NeRF 电感偏置的变压器。另一方面，NeRFA 执行多阶段注意力以减少计算开销。此外，NeRFA 模型采用光线和像素转换器来学习光线和像素之间的相互作用。 NeRFA 在四个数据集上展示了优于 NeRF 和 NerFormer 的性能：DeepVoxels、Blender、LLFF 和 CO3D。此外，NeRFA 在两种设置下建立了新的 state-of-the-art：单场景视图合成和以类别为中心的新颖视图合成。该代码将公开发布。
  - [脱离网格：用于 3D 血管建模的连续隐式神经表示, MICCAI STACOM 2022](https://arxiv.org/abs/2207.14663) | [code]
    > 个性化 3D 血管模型对于心血管疾病患者的诊断、预后和治疗计划非常有价值。传统上，此类模型是用网格和体素掩码等显式表示或径向基函数或原子（管状）形状等隐式表示构建的。在这里，我们建议在可微的隐式神经表示 (INR) 中通过其有符号距离函数 (SDF) 的零水平集来表示表面。这使我们能够用隐式、连续、轻量级且易于与深度学习算法集成的表示来对复杂的血管结构进行建模。我们在这里通过三个实际示例展示了这种方法的潜力。首先，我们从 CT 图像中获得了腹主动脉瘤 (AAA) 的准确且防水的表面，并从表面上的 200 个点显示出稳健的拟合。其次，我们同时将嵌套的血管壁安装在单个 INR 中，没有交叉点。第三，我们展示了如何将单个动脉的 3D 模型平滑地融合到单个防水表面中。我们的结果表明，INR 是一种灵活的表示形式，具有最小交互注释的潜力复杂血管结构的研究和操作。
## Previous weeks
  - [﻿Plenoxels：没有神经网络的辐射场, CVPR2022(oral)](https://arxiv.org/abs/2112.05131) | [***``[code]``***](https://alexyu.net/plenoxels)
    > 我们介绍了 Plenoxels（全光体素），一种用于照片级真实视图合成的系统。 Plenoxels 将场景表示为具有球谐函数的稀疏 3D 网格。这种表示可以通过梯度方法和正则化从校准图像中优化，而无需任何神经组件。在标准的基准任务中，Plenoxels 的优化速度比神经辐射场快两个数量级，而视觉质量没有损失。
  - [神经稀疏体素场, NeurIPS2020](https://lingjie0206.github.io/papers/NSVF/) | [***``[code]``***](https://github.com/facebookresearch/NSVF)
    > 我们介绍了神经稀疏体素场 (NSVF)，这是一种用于快速和高质量自由视点渲染的新神经场景表示。 NSVF 定义了一组以稀疏体素八叉树组织的体素有界隐式字段，以对每个单元中的局部属性进行建模。 我们仅从一组姿势的 RGB 图像中通过可区分的光线行进操作逐步学习底层体素结构。 使用稀疏体素八叉树结构，可以通过跳过不包含相关场景内容的体素来加速渲染新颖的视图。 我们的方法在推理时比最先进的方法（即 NeRF (Mildenhall et al., 2020)）快 10 倍以上，同时获得更高质量的结果。 此外，通过利用显式稀疏体素表示，我们的方法可以很容易地应用于场景编辑和场景合成。 我们还展示了几个具有挑战性的任务，包括多场景学习、移动人体的自由视点渲染和大规模场景渲染。
  - [AutoInt：快速神经体积渲染的自动集成, CVPR2021](http://www.computationalimaging.org/publications/automatic-integration/) | [***``[code]``***](https://github.com/computational-imaging/automatic-integration)
    > 数值积分是科学计算的基础技术，是许多计算机视觉应用的核心。在这些应用中，隐式神经体绘制最近被提出作为视图合成的新范式，实现逼真的图像质量。然而，使这些方法实用的一个基本障碍是在训练和推理期间沿渲染光线所需的体积积分导致的极端计算和内存要求。需要数百万条光线，每条光线都需要数百次通过神经网络的前向传播，才能通过蒙特卡罗采样来近似这些集成。在这里，我们提出了自动积分，这是一种使用隐式神经表示网络来学习有效的、封闭形式的积分解决方案的新框架。对于训练，我们实例化对应于隐式神经表示的导数的计算图。该图适合要积分的信号。优化后，我们重新组装图以获得代表反导数的网络。根据微积分的基本定理，这可以在网络的两次评估中计算任何定积分。使用这种方法，我们展示了超过 10 倍的计算要求改进，从而实现了快速的神经体绘制。
  - [DeRF：分解的辐射场](https://arxiv.org/abs/2011.12490) | [code]
    > 随着神经辐射场 (NeRF) 的出现，神经网络现在可以渲染 3D 场景的新颖视图，其质量足以愚弄人眼。然而，生成这些图像的计算量非常大，限制了它们在实际场景中的适用性。在本文中，我们提出了一种基于空间分解的技术，能够缓解这个问题。我们的主要观察结果是，使用更大（更深和/或更宽）的网络会带来收益递减。因此，我们建议对场景进行空间分解，并为每个分解部分分配更小的网络。当一起工作时，这些网络可以渲染整个场景。这使我们无论分解部分的数量如何，都能获得近乎恒定的推理时间。此外，我们表明，Voronoi 空间分解更适合此目的，因为它可证明与 Painter 算法兼容，可实现高效且 GPU 友好的渲染。我们的实验表明，对于现实世界的场景，我们的方法提供的推理效率比 NeRF 高出 3 倍（具有相同的渲染质量），或者 PSNR 提高了 1.0~dB（对于相同的推理成本）。
  - [DONeRF：使用 Depth Oracle Networks 实现紧凑神经辐射场的实时渲染, CGF2021](https://depthoraclenerf.github.io/) | [***``[code]``***](https://github.com/facebookresearch/DONERF)
    > 最近围绕神经辐射场 (NeRFs) 的研究爆炸表明，在神经网络中隐式存储场景和照明信息具有巨大的潜力，例如，用于生成新的视图。然而，阻止 NeRF 广泛使用的一个主要限制是沿每个视图射线进行过多网络评估的计算成本过高，当针对当前设备上的实时渲染时需要数十 petaFLOPS。我们表明，当将局部样本放置在场景中的表面周围时，可以显着减少每个视图光线所需的样本数量。为此，我们提出了一个深度预言网络，它通过单个网络评估来预测每个视图光线的光线样本位置。我们表明，使用围绕对数离散和球面扭曲深度值的分类网络对于编码表面位置而不是直接估计深度至关重要。这些技术的结合产生了 DONeRF，这是一种双网络设计，第一步是深度预言网络，以及用于光线累积的局部采样着色网络。通过我们的设计，与 NeRF 相比，我们将推理成本降低了 48 倍。使用现成的推理 API 与简单的计算内核相结合，我们率先在单个 GPU 上以交互式帧速率（每秒 15 帧，800x800）渲染基于光线追踪的神经表示。同时，由于我们专注于表面周围场景的重要部分，与 NeRF 相比，我们获得了相同或更好的质量。
  - [FastNeRF：200FPS 的高保真神经渲染, ICCV2021](https://arxiv.org/abs/2103.10380) | [code]
    > 最近关于神经辐射场 (NeRF) 的工作展示了如何使用神经网络对复杂的 3D 环境进行编码，这些环境可以从新颖的视角进行逼真的渲染。渲染这些图像对计算的要求非常高，最近的改进距离实现交互速率还有很长的路要走，即使在高端硬件上也是如此。受移动和混合现实设备场景的启发，我们提出了 FastNeRF，这是第一个基于 NeRF 的系统，能够在高端消费 GPU 上以 200Hz 渲染高保真逼真图像。我们方法的核心是受图形启发的分解，它允许 (i) 在空间中的每个位置紧凑地缓存深度辐射图，(ii) 使用光线方向有效地查询该图以估计渲染图像中的像素值。大量实验表明，所提出的方法比原始的 NeRF 算法快 3000 倍，并且比现有的加速 NeRF 的工作至少快一个数量级，同时保持视觉质量和可扩展性。
  - [KiloNeRF：使用数千个微型 MLP 加速神经辐射场, ICCV2021](https://arxiv.org/abs/2103.13744) | [***``[code]``***](https://github.com/creiser/kilonerf/)
    > NeRF 通过将神经辐射场拟合到 RGB 图像，以前所未有的质量合成场景的新视图。然而，NeRF 需要数百万次查询深度多层感知器 (MLP)，导致渲染时间变慢，即使在现代 GPU 上也是如此。在本文中，我们证明了通过使用数千个微型 MLP 而不是一个大型 MLP，实时渲染是可能的。在我们的设置中，每个单独的 MLP 只需要表示场景的一部分，因此可以使用更小、更快评估的 MLP。通过将这种分而治之的策略与进一步的优化相结合，与原始 NeRF 模型相比，渲染速度提高了三个数量级，而不会产生高昂的存储成本。此外，使用师生蒸馏进行培训，我们表明可以在不牺牲视觉质量的情况下实现这种加速。
  - [用于实时渲染神经辐射场的 PlenOctrees, ICCV2021(oral)](https://alexyu.net/plenoctrees/) | [***``[code]``***](https://github.com/sxyu/volrend)
    > 实时性能是通过将 NeRF 预先制成基于八叉树的辐射场（我们称为 PlenOctrees）来实现的。为了保留与视图相关的效果，例如镜面反射，我们建议通过封闭形式的球面基函数对外观进行编码。具体来说，我们表明可以训练 NeRFs 来预测辐射的球谐表示，将观察方向作为神经网络的输入。此外，我们表明我们的 PlenOctrees 可以直接优化以进一步最小化重建损失，这导致与竞争方法相同或更好的质量。我们进一步表明，这个八叉树优化步骤可用于加快训练时间，因为我们不再需要等待 NeRF 训练完全收敛。我们的实时神经渲染方法可能会支持新的应用，例如 6 自由度工业和产品可视化，以及下一代 AR/VR 系统。
  - [用于高效神经渲染的体积基元混合, SIGGRAPH2021](https://arxiv.org/abs/2103.01954) | [code]
    > 人类的实时渲染和动画是游戏、电影和远程呈现应用中的核心功能。现有方法有许多我们的工作旨在解决的缺点。三角形网格难以建模像头发这样的细结构，像神经体积这样的体积表示在合理的内存预算下分辨率太低，而像神经辐射场这样的高分辨率隐式表示在实时应用中使用太慢。我们提出了体积基元混合（MVP），一种用于渲染动态 3D 内容的表示，它结合了体积表示的完整性和基于基元的渲染的效率，例如，基于点或基于网格的方法。我们的方法通过利用具有反卷积架构的空间共享计算以及通过使用可以移动以仅覆盖被占用区域的体积基元来最小化空间空白区域中的计算来实现这一点。我们的参数化支持对应和跟踪约束的集成，同时对经典跟踪失败的区域具有鲁棒性，例如薄或半透明结构周围以及具有大拓扑可变性的区域。 MVP 是一种混合体，它概括了基于体积和基元的表示。通过一系列广泛的实验，我们证明它继承了每种方法的优点，同时避免了它们的许多局限性。我们还将我们的方法与几种最先进的方法进行比较，并证明 MVP 在质量和运行时性能方面产生了卓越的结果。
  - [光场网络：具有单次评估渲染的神经场景表示, NeurIPS2021(spotlight)](https://www.vincentsitzmann.com/lfns/) | [***``[code]``***](https://github.com/vsitzmann/light-field-networks)
    > 从 2D 观察推断 3D 场景的表示是计算机图形学、计算机视觉和人工智能的基本问题。新兴的 3D 结构神经场景表示是一种有前途的 3D 场景理解方法。在这项工作中，我们提出了一种新的神经场景表示，光场网络或 LFN，它通过神经隐式表示在 360 度、四维光场中表示底层 3D 场景的几何形状和外观。渲染来自 LFN 的光线只需要*单个*网络评估，而 3D 结构化神经场景表示中的光线行进或基于体积的渲染器每条光线需要数百次评估。在简单场景的设置中，我们利用元学习来学习 LFN 的先验，从而能够从单个图像观察中进行多视图一致的光场重建。这导致时间和内存复杂性的显着降低，并实现了实时渲染。通过 LFN 存储 360 度光场的成本比 Lumigraph 等传统方法低两个数量级。利用神经隐式表示的分析可微性和光空间的新参数化，我们进一步证明了从 LFN 中提取稀疏深度图。
  - [深度监督的 NeRF：更少的视图和更快的免费训练, CVPR2022](https://arxiv.org/abs/2107.02791) | [***``[code]``***](https://github.com/dunbar12138/DSNeRF)
    > 当输入视图数量不足时，通常观察到的神经辐射场 (NeRF) 故障模式会拟合不正确的几何形状。一个潜在的原因是标准体积渲染不会强制执行大多数场景几何体由空白空间和不透明表面组成的约束。我们通过 DS-NeRF（深度监督神经辐射场）将上述假设形式化，这是一种利用现成的深度监督学习辐射场的损失。我们利用当前的 NeRF 管道需要具有已知相机姿势的图像这一事实，这些图像通常通过运行从运动结构 (SFM) 来估计。至关重要的是，SFM 还产生稀疏 3D 点，可在训练期间用作“免费”深度监督：我们添加损失以鼓励光线的终止深度分布匹配给定的 3D 关键点，并结合深度不确定性。 DS-NeRF 可以在训练视图更少的情况下渲染更好的图像，同时训练速度提高 2-3 倍。此外，我们表明我们的损失与最近提出的其他 NeRF 方法兼容，证明深度是一种廉价且易于消化的监督信号。最后，我们发现 DS-NeRF 可以支持其他类型的深度监督，例如扫描深度传感器和 RGB-D 重建输出。
  - [直接体素网格优化：辐射场重建的超快速收敛, CVPR2022(oral)](https://arxiv.org/abs/2111.11215) | [***``[code]``***](https://github.com/sunset1995/DirectVoxGO)
    > 我们提出了一种超快速收敛方法，用于从一组捕获具有已知姿势的场景的图像中重建每个场景的辐射场。这项任务通常应用于新颖的视图合成，最近因其最先进的质量和灵活性而被神经辐射场 (NeRF) 彻底改变。然而，对于单个场景，NeRF 及其变体需要很长的训练时间，从数小时到数天不等。相比之下，我们的方法实现了与 NeRF 相当的质量，并在不到 15 分钟的时间内使用单个 GPU 从头开始​​快速收敛。我们采用由用于场景几何的密度体素网格和具有浅层网络的特征体素网格组成的表示，用于复杂的依赖于视图的外观。使用显式和离散化的体积表示进行建模并不新鲜，但我们提出了两种简单但非平凡的技术，有助于快速收敛和高质量输出。首先，我们介绍了体素密度的激活后插值，它能够以较低的网格分辨率产生锐利的表面。其次，直接体素密度优化容易出现次优几何解决方案，因此我们通过强加几个先验来加强优化过程。最后，对五个内向基准的评估表明，我们的方法与 NeRF 的质量相匹配，甚至超过，但从头开始训练新场景只需要大约 15 分钟。
  - [实时隐式映射和定位, ICCV2021](https://arxiv.org/abs/2103.12352) | [code]
    > 我们首次展示了多层感知器 (MLP) 可以作为手持 RGB-D 相机的实时 SLAM 系统中唯一的场景表示。我们的网络在没有先验数据的情况下进行实时操作训练，构建了一个密集的、特定于场景的隐式 3D 占用率和颜色模型，该模型也可立即用于跟踪。
  - [Mip-NeRF：抗锯齿神经辐射场的多尺度表示, ICCV2021(oral)](https://jonbarron.info/mipnerf/) | [***``[code]``***](https://github.com/google/mipnerf)
    > 神经辐射场 (NeRF) 使用的渲染过程对每个像素单条射线进行采样，因此在训练或测试图像以不同分辨率观察场景内容时，可能会产生过度模糊或混叠的渲染。对于 NeRF 来说，通过每个像素渲染多条光线来进行超级采样的直接解决方案是不切实际的，因为渲染每条光线需要查询多层感知器数百次。我们的解决方案，我们称之为“mip-NeRF”（à la“mipmap”），扩展了 NeRF 以在连续值的尺度上表示场景。通过有效地渲染抗锯齿圆锥截头体而不是射线，mip-NeRF 减少了令人反感的锯齿伪影并显着提高了 NeRF 表示精细细节的能力，同时也比 NeRF 快 7% 和一半的大小。与 NeRF 相比，mip-NeRF 在使用 NeRF 呈现的数据集上将平均错误率降低了 17%，在我们呈现的该数据集的具有挑战性的多尺度变体上降低了 60%。 mip-NeRF 还能够在我们的多尺度数据集上匹配蛮力超采样 NeRF 的准确性，同时速度提高 22 倍。
