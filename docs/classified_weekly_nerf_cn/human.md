
每周分类神经辐射场 - human ![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)
==================================================================================================================================
## 按类别筛选: 
 [全部](../weekly_nerf_cn.md) | [动态](./dynamic.md) | [编辑](./editing.md) | [快速](./fast.md) | [泛化](./generalization.md) | [人体](./human.md) | [视频](./video.md) | [光照](./lighting.md) | [重建](./reconstruction.md) | [纹理](./texture.md) | [语义](./semantic.md) | [姿态-SLAM](./pose-slam.md) | [其他](./others.md) 
## Dec27 - Jan3, 2023
## Dec25 - Dec31, 2022
## Dec18 - Dec24, 2022
## Dec11 - Dec17, 2022
## Dec4 - Dec10, 2022
## Nov27 - Dec3, 2022
  - [NeuWigs：用于体积头发捕捉和动画的神经动态模型](https://arxiv.org/abs/2212.00613) | [code]
    > 人发的捕捉和动画是为虚拟现实创建逼真化身的两个主要挑战。 这两个问题都非常具有挑战性，因为头发具有复杂的几何形状和外观，并且表现出具有挑战性的运动。 在本文中，我们提出了一种两阶段方法，该方法独立于头部对头发进行建模，以数据驱动的方式应对这些挑战。 第一阶段，状态压缩，通过一种新颖的自动编码器作为跟踪器策略，学习包含运动和外观的 3D 头发状态的低维潜在空间。 为了在外观学习中更好地分离头发和头部，我们结合使用多视图头发分割蒙版和可区分的体积渲染器。 第二阶段学习一种新颖的毛发动力学模型，该模型根据发现的潜在代码执行时间毛发转移。 为了在驱动我们的动力学模型时加强稳定性，我们在压缩阶段使用 3D 点云自动编码器来对头发状态进行去噪。 我们的模型在新颖的视图合成方面优于现有技术，并且能够创建新颖的头发动画，而无需依赖头发观察作为驱动信号。 项目页面在此 https URL。
  - [NeRFInvertor：用于单次真实图像动画的高保真 NeRF-GAN 反演](https://arxiv.org/abs/2211.17235) | [code]
    > 基于 Nerf 的生成模型在生成具有一致 3D 几何形状的高质量图像方面表现出了令人印象深刻的能力。 尽管成功合成了从潜在空间随机采样的假身份图像，但由于所谓的反转问题，采用这些模型生成真实主体的面部图像仍然是一项具有挑战性的任务。 在本文中，我们提出了一种通用方法来对这些 NeRF-GAN 模型进行微调，以便仅通过单个图像实现真实对象的高保真动画。 给定域外真实图像的优化潜代码，我们在渲染图像上使用 2D 损失函数来减少身份差距。 此外，我们的方法利用显式和隐式 3D 正则化，使用优化潜在代码周围的域内邻域样本来消除几何和视觉伪影。 我们的实验证实了我们的方法在跨不同数据集的多个 NeRF-GAN 模型上真实、高保真和 3D 一致的真实面孔动画的有效性。
  - [LaplacianFusion：详细的 3D 衣服人体重建, SIGGRAPH-Asia2022](https://dl.acm.org/doi/abs/10.1145/3550454.3555511) | [code]
    > 我们提出了 LaplacianFusion，这是一种从输入深度或 3D 点云序列重建详细且可控的 3D 穿衣人体形状的新颖方法。 我们方法的关键思想是使用拉普拉斯坐标，即已用于网格编辑的众所周知的微分坐标，来表示输入扫描中包含的局部结构，而不是之前使用的隐式 3D 函数或顶点位移。 我们的方法使用 SMPL 重建一个可控的基础网格，并学习一个表面函数来预测表示基础网格表面细节的拉普拉斯坐标。 对于给定的姿势，我们首先构建并细分一个基础网格，这是一个变形的 SMPL 模板，然后使用表面函数估计网格顶点的拉普拉斯坐标。 姿势的最终重建是通过将估计的拉普拉斯坐标作为一个整体进行整合而获得的。 实验结果表明，我们基于拉普拉斯坐标的方法比以前的方法成功地重建了视觉上更令人愉悦的形状细节。 该方法还支持各种表面细节操作，例如细节传输和增强。
  - [DINER：基于深度感知图像的神经辐射场](https://arxiv.org/abs/2211.16630) | [code]
    > 我们提出了基于深度感知图像的神经辐射场 (DINER)。 给定一组稀疏的 RGB 输入视图，我们预测深度和特征图以指导重建体积场景表示，使我们能够在新视图下渲染 3D 对象。 具体来说，我们提出了将深度信息纳入特征融合和高效场景采样的新技术。 与之前的最先进技术相比，DINER 实现了更高的合成质量，并且可以处理具有更大视差的输入视图。 这使我们能够在不改变捕获硬件要求的情况下更完整地捕获场景，并最终在新视图合成过程中实现更大的视点变化。 我们通过合成人头和一般物体的新视图来评估我们的方法，并观察到与以前的现有技术相比，定性结果有了显着改善，感知指标也有所增加。 该代码将公开用于研究目的。
  - [从单目视频重建手持物体, SIGGRAPH-Asia2022](https://dl.acm.org/doi/abs/10.1145/3550469.3555401) | [code]
    > 本文提出了一种从单目视频中重建手持物体的方法。 与许多最近通过训练有素的网络直接预测对象几何形状的方法相比，所提出的方法不需要任何关于对象的先验知识，并且能够恢复更准确和详细的对象几何形状。 关键思想是手部运动自然地提供了对象的多个视图，并且可以通过手部姿势跟踪器可靠地估计该运动。 然后，可以通过解决多视图重建问题来恢复对象几何形状。 我们设计了一种基于隐式神经表示的方法来解决重建问题，并解决手部姿势估计不精确、手部相对运动和小物体的几何优化不足等问题。 我们还提供了一个新收集的具有 3D ground truth 的数据集来验证所提出的方法。 数据集和代码将发布在 https://dihuangdh.github.io/hhor。
  - [Dr.3D：将 3D GAN 应用于艺术绘画, SIGGRAPH-Asia2022](https://dl.acm.org/doi/abs/10.1145/3550469.3555422) | [code]
    > 虽然 3D GAN 最近展示了多视图一致图像和 3D 形状的高质量合成，但它们主要限于照片般逼真的人像。 本文旨在将 3D GAN 扩展到一种不同但有意义的视觉形式：艺术肖像画。 然而，由于绘图中存在不可避免的几何歧义，将现有的 3D GAN 扩展到绘图具有挑战性。 为了解决这个问题，我们提出了 Dr.3D，这是一种新颖的适应方法，可以将现有的 3D GAN 适应艺术绘画。 Dr.3D 配备了三个新组件来处理几何模糊：变形感知 3D 合成网络、姿势估计和图像合成的交替适应以及几何先验。 实验表明，我们的方法可以成功地将 3D GAN 应用于绘图，并实现多视图一致的绘图语义编辑。
  - [Fast-SNARF：一种用于关节神经场的快速变形器](https://arxiv.org/abs/2211.15601) | [code]
    > 神经场彻底改变了刚性场景的 3D 重建和新颖视图合成领域。 使这种方法适用于关节物体（例如人体）的一个关键挑战是对静止姿势（规范空间）和变形空间之间的 3D 位置的变形进行建模。 我们提出了一种新的神经场连接模块 Fast-SNARF，它通过迭代求根找到规范空间和姿势空间之间的准确对应关系。 Fast-SNARF 是我们之前工作 SNARF 功能的直接替代品，同时显着提高了其计算效率。 我们对 SNARF 进行了多项算法和实现改进，产生了 150 倍的加速。 这些改进包括基于体素的对应搜索、预计算线性混合蒙皮函数以及使用 CUDA 内核的高效软件实现。 Fast-SNARF 可以在没有对应的变形观察（例如 3D 网格）的情况下，高效地同时优化形状和蒙皮权重。 由于变形图的学习是许多 3D 人体化身方法中的重要组成部分，并且由于 Fast-SNARF 提供了一种计算高效的解决方案，我们相信这项工作代表了向实际创建 3D 虚拟人迈出的重要一步。
## Nov20 - Nov26, 2022
  - [动态神经肖像, WACV2023](https://arxiv.org/abs/2211.13994) | [code]
    > 我们提出了动态神经肖像，这是一种解决全头重现问题的新方法。我们的方法通过明确控制头部姿势、面部表情和眼睛注视来生成逼真的视频肖像。我们提出的架构不同于现有方法，后者依赖基于 GAN 的图像到图像转换网络将 3D 人脸渲染转换为逼真的图像。相反，我们在具有可控动力学的基于 2D 坐标的 MLP 上构建我们的系统。我们采用基于 2D 的表示而不是最近的 3D 类 NeRF 系统的直觉源于这样一个事实，即视频肖像是由单目固定摄像机拍摄的，因此，只有一个场景的视点可用。首先，我们将我们的生成模型设置为表达式混合形状，尽管如此，我们表明我们的系统也可以成功地由音频功能驱动。我们的实验表明，所提出的方法比最近基于 NeRF 的重演方法快 270 倍，我们的网络在分辨率高达 1024 x 1024 时达到 24 fps 的速度，同时在视觉质量方面优于之前的工作。
  - [FLNeRF：神经辐射场中的 3D 面部地标估计](https://arxiv.org/abs/2211.11202) | [code]
    > 本文介绍了在不使用 2D 图像、深度图或点云等中间表示的情况下直接预测神经辐射场 (NeRF) 上的 3D 面部地标的第一项重要工作。我们的 3D 从粗到细的人脸地标 NeRF (FLNeRF) 模型有效地从整个面部的 NeRF 中采样，并具有个人面部特征以获得准确的地标。为了缓解可用数据中面部表情的有限数量，局部和非线性 NeRF 扭曲被应用于精细的面部特征以模拟大范围的情绪，包括夸张的面部表情（例如，吹脸颊、张大嘴巴、眨眼） ), 用于训练 FLNeRF。通过这种表达增强，我们的模型可以预测 3D 地标，而不仅限于数据中给出的 20 个离散表达。强大的 3D NeRF 面部标志有助于许多下游任务。例如，我们修改 MoFaNeRF 以在 NeRF 上使用面部特征点启用高质量的面部编辑和交换，从而允许更直接的控制和更广泛的复杂表情。实验表明，使用地标的改进模型取得了相当好的结果。
## Nov13 - Nov19, 2022
## Nov6 - Nov12, 2022
## Oct30 - Nov5, 2022
## Oct23 - Oct29, 2022
## Oct16 - Oct22, 2022
  - [NeARportation：远程实时神经渲染框架, VRST22](https://arxiv.org/abs/2210.12398) | [code]
    > 虽然逼真外观的呈现在沉浸在增强虚拟环境中起着重要作用，但显示真实物体的逼真外观仍然是一个具有挑战性的问题。摄影测量学的最新发展促进了将真实物体纳入虚拟空间。然而，照片般逼真的摄影测量需要专用的测量环境，并且需要在测量成本和质量之间进行权衡。此外，即使使用逼真的外观测量，渲染质量和帧速率之间也存在权衡。没有任何框架可以解决这些权衡问题并轻松地实时提供照片般逼真的外观。我们的 NeARportation 框架结合了服务器-客户端双向通信和神经渲染来解决这些权衡问题。服务器上的神经渲染接收客户端的头部姿势并生成具有逼真外观再现的新视图图像，并将其流式传输到客户端的显示器上。通过将我们的框架应用于立体显示器，我们确认它可以根据用户的头部运动以每秒 35-40 帧 (fps) 的速度在全高清立体视频上显示高保真外观。
  - [HDHumans：高保真数字人类的混合方法](https://arxiv.org/abs/2210.12003) | [code]
    > 逼真的数字人类头像在图形中非常重要，因为它们可以在全球范围内实现沉浸式通信，改善游戏和娱乐体验，并且对 AR 和 VR 设置特别有益。然而，当前的头像生成方法要么在高保真新视图合成、对新动作的泛化、宽松衣服的再现方面存在不足，要么无法以现代显示器提供的高分辨率渲染角色。为此，我们提出了 HDHumans，这是第一个用于 HD 人物角色合成的方法，它共同产生准确且时间连贯的 3D 变形表面和任意新颖视图和训练时未看到的运动的高度逼真的图像。在技​​术核心，我们的方法将经典的变形字符模板与神经辐射场 (NeRF) 紧密集成。我们的方法经过精心设计，以实现经典表面变形和 NeRF 之间的协同作用。首先，模板引导 NeRF，它允许合成高度动态和清晰的角色的新视图，甚至可以合成新的动作。其次，我们还利用 NeRF 产生的密集点云通过 3D 到 3D 监督进一步改善变形表面。在合成质量和分辨率以及 3D 表面重建的质量方面，我们在数量和质量上都优于最先进的技术。
## Oct9 - Oct15, 2022
  - [AniFaceGAN：用于视频头像的动画 3D 感知人脸图像生成, NeurIPS2022](https://arxiv.org/abs/2210.06465) | [***``[code]``***](https://yuewuhkust.github.io/AniFaceGAN/files/github_icon.jpeg)
    > 尽管 2D 生成模型在人脸图像生成和动画方面取得了长足进步，但它们在从不同相机视点渲染图像时经常会遇到不希望的伪影，例如 3D 不一致。这可以防止他们合成与真实动画无法区分的视频动画。最近，3D 感知 GAN 扩展了 2D GAN，通过利用 3D 场景表示来明确解开相机姿势。这些方法可以很好地保持生成图像在不同视图中的 3D 一致性，但它们无法实现对其他属性的细粒度控制，其中面部表情控制可以说是面部动画最有用和最理想的方法。在本文中，我们提出了一种可动画的 3D 感知 GAN，用于多视图一致的人脸动画生成。关键思想是将 3D-aware GAN 的 3D 表示分解为模板字段和变形字段，其中前者用规范表达式表示不同的身份，后者表征每个身份的表达变化。为了通过变形实现对面部表情的有意义的控制，我们在 3D 感知 GAN 的对抗训练期间提出了生成器和参数 3D 面部模型之间的 3D 级模仿学习方案。这有助于我们的方法实现具有强烈视觉 3D 一致性的高质量动画人脸图像生成，即使仅使用非结构化 2D 图像进行训练。广泛的实验证明了我们优于以前的工作的性能。项目页面：此 https 网址
  - [从单目视频中重建个性化语义面部 NeRF 模型, SIGGRAPH-Asia2022](https://arxiv.org/abs/2210.06108) | [***``[code]``***](https://github.com/USTC3DV/NeRFBlendShape-code)
    > 我们提出了一种用神经辐射场定义的人头语义模型。 3D 一致的头部模型由一组解耦和可解释的基础组成，并且可以由低维表达系数驱动。由于神经辐射场强大的表示能力，所构建的模型可以表示复杂的面部属性，包括头发、着装等，这些属性是传统网格混合形状无法表示的。为了构建个性化的语义面部模型，我们建议将基础定义为几个多级体素字段。以短的单目 RGB 视频作为输入，我们的方法可以在 10 到 20 分钟内构建主体的语义面部 NeRF 模型，并且可以在给定的表情系数和视图方向下在数十毫秒内渲染出照片般逼真的人头图像。通过这种新颖的表示，我们将其应用于面部重定向和表情编辑等许多任务。实验结果证明了其强大的表示能力和训练/推理速度。我们的项目页面中提供了演示视频和发布的代码：此 https 网址
  - [动态人脸合成的可控辐射场, 3DV2022](https://arxiv.org/abs/2210.05825) | [code]
    > 最近关于 3D 感知图像合成的工作利用神经渲染的进步取得了令人瞩目的成果。然而，面部动态的 3D 感知合成并没有受到太多关注。在这里，我们研究如何明确控制表现出非刚性运动（例如，面部表情变化）的面部动力学的生成模型合成，同时确保 3D 感知。为此，我们提出了一种可控辐射场（CoRF）：1）通过在基于样式的生成器的分层潜在运动空间中嵌入运动特征来实现运动控制； 2）为了确保背景、运动特征和特定主题属性（如光照、纹理、形状、反照率和身份）的一致性，结合了人脸解析网络、头部回归器和身份编码器。在头部图像/视频数据上，我们表明 CoRF 具有 3D 感知能力，同时能够编辑身份、查看方向和运动。
  - [通过神经渲染在静态视频中进行自我监督的 3D 人体姿态估计](https://arxiv.org/abs/2210.04514) | [code]
    > 从 2D 图像推断 3D 人体姿势是计算机视觉领域中一个具有挑战性且长期存在的问题，具有许多应用，包括运动和医学的运动捕捉、虚拟现实、监视或步态分析。我们提供了一种从包含单个人和静态背景的 2D 视频中估计 3D 姿势的方法的初步结果，而无需任何手动地标注释。我们通过制定一个简单而有效的自我监督任务来实现这一点：我们的模型需要重建视频的随机帧，给定来自另一个时间点的帧和变换后的人体形状模板的渲染图像。对于优化至关重要，我们基于光线投射的渲染管道是完全可区分的，能够仅基于重建任务进行端到端训练。
  - [ReFu：细化和融合未观察到的视图以保留细节的单图像 3D 人体重建](https://dl.acm.org/doi/abs/10.1145/3503161.3547971) | [code]
    > 单图像 3D 人体重建旨在在给定单个图像的情况下重建人体的 3D 纹理表面。虽然基于隐式函数的方法最近实现了合理的重建性能，但它们仍然存在局限性，从未观察的角度显示表面几何形状和纹理质量下降。作为回应，为了生成逼真的纹理表面，我们提出了 ReFu，这是一种从粗到细的方法，可以细化投影的背面视图图像并融合细化的图像以预测最终的人体。为了抑制在投影图像和重建网格中引起噪声的扩散占用，我们建议通过同时利用 2D 和 3D 监督和基于占用的体渲染来训练占用概率。我们还引入了一种细化架构，该架构可以生成具有前后扭曲的保留细节的背面视图图像。大量实验表明，我们的方法从单个图像中实现了 3D 人体重建的最先进性能，从未观察到的视图中显示出增强的几何和纹理质量。
## Oct2 - Oct8, 2022
  - [一种基于关键点的音频驱动自由视角说话头合成增强方法](https://arxiv.org/abs/2210.03335) | [code]
    > 音频驱动的说话头合成是一项具有挑战性的任务，近年来越来越受到关注。虽然现有的基于 2D 标志或 3D 人脸模型的方法可以为任意身份合成准确的嘴唇同步和有节奏的头部姿势，但它们仍然存在局限性，例如嘴部映射中的切割感和缺乏皮肤高光。与周围的人脸相比，变形区域是模糊的。提出了一种基于关键点的增强（KPBE）方法用于音频驱动的自由视图说话头合成，以提高生成视频的自然度。首先，使用现有方法作为后端来合成中间结果。然后我们使用关键点分解从后端输出和源图像中提取视频合成控制参数。之后，将控制参数合成为源关键点和驱动关键点。使用基于运动场的方法从关键点表示生成最终图像。通过关键点表示，我们克服了嘴巴映射中的切割感和缺乏皮肤高光的问题。实验表明，我们提出的增强方法在平均意见得分方面提高了谈话头视频的质量。
  - [SelfNeRF：来自单目自旋转视频的人类快速训练 NeRF](https://arxiv.org/abs/2210.01651) | [code]
    > 在本文中，我们提出了 SelfNeRF，一种有效的基于神经辐射场的新型视图合成方法，用于人类表现。给定人类表演者的单目自旋转视频，SelfNeRF 可以从头开始训练并在大约 20 分钟内获得高保真结果。最近的一些工作利用神经辐射场进行动态人体重建。然而，这些方法中的大多数都需要多视图输入并且需要数小时的训练，因此仍然难以实际使用。为了解决这个具有挑战性的问题，我们引入了一种基于多分辨率哈希编码的表面相对表示，可以大大提高训练速度并聚合帧间信息。在几个不同数据集上的广泛实验结果证明了 SelfNeRF 对具有挑战性的单目视频的有效性和效率。
  - [从单目视频中捕捉和动画身体和服装](https://arxiv.org/abs/2210.01868) | [code]
    > 虽然最近的工作已经显示出从单个图像、视频或一组 3D 扫描中提取穿衣服的 3D 人体化身的进展，但仍然存在一些限制。大多数方法使用整体表示来对身体和服装进行联合建模，这意味着对于虚拟试穿等应用，服装和身体不能分开。其他方法分别对身体和衣服进行建模，但它们需要从从 3D/4D 扫描仪或物理模拟获得的大量 3D 衣服人体网格中进行训练。我们的洞察是身体和服装有不同的造型要求。虽然基于网格的参数 3D 模型可以很好地表示身体，但隐式表示和神经辐射场更适合捕捉服装中存在的各种形状和外观。基于这一见解，我们提出了 SCARF（分段穿衣化身辐射场），这是一种将基于网格的身体与神经辐射场相结合的混合模型。将网格与可微分光栅器相结合将网格集成到体积渲染中，使我们能够直接从单目视频优化 SCARF，而无需任何 3D 监督。混合建模使 SCARF 能够（i）通过改变身体姿势（包括手部关节和面部表情）为穿着衣服的身体化身制作动画，（ii）合成化身的新视图，以及（iii）在虚拟试穿中在化身之间转移衣服应用程序。我们证明了 SCARF 重建的服装比现有方法具有更高的视觉质量，服装随着身体姿势和体形的变化而变形，并且服装可以在不同主体的化身之间成功转移。代码和模型可在此 https 网址获得。
  - [MonoNHR：单眼神经人类渲染器](https://arxiv.org/abs/2210.00627) | [code]
    > 由于不可见区域中缺乏信息以及可见区域中像素的深度模糊性，现有的神经人类渲染方法难以处理单个图像输入。在这方面，我们提出了单目神经人类渲染器 (MonoNHR)，这是一种新颖的方法，可以仅在给定单个图像的情况下渲染任意人的鲁棒自由视点图像。 MonoNHR 是第一个（i）在单目设置中呈现在训练期间从未见过的人类受试者，以及（ii）在没有几何监督的情况下以弱监督方式训练的方法。首先，我们建议解开 3D 几何和纹理特征，并根据 3D 几何特征调整纹理推断。其次，我们引入了一个 Mesh Inpainter 模块，该模块利用人类结构先验（例如对称性）来修复被遮挡的部分。在 ZJU-MoCap、AIST 和 HUMBI 数据集上的实验表明，我们的方法明显优于最近适应单目情况的方法。
## Sep25 - Oct1, 2022
## Sep18 - Sep24, 2022
  - [FNeVR：面部动画的神经体积渲染](https://arxiv.org/abs/2209.10340) | [code]
    > 人脸动画是计算机视觉中最热门的话题之一，在生成模型的帮助下取得了可喜的成绩。然而，由于复杂的运动变形和复杂的面部细节建模，生成身份保持和照片般逼真的图像仍然是一个关键挑战。为了解决这些问题，我们提出了一个人脸神经体绘制 (FNeVR) 网络，以在一个统一的框架中充分挖掘 2D 运动扭曲和 3D 体绘制的潜力。在 FNeVR 中，我们设计了一个 3D 面部体积渲染 (FVR) 模块来增强图像渲染的面部细节。具体来说，我们首先使用精心设计的架构提取 3D 信息，然后引入正交自适应光线采样模块以实现高效渲染。我们还设计了一个轻量级的姿势编辑器，使 FNeVR 能够以简单而有效的方式编辑面部姿势。大量实验表明，我们的 FNeVR 在广泛使用的 Talking Head 基准测试中获得了最佳的整体质量和性能。
  - [通过神经动画网格进行人体性能建模和渲染](https://arxiv.org/abs/2209.08468) | [code]
    > 我们最近看到了照片真实人体建模和渲染的神经进步的巨大进步。但是，将它们集成到现有的基于网格的管道中以用于下游应用程序仍然具有挑战性。在本文中，我们提出了一种综合神经方法，用于从密集的多视图视频中对人类表演进行高质量的重建、压缩和渲染。我们的核心直觉是将传统的动画网格工作流程与新型高效神经技术联系起来。我们首先介绍了一种用于在几分钟内生成高质量表面的神经表面重建器。它将截断有符号距离场 (TSDF) 的隐式体积渲染与多分辨率哈希编码结合在一起。我们进一步提出了一种混合神经跟踪器来生成动画网格，它将显式非刚性跟踪与自监督框架中的隐式动态变形相结合。前者将粗略的变形提供回规范空间，而后者隐含的进一步使用我们的重构器中的 4D 哈希编码来预测位移。然后，我们讨论使用获得的动画网格的渲染方案，范围从动态纹理到各种带宽设置下的流明图渲染。为了在质量和带宽之间取得复杂的平衡，我们提出了一种分层解决方案，首先渲染覆盖表演者的 6 个虚拟视图，然后进行遮挡感知神经纹理混合。我们展示了我们的方法在各种基于网格的应用程序和各种平台上逼真的自由视图体验中的有效性，即通过移动 AR 将虚拟人类表演插入真实环境或使用 VR 耳机沉浸式观看才艺表演。
## Sep11 - Sep17, 2022
  - [3DMM-RF：用于 3D 人脸建模的卷积辐射场](https://arxiv.org/abs/2209.07366) | [code]
    > 面部 3D 可变形模型是具有无数应用的主要计算机视觉主题，并且在过去二十年中得到了高度优化。深度生成网络的巨大改进为改进此类模型创造了各种可能性，并引起了广泛的兴趣。此外，神经辐射领域的最新进展正在彻底改变已知场景的新视图合成。在这项工作中，我们提出了一个面部 3D 可变形模型，它利用了上述两者，并且可以准确地建模对象的身份、姿势和表情，并在任意光照下渲染它。这是通过利用强大的基于深度样式的生成器来克服神经辐射场的两个主要弱点，即它们的刚性和渲染速度来实现的。我们引入了一种基于样式的生成网络，它一次性合成所有且仅合成神经辐射场所需的渲染样本。我们创建了一个巨大的面部渲染标记合成数据集，并在这些数据上训练网络，以便它可以准确地建模和概括面部身份、姿势和外观。最后，我们证明该模型可以准确地拟合任意姿势和光照的“in-the-wild”人脸图像，提取人脸特征，并用于在可控条件下重新渲染人脸。
  - [明确可控的 3D 感知肖像生成](https://arxiv.org/abs/2209.05434) | [code]
    > 与成本高昂的传统头像创建流程相比，当代生成方法直接从照片中学习数据分布。虽然大量工作扩展了无条件生成模型并实现了一定程度的可控性，但确保多视图一致性仍然具有挑战性，尤其是在大姿势中。在这项工作中，我们提出了一个生成 3D 感知肖像的网络，同时可以根据有关姿势、身份、表情和照明的语义参数进行控制。我们的网络使用神经场景表示来建模 3D 感知肖像，其生成由支持显式控制的参数化面部模型引导。虽然通过对比具有部分不同属性的图像可以进一步增强潜在的解缠结，但在为表情制作动画时，非面部区域（例如头发和背景）仍然存在明显的不一致。我们通过提出一种体积混合策略来解决这个问题，在该策略中，我们通过混合动态和静态区域来形成复合输出，其中两部分从联合学习的语义场中分割出来。我们的方法在广泛的实验中优于现有技术，当从自由视角观看时，可以在自然光下生成逼真的肖像，并具有生动的表达。它还展示了对真实图像和域外数据的泛化能力，在实际应用中显示出巨大的前景。
## Sep4 - Sep10, 2022
  - [SIRA：来自单个图像的可重新点亮的头像](https://arxiv.org/abs/2209.03027) | [code]
    > 从单个图像中恢复人头的几何形状，同时分解材料和照明是一个严重不适定的问题，需要解决先验信息。基于 3D 可变形模型 (3DMM) 的方法，以及它们与可微渲染器的组合，已显示出可喜的结果。然而，3DMM 的表现力是有限的，它们通常会产生过度平滑且与身份无关的 3D 形状，仅限于面部区域。最近已经通过使用多层感知器参数化几何形状的神经场获得了高度准确的全头重建。这些表示的多功能性也被证明对于解开几何、材料和照明是有效的。然而，这些方法需要几十个输入图像。在本文中，我们介绍了 SIRA，这是一种从单个图像重建具有高保真几何形状和分解光和表面材料的人头头像的方法。我们的关键成分是两个基于神经场的数据驱动统计模型，可解决单视图 3D 表面重建和外观分解的模糊性。实验表明，SIRA 在 3D 头部重建中获得了最先进的结果，同时它成功地解开了全局照明、漫反射和镜面反射率。此外，我们的重建适用于基于物理的外观编辑和头部模型重新照明。
  - [MotionDiffuse：使用扩散模型的文本驱动人体运动生成](https://arxiv.org/abs/2208.15001) | [***``[code]``***](https://github.com/mingyuan-zhang/MotionDiffuse)
    > 人体运动建模对于许多现代图形应用程序很重要，这些应用程序通常需要专业技能。为了消除外行的技能障碍，最近的动作生成方法可以直接生成以自然语言为条件的人体动作。然而，通过各种文本输入实现多样化和细粒度的运动生成仍然具有挑战性。为了解决这个问题，我们提出了 MotionDiffuse，这是第一个基于扩散模型的文本驱动的运动生成框架，它展示了现有方法的几个所需属性。 1）概率映射。 MotionDiffuse 不是确定性的语言-运动映射，而是通过一系列注入变化的去噪步骤生成运动。 2）现实综合。 MotionDiffuse 擅长对复杂的数据分布进行建模并生成生动的运动序列。 3) 多级操作。 MotionDiffuse 响应身体部位的细粒度指令，以及带有时变文本提示的任意长度运动合成。我们的实验表明，MotionDiffuse 在文本驱动的运动生成和动作条件的运动生成方面具有令人信服的优势，从而优于现有的 SoTA 方法。定性分析进一步证明了 MotionDiffuse 对综合运动生成的可控性。主页：此 https 网址
## Aug28 - Sep3, 2022
  - [Dual-Space NeRF：在不同空间中学习动画化身和场景照明, 3DV2022](https://arxiv.org/abs/2208.14851) | [code]
    > 在规范空间中对人体进行建模是捕捉和动画的常见做法。但是当涉及到神经辐射场 (NeRF) 时，仅仅在标准空间中学习一个静态的 NeRF 是不够的，因为即使场景照明是恒定的，当人移动时身体的照明也会发生变化。以前的方法通过学习每帧嵌入来缓解光照的不一致性，但这种操作并不能推广到看不见的姿势。鉴于光照条件在世界空间中是静态的，而人体在规范空间中是一致的，我们提出了一种双空间 NeRF，它在两个独立的空间中使用两个 MLP 对场景光照和人体进行建模。为了弥合这两个空间，以前的方法主要依赖于线性混合蒙皮 (LBS) 算法。然而，动态神经领域的 LBS 的混合权重是难以处理的，因此通常用另一个 MLP 来记忆，这不能推广到新的姿势。尽管可以借用 SMPL 等参数网格的混合权重，但插值操作会引入更多伪影。在本文中，我们建议使用重心映射，它可以直接泛化到看不见的姿势，并且出人意料地取得了比具有神经混合权重的 LBS 更好的结果。 Human3.6M 和 ZJU-MoCap 数据集的定量和定性结果显示了我们方法的有效性。
  - [NerfCap：使用动态神经辐射场捕获人类表现, TVCG2022](https://ieeexplore.ieee.org/abstract/document/9870173) | [code]
    > 本文解决了从稀疏的多视图或单目视频中捕捉人类表演的挑战。给定表演者的模板网格，以前的方法通过将模板网格非刚性地注册到具有 2D 轮廓或密集光度对齐的图像来捕获人体运动。然而，详细的表面变形无法从轮廓中恢复，而光度对齐则受到视频外观变化引起的不稳定性的影响。为了解决这些问题，我们提出了 NerfCap，这是一种基于表演者动态神经辐射场 (NeRF) 表示的新型表演捕捉方法。具体来说，通过优化变形场和规范 NeRF 的外观模型，从模板几何初始化规范 NeRF 并注册到视频帧。为了捕捉大型身体运动和详细的表面变形，NerfCap 将线性混合蒙皮与嵌入式图形变形相结合。与受限于固定拓扑和纹理的基于网格的方法相比，NerfCap 能够灵活地捕捉视频中复杂的几何形状和外观变化，并合成更逼真的图像。此外，NerfCap 可以通过将合成视频与输入视频进行匹配，以自我监督的方式进行端到端的预训练。各种数据集的实验结果表明，NerfCap 在表面重建精度和新视图合成质量方面都优于先前的工作。
## Aug21 - Aug27, 2022
  - [神经小说演员：学习人类演员的广义动画神经表示](https://arxiv.org/abs/2208.11905) | [code]
    > 我们提出了一种新方法，用于从一组稀疏的多人多视图图像中学习广义的可动画神经人类表示。学习到的表示可用于从一组稀疏的相机中合成任意人的新颖视图图像，并使用用户的姿势控制进一步对它们进行动画处理。虽然现有方法可以推广到新人或使用用户控制合成动画，但它们都不能同时实现这两者。我们将这一成就归功于为共享的多人人体模型使用 3D 代理，并进一步将不同姿势的空间扭曲到共享的规范姿势空间，在该空间中，我们学习了一个神经领域并预测了人和与姿势相关的变形，以及从输入图像中提取的特征的外观。为了应对身体形状、姿势和服装变形的巨大变化的复杂性，我们设计了具有解开几何和外观的神经人体模型。此外，我们利用 3D 代理的空间点和表面点的图像特征来预测与人和姿势相关的属性。实验表明，我们的方法在这两项任务上都显着优于现有技术。视频和代码可在此 https 网址上找到。
## Aug14 - Aug20, 2022
  - [通过多平面图像的 3D 对象运动估计动态场景的时间视图合成, ISMAR2022](https://arxiv.org/abs/2208.09463) | [***``[code]``***](https://github.com/NagabhushanSN95/DeCOMPnet)
    > 在低计算设备上以图形方式渲染高帧率视频的挑战可以通过对未来帧的定期预测来解决，以增强虚拟现实应用程序中的用户体验。这是通过时间视图合成 (TVS) 的问题来研究的，其目标是在给定前一帧以及前一帧和下一帧的头部姿势的情况下预测视频的下一帧。在这项工作中，我们考虑了用户和对象都在移动的动态场景的 TVS。我们设计了一个框架，将运动解耦为用户和对象运动，以在预测下一帧的同时有效地使用可用的用户运动。我们通过隔离和估计过去帧中的 3D 对象运动然后外推来预测对象的运动。我们使用多平面图像 (MPI) 作为场景的 3D 表示，并将对象运动建模为 MPI 表示中对应点之间的 3D 位移。为了在估计运动时处理 MPI 中的稀疏性，我们结合了部分卷积和掩蔽相关层来估计对应点。然后将预测的对象运动与给定的用户或相机运动集成以生成下一帧。使用遮蔽填充模块，我们合成由于相机和物体运动而未覆盖的区域。我们为包含 800 个全高清分辨率视频的动态场景 TVS 开发了一个新的合成数据集。我们通过对我们的数据集和 MPI Sintel 数据集的实验表明，我们的模型优于文献中的所有竞争方法。
  - [LoRD：用于高保真动态人体建模的局部 4D 隐式表示, ECCV2022](https://arxiv.org/abs/2208.08622) | [code]
    > 4D 隐式表示的最新进展集中在使用低维潜在向量全局控制形状和运动，这容易丢失表面细节和累积跟踪误差。尽管许多深度局部表示已显示出可用于 3D 形状建模的有希望的结果，但它们的 4D 对应物尚不存在。在本文中，我们提出了一种新颖的用于动态服装人体的局部 4D 隐式表示，名为 LoRD，以填补这一空白，它兼具 4D 人体建模和局部表示的优点，并能够通过详细的表面变形进行高保真重建，例如衣服褶皱。特别是，我们的关键见解是鼓励网络学习局部部分级表示的潜在代码，能够解释局部几何和时间变形。为了在测试时进行推断，我们首先在每个时间步估计体内骨骼运动以跟踪局部部位，然后根据不同类型的观察数据通过自动解码优化每个部位的潜在代码。大量实验表明，该方法具有很强的表示 4D 人体的能力，并且在实际应用中优于最先进的方法，包括从稀疏点进行 4D 重建、非刚性深度融合，无论是定性还是定量。
  - [从单目视频中对动画 3D 人体进行神经捕获, ECCV2022](https://arxiv.org/abs/2208.08728) | [code]
    > 我们提出了一种从单目视频输入构建可动画 3D 人体表示的新颖范例，这样它就可以以任何看不见的姿势和视图进行渲染。我们的方法基于动态神经辐射场 (NeRF)，该动态神经辐射场 (NeRF) 由作为几何代理的基于网格的参数化 3D 人体模型装配。以前的方法通常依赖多视图视频或准确的 3D 几何信息作为附加输入；此外，大多数方法在推广到看不见的姿势时质量会下降。我们认为，泛化的关键是用于查询动态 NeRF 的良好输入嵌入：良好的输入嵌入应该定义全体积空间中的单射映射，由姿态变化下的表面网格变形引导。基于这一观察，我们建议嵌入输入查询及其与网格顶点上一组测地最近邻所跨越的局部表面区域的关系。通过包含位置和相对距离信息，我们的嵌入定义了距离保留的变形映射，并很好地推广到看不见的姿势。为了减少对额外输入的依赖，我们首先使用现成的工具初始化每帧 3D 网格，然后提出一个管道来联合优化 NeRF 并细化初始网格。大量实验表明，我们的方法可以在看不见的姿势和视图下合成合理的人类渲染结果。
## Aug7 - Aug13, 2022
  - [渐进式多尺度光场网络, 3DV2022](https://arxiv.org/abs/2208.06710) | [code]
    > 与图像集表示相比，神经表示在表示辐射和光场的能力方面显示出了巨大的希望，同时非常紧凑。然而，当前的表示不太适合流式传输，因为解码只能在单个细节级别上完成，并且需要下载整个神经网络模型。此外，高分辨率光场网络可能会出现闪烁和混叠，因为在没有适当过滤的情况下对神经网络进行采样。为了解决这些问题，我们提出了一个渐进式多尺度光场网络，它对具有多层次细节的光场进行编码。使用较少的神经网络权重对较低级别的细节进行编码，从而实现渐进式流传输并减少渲染时间。我们的渐进式多尺度光场网络通过在较低细节级别编码较小的抗锯齿表示来解决锯齿问题。此外，每个像素级别的细节使我们的表示能够支持抖动过渡和中心点渲染。
## Jul31 - Aug6, 2022
## Jul24 - Jul30, 2022
  - [神经链：从多视图图像中学习头发的几何形状和外观, ECCV2022](https://arxiv.org/pdf/2207.14067) | [***``[code]``***](https://radualexandru.github.io/neural_strands/)
    > 我们提出了 Neural Strands，这是一种新颖的学习框架，用于从多视图图像输入中对精确的头发几何形状和外观进行建模。学习的头发模型可以从具有高保真视图相关效果的任何视点实时渲染。与体积模型不同，我们的模型实现了直观的形状和样式控制。为了实现这些特性，我们提出了一种基于神经头皮纹理的新型头发表示，该神经头皮纹理对每个纹素位置的单个股线的几何形状和外观进行编码。此外，我们引入了一种基于学习发束光栅化的新型神经渲染框架。我们的神经渲染是精确的和抗锯齿的，使渲染视图一致且逼真。将外观与多视图几何先验相结合，我们首次实现了从多视图设置中联合学习外观和显式头发几何形状。我们展示了我们的方法在各种发型的保真度和效率方面的有效性。
## Previous weeks
  - [用于单目 4D 面部头像重建的动态神经辐射场, CVPR2021](https://gafniguy.github.io/4D-Facial-Avatars/) | [***``[code]``***](https://github.com/gafniguy/4D-Facial-Avatars)
    > 我们提出了用于模拟人脸外观和动态的动态神经辐射场。对说话的人进行数字建模和重建是各种应用程序的关键组成部分。特别是对于 AR 或 VR 中的远程呈现应用，需要忠实再现外观，包括新颖的视点或头部姿势。与显式建模几何和材料属性或纯粹基于图像的最先进方法相比，我们引入了基于场景表示网络的头部隐式表示。为了处理面部的动态，我们将场景表示网络与低维可变形模型相结合，该模型提供对姿势和表情的显式控制。我们使用体积渲染从这种混合表示中生成图像，并证明这种动态神经场景表示只能从单目输入数据中学习，而不需要专门的捕获设置。在我们的实验中，我们表明这种学习的体积表示允许生成照片般逼真的图像，其质量超过了基于视频的最先进的重演方法的质量。
  - [PVA：像素对齐的体积化身, CVPR2021](https://volumetric-avatars.github.io/) | [code]
    > 逼真的人头的采集和渲染是一个极具挑战性的研究问题，对于虚拟远程呈现特别重要。目前，最高质量是通过在多视图数据上以个人特定方式训练的体积方法实现的。与更简单的基于网格的模型相比，这些模型更好地表示精细结构，例如头发。体积模型通常使用全局代码来表示面部表情，以便它们可以由一小组动画参数驱动。虽然这样的架构实现了令人印象深刻的渲染质量，但它们不能轻易地扩展到多身份设置。在本文中，我们设计了一种新颖的方法，用于在仅给定少量输入的情况下预测人头的体积化身。我们通过一种新颖的参数化实现跨身份的泛化，该参数化将神经辐射场与直接从输入中提取的局部像素对齐特征相结合，从而避免了对非常深或复杂网络的需求。我们的方法仅基于光度重新渲染损失以端到端的方式进行训练，无需明确的 3D 监督。我们证明我们的方法在质量方面优于现有的现有技术，并且能够生成忠实的面部表情多身份设置。
  - [用于人体建模的动画神经辐射场, ICCV2021](https://zju3dv.github.io/animatable_nerf/) | [***``[code]``***](https://github.com/zju3dv/animatable_nerf)
    > 本文解决了从多视图视频中重建可动画人体模型的挑战。最近的一些工作提出将非刚性变形场景分解为规范神经辐射场和一组将观察空间点映射到规范空间的变形场，从而使他们能够从图像中学习动态场景。然而，它们将变形场表示为平移矢量场或 SE(3) 场，这使得优化受到高度约束。此外，这些表示不能由输入运动明确控制。相反，我们引入了神经混合权重场来产生变形场。基于骨架驱动的变形，混合权重场与 3D 人体骨骼一起使用，以生成观察到规范和规范到观察的对应关系。由于 3D 人体骨骼更易观察，它们可以规范变形场的学习。此外，学习到的混合权重场可以与输入的骨骼运动相结合，以生成新的变形场来为人体模型设置动画。实验表明，我们的方法明显优于最近的人类合成方法。该代码将在 https://zju3dv.github.io/animatable_nerf/ 上提供。
  - [神经演员：具有姿势控制的人类演员的神经自由视图合成, SIGSIGGRAPH Asia 2021](https://vcai.mpi-inf.mpg.de/projects/NeuralActor/) | [***``[code]``***](https://people.mpi-inf.mpg.de/~lliu/projects/NeuralActor/)
    > 我们提出了神经演员 (NA)，这是一种从任意视角和任意可控姿势下高质量合成人类的新方法。我们的方法建立在最近的神经场景表示和渲染工作之上，这些工作仅从 2D 图像中学习几何和外观的表示。虽然现有作品展示了令人信服的静态场景渲染和动态场景回放，但使用神经隐式方法对人类进行逼真的重建和渲染，特别是在用户控制的新姿势下，仍然很困难。为了解决这个问题，我们利用粗体模型作为代理将周围的 3D 空间展开为规范姿势。神经辐射场从多视图视频输入中学习规范空间中与姿势相关的几何变形以及与姿势和视图相关的外观效果。为了合成高保真动态几何和外观的新视图，我们利用在身体模型上定义的 2D 纹理图作为潜在变量来预测残余变形和动态外观。实验表明，我们的方法在回放和新颖的姿势合成方面取得了比现有技术更好的质量，甚至可以很好地推广到与训练姿势截然不同的新姿势。此外，我们的方法还支持合成结果的体形控制。
  - [神经体：具有结构化潜在代码的隐式神经表示，用于动态人类的新视图合成, CVPR2021](https://zju3dv.github.io/neuralbody/) | [***``[code]``***](https://github.com/zju3dv/neuralbody)
    > 本文解决了人类表演者从一组非常稀疏的摄像机视图中合成新颖视图的挑战。最近的一些工作表明，在给定密集输入视图的情况下，学习 3D 场景的隐式神经表示可以实现显着的视图合成质量。但是，如果视图高度稀疏，则表示学习将是不适定的。为了解决这个不适定问题，我们的关键思想是整合对视频帧的观察。为此，我们提出了神经体，这是一种新的人体表示，它假设在不同帧上学习到的神经表示共享同一组锚定到可变形网格的潜在代码，以便可以自然地整合跨帧的观察结果。可变形网格还为网络提供几何指导，以更有效地学习 3D 表示。为了评估我们的方法，我们创建了一个名为 ZJU-MoCap 的多视图数据集，用于捕捉具有复杂动作的表演者。 ZJU-MoCap 的实验表明，我们的方法在新颖的视图合成质量方面大大优于先前的工作。我们还展示了我们的方法从 People-Snapshot 数据集上的单目视频重建移动人物的能力。
  - [单张图像的人像神经辐射场](https://portrait-nerf.github.io/) | [code]
    > 我们提出了一种从单个爆头肖像估计神经辐射场 (NeRF) 的方法。虽然 NeRF 已经展示了高质量的视图合成，但它需要静态场景的多个图像，因此对于随意捕捉和移动主体是不切实际的。在这项工作中，我们建议使用使用灯光舞台肖像数据集的元学习框架来预训练多层感知器 (MLP) 的权重，该多层感知器隐含地对体积密度和颜色进行建模。为了提高对看不见的人脸的泛化能力，我们在由 3D 人脸可变形模型近似的规范坐标空间中训练 MLP。我们使用受控捕获对方法进行定量评估，并展示了对真实肖像图像的泛化性，显示出对最先进技术的有利结果。
  - [A-NeRF：通过神经渲染进行无表面人体 3D 姿势细化, NeurIPS2021](https://arxiv.org/abs/2102.06199) | [***``[code]``***](https://github.com/LemonATsu/A-NeRF)
    > 虽然深度学习使用前馈网络重塑了经典的运动捕捉管道，但需要生成模型通过迭代细化来恢复精细对齐。不幸的是，现有模型通常是在受控条件下手工制作或学习的，仅适用于有限的领域。我们提出了一种通过扩展神经辐射场 (NeRFs) 从未标记的单目视频中学习生成神经体模型的方法。我们为它们配备了骨架，以适用于时变和关节运动。一个关键的见解是，隐式模型需要与显式曲面模型中使用的正向运动学相反。我们的重新参数化定义了相对于身体部位姿势的空间潜在变量，从而克服了过度参数化的不适定逆运算。这使得从头开始学习体积身体形状和外观，同时共同改进关节姿势；输入视频上的所有外观、姿势或 3D 形状都没有地面实况标签。当用于新视图合成和动作捕捉时，我们的神经模型提高了不同数据集的准确性。项目网站：此 https 网址。
  - [学习动态人头的组成辐射场, CVPR2021(oral)](https://ziyanw1.github.io/hybrid_nerf/) | [code]
    > 动态人体的逼真渲染是远程呈现系统、虚拟购物、合成数据生成等的重要能力。最近，结合计算机图形学和机器学习技术的神经渲染方法已经创建了人类和物体的高保真模型。其中一些方法不会为可驱动的人体模型（神经体积）产生足够高保真度的结果，而其他方法则具有极长的渲染时间（NeRF）。我们提出了一种新颖的组合 3D 表示，它结合了以前最好的方法来产生更高分辨率和更快的结果。我们的表示通过将粗略的 3D 结构感知动画代码网格与连续学习的场景函数相结合，弥合了离散和连续体积表示之间的差距，该函数将每个位置及其相应的局部动画代码映射到其与视图相关的发射辐射和局部体积密度。可微分体渲染用于计算人头和上身的照片般逼真的新颖视图，并仅使用 2D 监督来端到端训练我们的新颖表示。此外，我们表明，学习到的动态辐射场可用于基于全局动画代码合成新的看不见的表情。我们的方法在合成动态人头和上半身的新视图方面取得了最先进的结果。
  - [使用分层神经表示的可编辑自由视点视频, SIGGRAPH2021](https://jiakai-zhang.github.io/st-nerf/) | [***``[code]``***](https://jiakai-zhang.github.io/st-nerf/#code)
    > 生成自由视点视频对于沉浸式 VR/AR 体验至关重要，但最近的神经学进展仍然缺乏编辑能力来操纵大型动态场景的视觉感知。为了填补这一空白，在本文中，我们提出了第一种仅使用稀疏的 16 个摄像头为大规模动态场景生成可编辑照片般逼真的自由视点视频的方法。我们方法的核心是一种新的分层神经表示，其中包括环境本身的每个动态实体都被制定为称为 ST-NeRF 的时空相干神经分层辐射表示。这种分层表示支持对动态场景的完全感知和真实操作，同时仍支持大范围的自由观看体验。在我们的 ST-NeRF 中，动态实体/层被表示为连续函数，以连续和自监督的方式实现动态实体的位置、变形以及外观的解耦。我们提出了一个场景解析 4D 标签映射跟踪来显式地解开空间信息，以及一个连续变形模块来隐式地解开时间运动。进一步引入了一种对象感知体绘制方案，用于重新组装所有神经层。我们采用了一种新颖的分层损失和运动感知光线采样策略，以实现对具有多个表演者的大型动态场景的有效训练，我们的框架进一步实现了各种编辑功能，即操纵规模和位置，复制或重新定时单个神经层在保持高度真实感的同时创造众多视觉效果。大量实验证明了我们的方法在为动态场景生成高质量、照片般逼真和可编辑的自由视点视频方面的有效性。
