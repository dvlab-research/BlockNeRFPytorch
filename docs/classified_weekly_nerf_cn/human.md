
每周分类神经辐射场 - human ![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)
==================================================================================================================================
## 按类别筛选: 
 [全部](../weekly_nerf_cn.md) | [动态](./dynamic.md) | [编辑](./editing.md) | [快速](./fast.md) | [泛化](./generalization.md) | [人体](./human.md) | [视频](./video.md) | [光照](./lighting.md) | [重建](./reconstruction.md) | [纹理](./texture.md) | [语义](./semantic.md) | [姿态-SLAM](./pose-slam.md) | [其他](./others.md) 
## Oct2- Oct8, 2022
  - [SelfNeRF：来自单目自旋转视频的人类快速训练 NeRF](https://arxiv.org/abs/2210.01651) | [code]
    > 在本文中，我们提出了 SelfNeRF，一种有效的基于神经辐射场的新型视图合成方法，用于人类表现。给定人类表演者的单目自旋转视频，SelfNeRF 可以从头开始训练并在大约 20 分钟内获得高保真结果。最近的一些工作利用神经辐射场进行动态人体重建。然而，这些方法中的大多数都需要多视图输入并且需要数小时的训练，因此仍然难以实际使用。为了解决这个具有挑战性的问题，我们引入了一种基于多分辨率哈希编码的表面相对表示，可以大大提高训练速度并聚合帧间信息。在几个不同数据集上的广泛实验结果证明了 SelfNeRF 对具有挑战性的单目视频的有效性和效率。
  - [从单目视频中捕捉和动画身体和服装](https://arxiv.org/abs/2210.01868) | [code]
    > 虽然最近的工作已经显示出从单个图像、视频或一组 3D 扫描中提取穿衣服的 3D 人体化身的进展，但仍然存在一些限制。大多数方法使用整体表示来对身体和服装进行联合建模，这意味着对于虚拟试穿等应用，服装和身体不能分开。其他方法分别对身体和衣服进行建模，但它们需要从从 3D/4D 扫描仪或物理模拟获得的大量 3D 衣服人体网格中进行训练。我们的洞察是身体和服装有不同的造型要求。虽然基于网格的参数 3D 模型可以很好地表示身体，但隐式表示和神经辐射场更适合捕捉服装中存在的各种形状和外观。基于这一见解，我们提出了 SCARF（分段穿衣化身辐射场），这是一种将基于网格的身体与神经辐射场相结合的混合模型。将网格与可微分光栅器相结合将网格集成到体积渲染中，使我们能够直接从单目视频优化 SCARF，而无需任何 3D 监督。混合建模使 SCARF 能够（i）通过改变身体姿势（包括手部关节和面部表情）为穿着衣服的身体化身制作动画，（ii）合成化身的新视图，以及（iii）在虚拟试穿中在化身之间转移衣服应用程序。我们证明了 SCARF 重建的服装比现有方法具有更高的视觉质量，服装随着身体姿势和体形的变化而变形，并且服装可以在不同主体的化身之间成功转移。代码和模型可在此 https 网址获得。
  - [MonoNHR：单眼神经人类渲染器](https://arxiv.org/abs/2210.00627) | [code]
    > 由于不可见区域中缺乏信息以及可见区域中像素的深度模糊性，现有的神经人类渲染方法难以处理单个图像输入。在这方面，我们提出了单目神经人类渲染器 (MonoNHR)，这是一种新颖的方法，可以仅在给定单个图像的情况下渲染任意人的鲁棒自由视点图像。 MonoNHR 是第一个（i）在单目设置中呈现在训练期间从未见过的人类受试者，以及（ii）在没有几何监督的情况下以弱监督方式训练的方法。首先，我们建议解开 3D 几何和纹理特征，并根据 3D 几何特征调整纹理推断。其次，我们引入了一个 Mesh Inpainter 模块，该模块利用人类结构先验（例如对称性）来修复被遮挡的部分。在 ZJU-MoCap、AIST 和 HUMBI 数据集上的实验表明，我们的方法明显优于最近适应单目情况的方法。
## Sep25 - Oct1, 2022
## Sep18 - Sep24, 2022
  - [FNeVR：面部动画的神经体积渲染](https://arxiv.org/abs/2209.10340) | [code]
    > 人脸动画是计算机视觉中最热门的话题之一，在生成模型的帮助下取得了可喜的成绩。然而，由于复杂的运动变形和复杂的面部细节建模，生成身份保持和照片般逼真的图像仍然是一个关键挑战。为了解决这些问题，我们提出了一个人脸神经体绘制 (FNeVR) 网络，以在一个统一的框架中充分挖掘 2D 运动扭曲和 3D 体绘制的潜力。在 FNeVR 中，我们设计了一个 3D 面部体积渲染 (FVR) 模块来增强图像渲染的面部细节。具体来说，我们首先使用精心设计的架构提取 3D 信息，然后引入正交自适应光线采样模块以实现高效渲染。我们还设计了一个轻量级的姿势编辑器，使 FNeVR 能够以简单而有效的方式编辑面部姿势。大量实验表明，我们的 FNeVR 在广泛使用的 Talking Head 基准测试中获得了最佳的整体质量和性能。
  - [通过神经动画网格进行人体性能建模和渲染](https://arxiv.org/abs/2209.08468) | [code]
    > 我们最近看到了照片真实人体建模和渲染的神经进步的巨大进步。但是，将它们集成到现有的基于网格的管道中以用于下游应用程序仍然具有挑战性。在本文中，我们提出了一种综合神经方法，用于从密集的多视图视频中对人类表演进行高质量的重建、压缩和渲染。我们的核心直觉是将传统的动画网格工作流程与新型高效神经技术联系起来。我们首先介绍了一种用于在几分钟内生成高质量表面的神经表面重建器。它将截断有符号距离场 (TSDF) 的隐式体积渲染与多分辨率哈希编码结合在一起。我们进一步提出了一种混合神经跟踪器来生成动画网格，它将显式非刚性跟踪与自监督框架中的隐式动态变形相结合。前者将粗略的变形提供回规范空间，而后者隐含的进一步使用我们的重构器中的 4D 哈希编码来预测位移。然后，我们讨论使用获得的动画网格的渲染方案，范围从动态纹理到各种带宽设置下的流明图渲染。为了在质量和带宽之间取得复杂的平衡，我们提出了一种分层解决方案，首先渲染覆盖表演者的 6 个虚拟视图，然后进行遮挡感知神经纹理混合。我们展示了我们的方法在各种基于网格的应用程序和各种平台上逼真的自由视图体验中的有效性，即通过移动 AR 将虚拟人类表演插入真实环境或使用 VR 耳机沉浸式观看才艺表演。
## Sep11 - Sep17, 2022
  - [3DMM-RF：用于 3D 人脸建模的卷积辐射场](https://arxiv.org/abs/2209.07366) | [code]
    > 面部 3D 可变形模型是具有无数应用的主要计算机视觉主题，并且在过去二十年中得到了高度优化。深度生成网络的巨大改进为改进此类模型创造了各种可能性，并引起了广泛的兴趣。此外，神经辐射领域的最新进展正在彻底改变已知场景的新视图合成。在这项工作中，我们提出了一个面部 3D 可变形模型，它利用了上述两者，并且可以准确地建模对象的身份、姿势和表情，并在任意光照下渲染它。这是通过利用强大的基于深度样式的生成器来克服神经辐射场的两个主要弱点，即它们的刚性和渲染速度来实现的。我们引入了一种基于样式的生成网络，它一次性合成所有且仅合成神经辐射场所需的渲染样本。我们创建了一个巨大的面部渲染标记合成数据集，并在这些数据上训练网络，以便它可以准确地建模和概括面部身份、姿势和外观。最后，我们证明该模型可以准确地拟合任意姿势和光照的“in-the-wild”人脸图像，提取人脸特征，并用于在可控条件下重新渲染人脸。
  - [明确可控的 3D 感知肖像生成](https://arxiv.org/abs/2209.05434) | [code]
    > 与成本高昂的传统头像创建流程相比，当代生成方法直接从照片中学习数据分布。虽然大量工作扩展了无条件生成模型并实现了一定程度的可控性，但确保多视图一致性仍然具有挑战性，尤其是在大姿势中。在这项工作中，我们提出了一个生成 3D 感知肖像的网络，同时可以根据有关姿势、身份、表情和照明的语义参数进行控制。我们的网络使用神经场景表示来建模 3D 感知肖像，其生成由支持显式控制的参数化面部模型引导。虽然通过对比具有部分不同属性的图像可以进一步增强潜在的解缠结，但在为表情制作动画时，非面部区域（例如头发和背景）仍然存在明显的不一致。我们通过提出一种体积混合策略来解决这个问题，在该策略中，我们通过混合动态和静态区域来形成复合输出，其中两部分从联合学习的语义场中分割出来。我们的方法在广泛的实验中优于现有技术，当从自由视角观看时，可以在自然光下生成逼真的肖像，并具有生动的表达。它还展示了对真实图像和域外数据的泛化能力，在实际应用中显示出巨大的前景。
## Previous weeks
## Sep4 - Sep10, 2022
  - [SIRA：来自单个图像的可重新点亮的头像](https://arxiv.org/abs/2209.03027) | [code]
    > 从单个图像中恢复人头的几何形状，同时分解材料和照明是一个严重不适定的问题，需要解决先验信息。基于 3D 可变形模型 (3DMM) 的方法，以及它们与可微渲染器的组合，已显示出可喜的结果。然而，3DMM 的表现力是有限的，它们通常会产生过度平滑且与身份无关的 3D 形状，仅限于面部区域。最近已经通过使用多层感知器参数化几何形状的神经场获得了高度准确的全头重建。这些表示的多功能性也被证明对于解开几何、材料和照明是有效的。然而，这些方法需要几十个输入图像。在本文中，我们介绍了 SIRA，这是一种从单个图像重建具有高保真几何形状和分解光和表面材料的人头头像的方法。我们的关键成分是两个基于神经场的数据驱动统计模型，可解决单视图 3D 表面重建和外观分解的模糊性。实验表明，SIRA 在 3D 头部重建中获得了最先进的结果，同时它成功地解开了全局照明、漫反射和镜面反射率。此外，我们的重建适用于基于物理的外观编辑和头部模型重新照明。
  - [MotionDiffuse：使用扩散模型的文本驱动人体运动生成](https://arxiv.org/abs/2208.15001) | [***``[code]``***](https://github.com/mingyuan-zhang/MotionDiffuse)
    > 人体运动建模对于许多现代图形应用程序很重要，这些应用程序通常需要专业技能。为了消除外行的技能障碍，最近的动作生成方法可以直接生成以自然语言为条件的人体动作。然而，通过各种文本输入实现多样化和细粒度的运动生成仍然具有挑战性。为了解决这个问题，我们提出了 MotionDiffuse，这是第一个基于扩散模型的文本驱动的运动生成框架，它展示了现有方法的几个所需属性。 1）概率映射。 MotionDiffuse 不是确定性的语言-运动映射，而是通过一系列注入变化的去噪步骤生成运动。 2）现实综合。 MotionDiffuse 擅长对复杂的数据分布进行建模并生成生动的运动序列。 3) 多级操作。 MotionDiffuse 响应身体部位的细粒度指令，以及带有时变文本提示的任意长度运动合成。我们的实验表明，MotionDiffuse 在文本驱动的运动生成和动作条件的运动生成方面具有令人信服的优势，从而优于现有的 SoTA 方法。定性分析进一步证明了 MotionDiffuse 对综合运动生成的可控性。主页：此 https 网址
## Aug28 - Sep3, 2022
  - [Dual-Space NeRF：在不同空间中学习动画化身和场景照明, 3DV2022](https://arxiv.org/abs/2208.14851) | [code]
    > 在规范空间中对人体进行建模是捕捉和动画的常见做法。但是当涉及到神经辐射场 (NeRF) 时，仅仅在标准空间中学习一个静态的 NeRF 是不够的，因为即使场景照明是恒定的，当人移动时身体的照明也会发生变化。以前的方法通过学习每帧嵌入来缓解光照的不一致性，但这种操作并不能推广到看不见的姿势。鉴于光照条件在世界空间中是静态的，而人体在规范空间中是一致的，我们提出了一种双空间 NeRF，它在两个独立的空间中使用两个 MLP 对场景光照和人体进行建模。为了弥合这两个空间，以前的方法主要依赖于线性混合蒙皮 (LBS) 算法。然而，动态神经领域的 LBS 的混合权重是难以处理的，因此通常用另一个 MLP 来记忆，这不能推广到新的姿势。尽管可以借用 SMPL 等参数网格的混合权重，但插值操作会引入更多伪影。在本文中，我们建议使用重心映射，它可以直接泛化到看不见的姿势，并且出人意料地取得了比具有神经混合权重的 LBS 更好的结果。 Human3.6M 和 ZJU-MoCap 数据集的定量和定性结果显示了我们方法的有效性。
  - [NerfCap：使用动态神经辐射场捕获人类表现, TVCG2022](https://ieeexplore.ieee.org/abstract/document/9870173) | [code]
    > 本文解决了从稀疏的多视图或单目视频中捕捉人类表演的挑战。给定表演者的模板网格，以前的方法通过将模板网格非刚性地注册到具有 2D 轮廓或密集光度对齐的图像来捕获人体运动。然而，详细的表面变形无法从轮廓中恢复，而光度对齐则受到视频外观变化引起的不稳定性的影响。为了解决这些问题，我们提出了 NerfCap，这是一种基于表演者动态神经辐射场 (NeRF) 表示的新型表演捕捉方法。具体来说，通过优化变形场和规范 NeRF 的外观模型，从模板几何初始化规范 NeRF 并注册到视频帧。为了捕捉大型身体运动和详细的表面变形，NerfCap 将线性混合蒙皮与嵌入式图形变形相结合。与受限于固定拓扑和纹理的基于网格的方法相比，NerfCap 能够灵活地捕捉视频中复杂的几何形状和外观变化，并合成更逼真的图像。此外，NerfCap 可以通过将合成视频与输入视频进行匹配，以自我监督的方式进行端到端的预训练。各种数据集的实验结果表明，NerfCap 在表面重建精度和新视图合成质量方面都优于先前的工作。
## Aug21 - Aug27, 2022
## Previous weeks
## Aug21 - Aug27, 2022
  - [神经小说演员：学习人类演员的广义动画神经表示](https://arxiv.org/abs/2208.11905) | [code]
    > 我们提出了一种新方法，用于从一组稀疏的多人多视图图像中学习广义的可动画神经人类表示。学习到的表示可用于从一组稀疏的相机中合成任意人的新颖视图图像，并使用用户的姿势控制进一步对它们进行动画处理。虽然现有方法可以推广到新人或使用用户控制合成动画，但它们都不能同时实现这两者。我们将这一成就归功于为共享的多人人体模型使用 3D 代理，并进一步将不同姿势的空间扭曲到共享的规范姿势空间，在该空间中，我们学习了一个神经领域并预测了人和与姿势相关的变形，以及从输入图像中提取的特征的外观。为了应对身体形状、姿势和服装变形的巨大变化的复杂性，我们设计了具有解开几何和外观的神经人体模型。此外，我们利用 3D 代理的空间点和表面点的图像特征来预测与人和姿势相关的属性。实验表明，我们的方法在这两项任务上都显着优于现有技术。视频和代码可在此 https 网址上找到。
## Aug14 - Aug20, 2022
  - [通过多平面图像的 3D 对象运动估计动态场景的时间视图合成, ISMAR2022](https://arxiv.org/abs/2208.09463) | [***``[code]``***](https://github.com/NagabhushanSN95/DeCOMPnet)
    > 在低计算设备上以图形方式渲染高帧率视频的挑战可以通过对未来帧的定期预测来解决，以增强虚拟现实应用程序中的用户体验。这是通过时间视图合成 (TVS) 的问题来研究的，其目标是在给定前一帧以及前一帧和下一帧的头部姿势的情况下预测视频的下一帧。在这项工作中，我们考虑了用户和对象都在移动的动态场景的 TVS。我们设计了一个框架，将运动解耦为用户和对象运动，以在预测下一帧的同时有效地使用可用的用户运动。我们通过隔离和估计过去帧中的 3D 对象运动然后外推来预测对象的运动。我们使用多平面图像 (MPI) 作为场景的 3D 表示，并将对象运动建模为 MPI 表示中对应点之间的 3D 位移。为了在估计运动时处理 MPI 中的稀疏性，我们结合了部分卷积和掩蔽相关层来估计对应点。然后将预测的对象运动与给定的用户或相机运动集成以生成下一帧。使用遮蔽填充模块，我们合成由于相机和物体运动而未覆盖的区域。我们为包含 800 个全高清分辨率视频的动态场景 TVS 开发了一个新的合成数据集。我们通过对我们的数据集和 MPI Sintel 数据集的实验表明，我们的模型优于文献中的所有竞争方法。
  - [LoRD：用于高保真动态人体建模的局部 4D 隐式表示, ECCV2022](https://arxiv.org/abs/2208.08622) | [code]
    > 4D 隐式表示的最新进展集中在使用低维潜在向量全局控制形状和运动，这容易丢失表面细节和累积跟踪误差。尽管许多深度局部表示已显示出可用于 3D 形状建模的有希望的结果，但它们的 4D 对应物尚不存在。在本文中，我们提出了一种新颖的用于动态服装人体的局部 4D 隐式表示，名为 LoRD，以填补这一空白，它兼具 4D 人体建模和局部表示的优点，并能够通过详细的表面变形进行高保真重建，例如衣服褶皱。特别是，我们的关键见解是鼓励网络学习局部部分级表示的潜在代码，能够解释局部几何和时间变形。为了在测试时进行推断，我们首先在每个时间步估计体内骨骼运动以跟踪局部部位，然后根据不同类型的观察数据通过自动解码优化每个部位的潜在代码。大量实验表明，该方法具有很强的表示 4D 人体的能力，并且在实际应用中优于最先进的方法，包括从稀疏点进行 4D 重建、非刚性深度融合，无论是定性还是定量。
  - [从单目视频中对动画 3D 人体进行神经捕获, ECCV2022](https://arxiv.org/abs/2208.08728) | [code]
    > 我们提出了一种从单目视频输入构建可动画 3D 人体表示的新颖范例，这样它就可以以任何看不见的姿势和视图进行渲染。我们的方法基于动态神经辐射场 (NeRF)，该动态神经辐射场 (NeRF) 由作为几何代理的基于网格的参数化 3D 人体模型装配。以前的方法通常依赖多视图视频或准确的 3D 几何信息作为附加输入；此外，大多数方法在推广到看不见的姿势时质量会下降。我们认为，泛化的关键是用于查询动态 NeRF 的良好输入嵌入：良好的输入嵌入应该定义全体积空间中的单射映射，由姿态变化下的表面网格变形引导。基于这一观察，我们建议嵌入输入查询及其与网格顶点上一组测地最近邻所跨越的局部表面区域的关系。通过包含位置和相对距离信息，我们的嵌入定义了距离保留的变形映射，并很好地推广到看不见的姿势。为了减少对额外输入的依赖，我们首先使用现成的工具初始化每帧 3D 网格，然后提出一个管道来联合优化 NeRF 并细化初始网格。大量实验表明，我们的方法可以在看不见的姿势和视图下合成合理的人类渲染结果。
## Aug7 - Aug13, 2022
  - [渐进式多尺度光场网络, 3DV2022](https://arxiv.org/abs/2208.06710) | [code]
    > 与图像集表示相比，神经表示在表示辐射和光场的能力方面显示出了巨大的希望，同时非常紧凑。然而，当前的表示不太适合流式传输，因为解码只能在单个细节级别上完成，并且需要下载整个神经网络模型。此外，高分辨率光场网络可能会出现闪烁和混叠，因为在没有适当过滤的情况下对神经网络进行采样。为了解决这些问题，我们提出了一个渐进式多尺度光场网络，它对具有多层次细节的光场进行编码。使用较少的神经网络权重对较低级别的细节进行编码，从而实现渐进式流传输并减少渲染时间。我们的渐进式多尺度光场网络通过在较低细节级别编码较小的抗锯齿表示来解决锯齿问题。此外，每个像素级别的细节使我们的表示能够支持抖动过渡和中心点渲染。
## Jul31 - Aug6, 2022
## Jul24 - Jul30, 2022
  - [神经链：从多视图图像中学习头发的几何形状和外观, ECCV2022](https://arxiv.org/pdf/2207.14067) | [***``[code]``***](https://radualexandru.github.io/neural_strands/)
    > 我们提出了 Neural Strands，这是一种新颖的学习框架，用于从多视图图像输入中对精确的头发几何形状和外观进行建模。学习的头发模型可以从具有高保真视图相关效果的任何视点实时渲染。与体积模型不同，我们的模型实现了直观的形状和样式控制。为了实现这些特性，我们提出了一种基于神经头皮纹理的新型头发表示，该神经头皮纹理对每个纹素位置的单个股线的几何形状和外观进行编码。此外，我们引入了一种基于学习发束光栅化的新型神经渲染框架。我们的神经渲染是精确的和抗锯齿的，使渲染视图一致且逼真。将外观与多视图几何先验相结合，我们首次实现了从多视图设置中联合学习外观和显式头发几何形状。我们展示了我们的方法在各种发型的保真度和效率方面的有效性。
## Previous weeks
  - [用于单目 4D 面部头像重建的动态神经辐射场, CVPR2021](https://gafniguy.github.io/4D-Facial-Avatars/) | [***``[code]``***](https://github.com/gafniguy/4D-Facial-Avatars)
    > 我们提出了用于模拟人脸外观和动态的动态神经辐射场。对说话的人进行数字建模和重建是各种应用程序的关键组成部分。特别是对于 AR 或 VR 中的远程呈现应用，需要忠实再现外观，包括新颖的视点或头部姿势。与显式建模几何和材料属性或纯粹基于图像的最先进方法相比，我们引入了基于场景表示网络的头部隐式表示。为了处理面部的动态，我们将场景表示网络与低维可变形模型相结合，该模型提供对姿势和表情的显式控制。我们使用体积渲染从这种混合表示中生成图像，并证明这种动态神经场景表示只能从单目输入数据中学习，而不需要专门的捕获设置。在我们的实验中，我们表明这种学习的体积表示允许生成照片般逼真的图像，其质量超过了基于视频的最先进的重演方法的质量。
  - [PVA：像素对齐的体积化身, CVPR2021](https://volumetric-avatars.github.io/) | [code]
    > 逼真的人头的采集和渲染是一个极具挑战性的研究问题，对于虚拟远程呈现特别重要。目前，最高质量是通过在多视图数据上以个人特定方式训练的体积方法实现的。与更简单的基于网格的模型相比，这些模型更好地表示精细结构，例如头发。体积模型通常使用全局代码来表示面部表情，以便它们可以由一小组动画参数驱动。虽然这样的架构实现了令人印象深刻的渲染质量，但它们不能轻易地扩展到多身份设置。在本文中，我们设计了一种新颖的方法，用于在仅给定少量输入的情况下预测人头的体积化身。我们通过一种新颖的参数化实现跨身份的泛化，该参数化将神经辐射场与直接从输入中提取的局部像素对齐特征相结合，从而避免了对非常深或复杂网络的需求。我们的方法仅基于光度重新渲染损失以端到端的方式进行训练，无需明确的 3D 监督。我们证明我们的方法在质量方面优于现有的现有技术，并且能够生成忠实的面部表情多身份设置。
  - [用于人体建模的动画神经辐射场, ICCV2021](https://zju3dv.github.io/animatable_nerf/) | [***``[code]``***](https://github.com/zju3dv/animatable_nerf)
    > 本文解决了从多视图视频中重建可动画人体模型的挑战。最近的一些工作提出将非刚性变形场景分解为规范神经辐射场和一组将观察空间点映射到规范空间的变形场，从而使他们能够从图像中学习动态场景。然而，它们将变形场表示为平移矢量场或 SE(3) 场，这使得优化受到高度约束。此外，这些表示不能由输入运动明确控制。相反，我们引入了神经混合权重场来产生变形场。基于骨架驱动的变形，混合权重场与 3D 人体骨骼一起使用，以生成观察到规范和规范到观察的对应关系。由于 3D 人体骨骼更易观察，它们可以规范变形场的学习。此外，学习到的混合权重场可以与输入的骨骼运动相结合，以生成新的变形场来为人体模型设置动画。实验表明，我们的方法明显优于最近的人类合成方法。该代码将在 https://zju3dv.github.io/animatable_nerf/ 上提供。
  - [神经演员：具有姿势控制的人类演员的神经自由视图合成, SIGSIGGRAPH Asia 2021](https://vcai.mpi-inf.mpg.de/projects/NeuralActor/) | [***``[code]``***](https://people.mpi-inf.mpg.de/~lliu/projects/NeuralActor/)
    > 我们提出了神经演员 (NA)，这是一种从任意视角和任意可控姿势下高质量合成人类的新方法。我们的方法建立在最近的神经场景表示和渲染工作之上，这些工作仅从 2D 图像中学习几何和外观的表示。虽然现有作品展示了令人信服的静态场景渲染和动态场景回放，但使用神经隐式方法对人类进行逼真的重建和渲染，特别是在用户控制的新姿势下，仍然很困难。为了解决这个问题，我们利用粗体模型作为代理将周围的 3D 空间展开为规范姿势。神经辐射场从多视图视频输入中学习规范空间中与姿势相关的几何变形以及与姿势和视图相关的外观效果。为了合成高保真动态几何和外观的新视图，我们利用在身体模型上定义的 2D 纹理图作为潜在变量来预测残余变形和动态外观。实验表明，我们的方法在回放和新颖的姿势合成方面取得了比现有技术更好的质量，甚至可以很好地推广到与训练姿势截然不同的新姿势。此外，我们的方法还支持合成结果的体形控制。
  - [神经体：具有结构化潜在代码的隐式神经表示，用于动态人类的新视图合成, CVPR2021](https://zju3dv.github.io/neuralbody/) | [***``[code]``***](https://github.com/zju3dv/neuralbody)
    > 本文解决了人类表演者从一组非常稀疏的摄像机视图中合成新颖视图的挑战。最近的一些工作表明，在给定密集输入视图的情况下，学习 3D 场景的隐式神经表示可以实现显着的视图合成质量。但是，如果视图高度稀疏，则表示学习将是不适定的。为了解决这个不适定问题，我们的关键思想是整合对视频帧的观察。为此，我们提出了神经体，这是一种新的人体表示，它假设在不同帧上学习到的神经表示共享同一组锚定到可变形网格的潜在代码，以便可以自然地整合跨帧的观察结果。可变形网格还为网络提供几何指导，以更有效地学习 3D 表示。为了评估我们的方法，我们创建了一个名为 ZJU-MoCap 的多视图数据集，用于捕捉具有复杂动作的表演者。 ZJU-MoCap 的实验表明，我们的方法在新颖的视图合成质量方面大大优于先前的工作。我们还展示了我们的方法从 People-Snapshot 数据集上的单目视频重建移动人物的能力。
  - [单张图像的人像神经辐射场](https://portrait-nerf.github.io/) | [code]
    > 我们提出了一种从单个爆头肖像估计神经辐射场 (NeRF) 的方法。虽然 NeRF 已经展示了高质量的视图合成，但它需要静态场景的多个图像，因此对于随意捕捉和移动主体是不切实际的。在这项工作中，我们建议使用使用灯光舞台肖像数据集的元学习框架来预训练多层感知器 (MLP) 的权重，该多层感知器隐含地对体积密度和颜色进行建模。为了提高对看不见的人脸的泛化能力，我们在由 3D 人脸可变形模型近似的规范坐标空间中训练 MLP。我们使用受控捕获对方法进行定量评估，并展示了对真实肖像图像的泛化性，显示出对最先进技术的有利结果。
  - [A-NeRF：通过神经渲染进行无表面人体 3D 姿势细化, NeurIPS2021](https://arxiv.org/abs/2102.06199) | [***``[code]``***](https://github.com/LemonATsu/A-NeRF)
    > 虽然深度学习使用前馈网络重塑了经典的运动捕捉管道，但需要生成模型通过迭代细化来恢复精细对齐。不幸的是，现有模型通常是在受控条件下手工制作或学习的，仅适用于有限的领域。我们提出了一种通过扩展神经辐射场 (NeRFs) 从未标记的单目视频中学习生成神经体模型的方法。我们为它们配备了骨架，以适用于时变和关节运动。一个关键的见解是，隐式模型需要与显式曲面模型中使用的正向运动学相反。我们的重新参数化定义了相对于身体部位姿势的空间潜在变量，从而克服了过度参数化的不适定逆运算。这使得从头开始学习体积身体形状和外观，同时共同改进关节姿势；输入视频上的所有外观、姿势或 3D 形状都没有地面实况标签。当用于新视图合成和动作捕捉时，我们的神经模型提高了不同数据集的准确性。项目网站：此 https 网址。
  - [学习动态人头的组成辐射场, CVPR2021(oral)](https://ziyanw1.github.io/hybrid_nerf/) | [code]
    > 动态人体的逼真渲染是远程呈现系统、虚拟购物、合成数据生成等的重要能力。最近，结合计算机图形学和机器学习技术的神经渲染方法已经创建了人类和物体的高保真模型。其中一些方法不会为可驱动的人体模型（神经体积）产生足够高保真度的结果，而其他方法则具有极长的渲染时间（NeRF）。我们提出了一种新颖的组合 3D 表示，它结合了以前最好的方法来产生更高分辨率和更快的结果。我们的表示通过将粗略的 3D 结构感知动画代码网格与连续学习的场景函数相结合，弥合了离散和连续体积表示之间的差距，该函数将每个位置及其相应的局部动画代码映射到其与视图相关的发射辐射和局部体积密度。可微分体渲染用于计算人头和上身的照片般逼真的新颖视图，并仅使用 2D 监督来端到端训练我们的新颖表示。此外，我们表明，学习到的动态辐射场可用于基于全局动画代码合成新的看不见的表情。我们的方法在合成动态人头和上半身的新视图方面取得了最先进的结果。
  - [使用分层神经表示的可编辑自由视点视频, SIGGRAPH2021](https://jiakai-zhang.github.io/st-nerf/) | [***``[code]``***](https://jiakai-zhang.github.io/st-nerf/#code)
    > 生成自由视点视频对于沉浸式 VR/AR 体验至关重要，但最近的神经学进展仍然缺乏编辑能力来操纵大型动态场景的视觉感知。为了填补这一空白，在本文中，我们提出了第一种仅使用稀疏的 16 个摄像头为大规模动态场景生成可编辑照片般逼真的自由视点视频的方法。我们方法的核心是一种新的分层神经表示，其中包括环境本身的每个动态实体都被制定为称为 ST-NeRF 的时空相干神经分层辐射表示。这种分层表示支持对动态场景的完全感知和真实操作，同时仍支持大范围的自由观看体验。在我们的 ST-NeRF 中，动态实体/层被表示为连续函数，以连续和自监督的方式实现动态实体的位置、变形以及外观的解耦。我们提出了一个场景解析 4D 标签映射跟踪来显式地解开空间信息，以及一个连续变形模块来隐式地解开时间运动。进一步引入了一种对象感知体绘制方案，用于重新组装所有神经层。我们采用了一种新颖的分层损失和运动感知光线采样策略，以实现对具有多个表演者的大型动态场景的有效训练，我们的框架进一步实现了各种编辑功能，即操纵规模和位置，复制或重新定时单个神经层在保持高度真实感的同时创造众多视觉效果。大量实验证明了我们的方法在为动态场景生成高质量、照片般逼真和可编辑的自由视点视频方面的有效性。
