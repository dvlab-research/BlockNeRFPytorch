
每周分类神经辐射场 - semantic ![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)
=====================================================================================================================================
## 按类别筛选: 
 [全部](../weekly_nerf_cn.md) | [动态](./dynamic.md) | [编辑](./editing.md) | [快速](./fast.md) | [泛化](./generalization.md) | [人体](./human.md) | [视频](./video.md) | [光照](./lighting.md) | [重建](./reconstruction.md) | [纹理](./texture.md) | [语义](./semantic.md) | [姿态-SLAM](./pose-slam.md) | [其他](./others.md) 
## Sep18 - Sep24, 2022
  - [NeRF-SOS：复杂场景上的任意视图自监督对象分割](https://zhiwenfan.github.io/NeRF-SOS/) | [***``[code]``***](https://github.com/VITA-Group/NeRF-SOS)
    > 神经体积表示已经显示了多层感知器 (MLP) 可以使用多视图校准图像进行优化以表示场景几何和外观的潜力，而无需明确的 3D 监督。对象分割可以基于学习到的辐射场丰富许多下游应用。然而，引入手工分割来定义复杂现实世界场景中的感兴趣区域并非易事且成本高昂，因为它需要每个视图注释。本文针对复杂的现实世界场景使用 NeRF 进行对象分割的自监督学习探索。我们的框架称为带有自监督对象分割 NeRF-SOS 的 NeRF，它结合了对象分割和神经辐射场来分割场景中任何视图中的对象。通过在外观和几何级别上提出一种新颖的协作对比损失，NeRF-SOS 鼓励 NeRF 模型从其密度场和自我监督的预训练 2D 视觉特征中提取紧凑的几何感知分割簇。自监督对象分割框架可以应用于各种 NeRF 模型，这些模型既可以产生逼真的渲染结果，又可以在室内和室外场景中提供令人信服的分割图。 LLFF、Tank & Temple 和 BlendedMVS 数据集的广泛结果验证了 NeRF-SOS 的有效性。它始终超越其他基于 2D 的自我监督基线，并预测比现有监督对应物更精细的语义掩码。请参阅我们项目页面上的视频以获取更多详细信息：此 https URL。
  - [医学影像分割的隐式神经表示, MICCAI2022](https://link.springer.com/chapter/10.1007/978-3-031-16443-9_42) | [code]
    > 医学成像中的 3D 信号（例如 CT 扫描）通常被参数化为体素的离散网格。例如，现有的最先进的器官分割方法学习离散的分割图。不幸的是，这些方法的内存需求随着空间分辨率的增加而呈立方增长，这使得它们不适合处理高分辨率扫描。为了克服这个问题，我们设计了一个隐式器官分割网络 (IOSNet)，它利用连续的隐式神经表示并具有几个有用的属性。首先，IOSNet 解码器内存大致恒定且独立于空间分辨率，因为它将分割图参数化为连续函数。其次，IOSNet 的收敛速度比基于离散体素的方法快得多，因为它能够准确地分割器官而不受器官大小的影响，从而在不需要任何辅助技巧的情况下缓解大小不平衡问题。第三，由于其连续学习表示，IOSNet 自然支持超分辨率（即在推理过程中以任意分辨率采样）。此外，尽管使用了一个简单的轻量级解码器，IOSNet 始终优于离散专业分割架构 UNet。因此，我们的方法表明隐式神经表示非常适合医学成像应用，尤其是处理高分辨率 3D 医学扫描。
## Sep11 - Sep17, 2022
## Previous weeks
## Sep4 - Sep10, 2022
  - [神经特征融合领域：自监督 2D 图像表示的 3D 蒸馏, 3DV2022(oral)](https://arxiv.org/abs/2209.03494) | [***``[code]``***](https://github.com/dichotomies/N3F)
    > 我们提出了神经特征融合场 (N3F)，这是一种在将密集 2D 图像特征提取器应用于可重构为 3D 场景的多张图像分析时改进密集 2D 图像特征提取器的方法。给定一个图像特征提取器，例如使用自我监督进行预训练，N3F 使用它作为教师来学习在 3D 空间中定义的学生网络。 3D 学生网络类似于提取所述特征的神经辐射场，并且可以使用通常的可微渲染机器进行训练。因此，N3F 很容易适用于大多数神经渲染公式，包括 vanilla NeRF 及其对复杂动态场景的扩展。我们表明，我们的方法不仅能够在不使用手动标签的情况下在特定场景的神经领域的上下文中实现语义理解，而且在自我监督的 2D 基线上持续改进。这通过考虑不同序列中的各种任务（例如 2D 对象检索、3D 分割和场景编辑）来证明，包括 EPIC-KITCHENS 基准测试中的以自我为中心的长视频。
## Aug28 - Sep3, 2022
## Aug21 - Aug27, 2022
## Previous weeks
## Aug21 - Aug27, 2022
  - [DreamBooth：为主题驱动生成微调文本到图像的扩散模型](https://dreambooth.github.io/) | [code]
    > 大型文本到图像模型在人工智能的演进中实现了显着的飞跃，能够从给定的文本提示中对图像进行高质量和多样化的合成。然而，这些模型缺乏模仿给定参考集中对象的外观并在不同上下文中合成它们的新颖再现的能力。在这项工作中，我们提出了一种“个性化”文本到图像扩散模型的新方法（专门针对用户的需求）。给定主题的几张图像作为输入，我们微调预训练的文本到图像模型（Imagen，尽管我们的方法不限于特定模型），以便它学会将唯一标识符与该特定主题绑定.一旦对象被嵌入模型的输出域中，唯一标识符就可以用于合成在不同场景中情境化的对象的完全新颖的真实感图像。通过利用嵌入在模型中的语义先验和新的自生类特定先验保存损失，我们的技术能够在参考图像中没有出现的不同场景、姿势、视图和照明条件下合成主体。我们将我们的技术应用于几个以前无懈可击的任务，包括主题重新上下文化、文本引导视图合成、外观修改和艺术渲染（同时保留主题的关键特征）。项目页面：此 https 网址
## Aug14 - Aug20, 2022
## Aug7 - Aug13, 2022
## Jul31 - Aug6, 2022
  - [NeSF: 用于 3D 场景的可概括语义分割的神经语义场](https://research.google/pubs/pub51563/) | [code]
    > 我们提出了 NeSF，一种从预训练的密度场和稀疏的 2D 语义监督产生 3D 语义场的方法。我们的方法通过利用将 3D 信息存储在神经域中的神经表示来避开传统的场景表示。尽管仅由 2D 信号监督，我们的方法能够从新颖的相机姿势生成 3D 一致的语义图，并且可以在任意 3D 点进行查询。值得注意的是，NeSF 与任何产生密度场的方法兼容，并且随着预训练密度场质量的提高，其准确性也会提高。我们的实证分析证明了在令人信服的合成场景上与竞争性 2D 和 3D 语义分割基线相当的质量，同时还提供了现有方法无法提供的功能。
## Jul24 - Jul30, 2022
## Previous weeks
  - [节食 NeRF：语义一致的 Few-Shot 视图合成, ICCV2021](https://www.ajayj.com/dietnerf) | [***``[code]``***](https://github.com/ajayjain/DietNeRF)
    > 我们提出了 DietNeRF，一种从几张图像估计的 3D 神经场景表示。神经辐射场 (NeRF) 通过多视图一致性学习场景的连续体积表示，并且可以通过光线投射从新颖的视点进行渲染。虽然 NeRF 在给定许多图像的情况下具有令人印象深刻的重建几何和精细细节的能力，对于具有挑战性的 360° 场景最多可重建 100 个，但当只有少数输入视图可用时，它通常会为其图像重建目标找到退化的解决方案。为了提高few-shot质量，我们提出了DietNeRF。我们引入了一种辅助语义一致性损失，它鼓励以新颖的姿势进行逼真的渲染。 DietNeRF 在单个场景上进行训练，以 (1) 从相同的姿势正确渲染给定的输入视图，以及 (2) 在不同的随机姿势中匹配高级语义属性。我们的语义损失使我们能够从任意姿势监督 DietNeRF。我们使用预训练的视觉编码器提取这些语义，例如 CLIP，这是一种视觉转换器，通过自然语言监督从网络挖掘出的数亿张不同的单视图 2D 照片进行训练。在实验中，DietNeRF 在从头开始学习时提高了少镜头视图合成的感知质量，在多视图数据集上进行预训练时，可以用少至一张观察到的图像渲染新视图，并生成完全未观察到的区域的合理完成。
  - [物体辐射场的无监督发现, ICLR2022](https://arxiv.org/abs/2107.07905) | [code]
    > 我们研究从单个图像推断以对象为中心的场景表示的问题，旨在推导出解释图像形成过程的表示，捕捉场景的 3D 性质，并且在没有监督的情况下学习。由于将复杂的 3D 到 2D 图像形成过程集成到强大的推理方案（如深度网络）中存在根本性挑战，大多数现有的场景分解方法都缺乏这些特征中的一个或多个。在本文中，我们提出了对象辐射场 (uORF) 的无监督发现，将神经 3D 场景表示和渲染的最新进展与深度推理网络相结合，用于无监督 3D 场景分解。在没有注释的多视图 RGB 图像上进行训练，uORF 学习从单个图像分解具有不同纹理背景的复杂场景。我们展示了 uORF 在无监督 3D 场景分割、新视图合成和三个数据集上的场景编辑方面表现良好。
  - [使用隐式场景表示进行就地场景标记和理解, ICCV2021(oral)](https://shuaifengzhi.com/Semantic-NeRF/) | [***``[code]``***](https://github.com/Harry-Zhi/semantic_nerf/)
    > 语义标签与几何和辐射重建高度相关，因为具有相似形状和外观的场景实体更有可能来自相似的类别。最近的隐式神经重建技术很有吸引力，因为它们不需要事先的训练数据，但同样的完全自我监督的方法对于语义来说是不可能的，因为标签是人类定义的属性。
