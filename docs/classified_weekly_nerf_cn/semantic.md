
每周分类神经辐射场 - semantic ![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)
=====================================================================================================================================
## 按类别筛选: 
 [全部](../weekly_nerf_cn.md) | [动态](./dynamic.md) | [编辑](./editing.md) | [快速](./fast.md) | [泛化](./generalization.md) | [人体](./human.md) | [视频](./video.md) | [光照](./lighting.md) | [重建](./reconstruction.md) | [纹理](./texture.md) | [语义](./semantic.md) | [姿态-SLAM](./pose-slam.md) | [其他](./others.md) 
## Dec27 - Jan3, 2023
## Dec25 - Dec31, 2022
## Dec18 - Dec24, 2022
  - [iLabel：揭示神经领域中的对象, RAL2022](https://ieeexplore.ieee.org/abstract/document/9996585) | [code]
    > 经过自我监督训练以有效表示 3D 场景的几何形状和颜色的神经场往往会自动将其分解为连贯且准确的类似物体的区域，这些区域可以通过稀疏标记交互来揭示以产生 3D 语义场景分割。 我们的实时 iLabel 系统从手持式 RGB-D 相机获取输入，需要零先验训练数据，并以“开放集”方式工作，语义类别由用户即时定义。 iLabel 的底层模型是一个简单的多层感知器 (MLP)，从头开始训练以学习单个 3D 场景的神经表示。 该模型不断更新并实时可视化，使用户能够专注于交互以实现极其高效的语义分割。 一个房间规模的场景可以准确地标记为 10 多个语义类别，只需大约 100 次点击，耗时不到 5 分钟。 定量标记的准确性随着点击次数的增加而显着增加，并迅速超越标准的预训练语义分割方法。 我们还展示了 iLabel 的分层标签变体和“免提”模式，用户只需为自动生成的位置提供标签名称。
## Dec11 - Dec17, 2022
## Dec4 - Dec10, 2022
## Nov27 - Dec3, 2022
## Nov20 - Nov26, 2022
  - [NeRF-RPN：NeRF 中对象检测的通用框架](https://arxiv.org/abs/2211.11646) | [code]
    > 本文介绍了第一个重要的目标检测框架 NeRF-RPN，它直接在 NeRF 上运行。给定预训练的 NeRF 模型，NeRF-RPN 旨在检测场景中对象的所有边界框。通过利用包含多尺度 3D 神经体积特征的新型体素表示，我们证明可以直接回归 NeRF 中对象的 3D 边界框，而无需在任何视点渲染 NeRF。 NeRF-RPN 是一个通用框架，可用于检测没有类标签的对象。我们用各种骨干架构、RPN 头部设计和损失函数对 NeRF-RPN 进行了实验。所有这些都可以以端到端的方式进行训练，以估计高质量的 3D 边界框。为了促进 NeRF 对象检测的未来研究，我们构建了一个新的基准数据集，其中包含经过仔细标记和清理的合成数据和真实数据。请单击此 https URL 以可视化我们的 NeRF-RPN 的 3D 区域提案。代码和数据集将可用。
  - [SegNeRF：具有神经辐射场的 3D 部分分割](https://arxiv.org/abs/2211.11215) | [code]
    > 神经辐射场 (NeRF) 的最新进展在生成任务（如新视图合成和 3D 重建）方面表现出色。基于神经辐射场的方法能够通过完全依赖姿势图像隐含地表示 3D 世界。然而，它们很少在 3D 零件分割等判别任务领域进行探索。在这项工作中，我们试图通过提出 SegNeRF 来弥合这一差距：一种将语义场与通常的辐射场集成在一起的神经场表示。 SegNeRF 继承了之前作品执行新视图合成和 3D 重建的能力，并能够从少量图像中进行 3D 部分分割。我们在 PartNet 上进行的广泛实验表明，SegNeRF 能够同时预测来自摆姿势图像的几何形状、外观和语义信息，即使对于看不见的物体也是如此。预测的语义场允许 SegNeRF 实现 2D 新视图分割的平均 mIoU 为 30.30%，3D 部分分割的平均 mIoU 为 37.46%，与基于点的方法相比，仅使用少量姿势图像具有竞争力的性能。此外，SegNeRF 能够从野外拍摄的物体的单个图像及其相应的部分分割生成显式 3D 模型。
## Nov13 - Nov19, 2022
## Nov6 - Nov12, 2022
## Oct30 - Nov5, 2022
## Oct23 - Oct29, 2022
## Oct16 - Oct22, 2022
## Oct9 - Oct15, 2022
  - [CLIP-Fields：机器人记忆的弱监督语义场](https://mahis.life/clip-fields/) | [code]
    > 我们提出了 CLIP-Fields，这是一种隐式场景模型，可以在没有直接人工监督的情况下进行训练。该模型学习从空间位置到语义嵌入向量的映射。然后，该映射可用于各种任务，例如分割、实例识别、空间语义搜索和视图定位。最重要的是，映射可以通过仅来自网络图像和网络文本训练模型（如 CLIP、Detic 和 Sentence-BERT）的监督进行训练。与 Mask-RCNN 之类的基线相比，我们的方法在 HM3D 数据集上的少量实例识别或语义分割方面表现优于仅一小部分示例。最后，我们展示了使用 CLIP-Fields 作为场景记忆，机器人可以在现实环境中执行语义导航。我们的代码和演示可在此处获得：https://mahis.life/clip-fields/
## Oct2 - Oct8, 2022
  - [ViewFool：评估视觉识别对对抗性观点的鲁棒性, NeurIPS2022](https://arxiv.org/abs/2210.03895) | [code]
    > 最近的研究表明，视觉识别模型对分布变化缺乏鲁棒性。然而，目前的工作主要考虑模型对 2D 图像转换的鲁棒性，而较少探索 3D 世界中的视点变化。一般来说，视点变化在各种实际应用（例如自动驾驶）中很普遍，因此评估视点鲁棒性势在必行。在本文中，我们提出了一种称为 ViewFool 的新方法来寻找误导视觉识别模型的对抗性视点。通过将现实世界中的物体编码为神经辐射场 (NeRF)，ViewFool 在熵正则化器下表征了不同对抗视点的分布，这有助于处理真实相机姿态的波动并减轻真实物体与其神经之间的现实差距申述。实验验证了常见的图像分类器极易受到生成的对抗性视点的影响，这也表现出很高的跨模型可迁移性。基于 ViewFool，我们引入了 ImageNet-V，这是一种新的分布外数据集，用于对图像分类器的视点鲁棒性进行基准测试。对具有不同架构、目标函数和数据增强的 40 个分类器的评估结果显示，在 ImageNet-V 上进行测试时模型性能显着下降，这为利用 ViewFool 作为一种有效的数据增强策略来提高视点鲁棒性提供了可能性。
  - [用于将图像转换为任意比例的简单插件](https://arxiv.org/abs/2210.03417) | [code]
    > 现有的超分辨率模型通常专门针对一个尺度，从根本上限制了它们在实际场景中的使用。在本文中，我们的目标是开发一个通用插件，可以插入到现有的超分辨率模型中，方便地增强它们对任意分辨率图像缩放的能力，因此被称为 ARIS。我们做出以下贡献：（i）我们提出了一个基于transformer的插件模块，它使用空间坐标作为查询，通过交叉注意迭代地关注低分辨率图像特征，并为查询的空间位置输出视觉特征，类似于图像的隐式表示； (ii) 我们引入了一种新颖的自我监督训练方案，该方案利用一致性约束来有效地增强模型将图像上采样到看不见的尺度的能力，即不提供真实的高分辨率图像； (iii) 在不失一般性的情况下，我们将提出的 ARIS 插件模块注入到多个现有模型中，即 IPT、SwinIR 和 HAT，表明生成的模型不仅可以在固定比例因子上保持其原始性能，而且可以外推到看不见的模型尺度，在标准基准上大大优于现有的任何尺度超分辨率模型，例如Urban100、DIV2K等
  - [用于实时、开放集场景理解的特征真实神经融合](https://arxiv.org/abs/2210.03043) | [code]
    > 机器人的一般场景理解需要灵活的语义表示，以便可以识别、分割和分组训练时可能不知道的新物体和结构。我们提出了一种算法，该算法在实时 SLAM 期间将来自标准预训练网络的一般学习特征融合到高效的 3D 几何神经场表示中。融合的 3D 特征图继承了神经域几何表示的连贯性。这意味着在运行时交互的少量人类标签使对象甚至对象的一部分能够以开放集的方式稳健而准确地分割。
  - [神经匹配字段：视觉对应匹配字段的隐式表示, NeurIPS2022](https://arxiv.org/abs/2210.02689) | [***``[code]``***](https://ku-cvlab.github.io/NeMF/)
    > 现有的语义对应管道通常包括提取高级语义特征以保持对类内变化和背景杂波的不变性。然而，这种架构不可避免地会导致低分辨率匹配字段，该字段还需要临时插值过程作为将其转换为高分辨率的后处理，这肯定会限制匹配结果的整体性能。为了克服这个问题，受隐式神经表示最近成功的启发，我们提出了一种新的语义对应方法，称为神经匹配场 (NeMF)。然而，4D 匹配场的复杂性和高维性是主要障碍，我们提出了一种成本嵌入网络来处理粗略的成本量，以作为通过以下全连接网络建立高精度匹配场的指导。然而，学习高维匹配字段仍然具有挑战性，主要是由于计算复杂性，因为简单的穷举推理需要从 4D 空间中的所有像素中查询以推断像素级对应关系。为了克服这个问题，我们提出了充分的训练和推理程序，在训练阶段，我们随机抽取匹配的候选者，在推理阶段，我们在测试时迭代地执行基于 PatchMatch 的推理和坐标优化。通过这些结合，在语义对应的几个标准基准上获得了具有竞争力的结果。此 https URL 提供了代码和预训练的权重。
## Sep25 - Oct1, 2022
  - [了解体素网格 NeRF 模型的纯 CLIP 指导](https://arxiv.org/abs/2209.15172) | [code]
    > 我们使用 CLIP 探索文本到 3D 对象生成的任务。具体来说，我们在不访问任何数据集的情况下使用 CLIP 进行指导，我们将这种设置称为纯 CLIP 指导。虽然之前的工作采用了这种设置，但没有系统研究防止 CLIP 中产生对抗性生成的机制。我们说明了不同的基于图像的增强如何防止对抗性生成问题，以及生成的结果如何受到影响。我们测试了不同的 CLIP 模型架构，并表明集成不同的模型进行指导可以防止更大模型中的对抗性生成并产生更清晰的结果。此外，我们实现了一个隐式体素网格模型，以展示神经网络如何提供额外的正则化层，从而产生更好的几何结构和生成对象的连贯性。与之前的工作相比，我们以更高的记忆效率和更快的训练速度获得了更连贯的结果。
  - [具有三层采样和全景表示的城市级增量神经映射](https://arxiv.org/abs/2209.14072) | [code]
    > 神经隐式表示最近引起了机器人界的广泛关注，因为它们具有表现力、连续性和紧凑性。然而，基于稀疏 LiDAR 输入的城市规模增量隐式密集映射仍然是一个未充分探索的挑战。为此，我们成功构建了第一个具有全景表示的城市规模增量神经映射系统，该系统由环境级和实例级建模组成。给定一个稀疏的 LiDAR 点云流，它维护一个动态生成模型，将 3D 坐标映射到有符号距离场 (SDF) 值。为了解决在城市尺度空间中表示不同层次几何信息的困难，我们提出了一种定制的三层采样策略来动态采样全局、局部和近地表域。同时，为了实现高保真映射，引入了特定类别的先验以更好地对几何细节进行建模，从而实现全景表示。我们评估了公共 SemanticKITTI 数据集，并使用定量和定性结果证明了新提出的三层采样策略和全景表示的重要性。代码和数据将公开。
  - [360FusionNeRF：具有联合引导的全景神经辐射场](https://arxiv.org/abs/2209.14265) | [code]
    > 我们提出了一种基于神经辐射场 (NeRF) 从单个 360 度全景图像合成新视图的方法。类似设置中的先前研究依赖于多层感知的邻域插值能力来完成由遮挡引起的缺失区域，这导致其预测中的伪影。我们提出了 360FusionNeRF，这是一个半监督学习框架，我们在其中引入几何监督和语义一致性来指导渐进式训练过程。首先，将输入图像重新投影到 360 度图像，并在其他相机位置提取辅助深度图。除了 NeRF 颜色指导之外，深度监督还改进了合成视图的几何形状。此外，我们引入了语义一致性损失，鼓励对新视图进行逼真的渲染。我们使用预训练的视觉编码器（例如 CLIP）提取这些语义特征，CLIP 是一种视觉转换器，通过自然语言监督从网络挖掘出的数亿张不同的 2D 照片进行训练。实验表明，我们提出的方法可以在保留场景特征的同时产生未观察到的区域的合理完成。在跨各种场景进行训练时，360FusionNeRF 在转移到合成 Structured3D 数据集（PSNR~5%，SSIM~3% LPIPS~13%）、真实世界的 Matterport3D 数据集（PSNR~3%）时始终保持最先进的性能, SSIM~3% LPIPS~9%) 和 Replica360 数据集 (PSNR~8%, SSIM~2% LPIPS~18%)。
  - [烘焙特征：通过渲染特征图加速体积分割](https://arxiv.org/abs/2209.12744) | [code]
    > 最近提出了一些方法，即仅使用彩色图像和专家监督以稀疏语义注释像素的形式将 3D 体积密集分割成类。虽然令人印象深刻，但这些方法仍然需要相对大量的监督，并且在实践中分割对象可能需要几分钟。这样的系统通常只优化它们在它们适合的特定场景上的表示，而不利用来自先前看到的图像的任何先验信息。在本文中，我们建议使用在现有大型数据集上训练的模型提取的特征来提高分割性能。我们通过体积渲染特征图并监督从每个输入图像中提取的特征，将这种特征表示烘焙到神经辐射场 (NeRF) 中。我们表明，通过将这种表示烘焙到 NeRF 中，我们使后续的分类任务变得更加容易。我们的实验表明，与现有方法相比，我们的方法在广泛的场景中以更少的语义注释实现了更高的分割精度。
## Sep18 - Sep24, 2022
  - [NeRF-SOS：复杂场景上的任意视图自监督对象分割](https://zhiwenfan.github.io/NeRF-SOS/) | [***``[code]``***](https://github.com/VITA-Group/NeRF-SOS)
    > 神经体积表示已经显示了多层感知器 (MLP) 可以使用多视图校准图像进行优化以表示场景几何和外观的潜力，而无需明确的 3D 监督。对象分割可以基于学习到的辐射场丰富许多下游应用。然而，引入手工分割来定义复杂现实世界场景中的感兴趣区域并非易事且成本高昂，因为它需要每个视图注释。本文针对复杂的现实世界场景使用 NeRF 进行对象分割的自监督学习探索。我们的框架称为带有自监督对象分割 NeRF-SOS 的 NeRF，它结合了对象分割和神经辐射场来分割场景中任何视图中的对象。通过在外观和几何级别上提出一种新颖的协作对比损失，NeRF-SOS 鼓励 NeRF 模型从其密度场和自我监督的预训练 2D 视觉特征中提取紧凑的几何感知分割簇。自监督对象分割框架可以应用于各种 NeRF 模型，这些模型既可以产生逼真的渲染结果，又可以在室内和室外场景中提供令人信服的分割图。 LLFF、Tank & Temple 和 BlendedMVS 数据集的广泛结果验证了 NeRF-SOS 的有效性。它始终超越其他基于 2D 的自我监督基线，并预测比现有监督对应物更精细的语义掩码。请参阅我们项目页面上的视频以获取更多详细信息：此 https URL。
  - [医学影像分割的隐式神经表示, MICCAI2022](https://link.springer.com/chapter/10.1007/978-3-031-16443-9_42) | [code]
    > 医学成像中的 3D 信号（例如 CT 扫描）通常被参数化为体素的离散网格。例如，现有的最先进的器官分割方法学习离散的分割图。不幸的是，这些方法的内存需求随着空间分辨率的增加而呈立方增长，这使得它们不适合处理高分辨率扫描。为了克服这个问题，我们设计了一个隐式器官分割网络 (IOSNet)，它利用连续的隐式神经表示并具有几个有用的属性。首先，IOSNet 解码器内存大致恒定且独立于空间分辨率，因为它将分割图参数化为连续函数。其次，IOSNet 的收敛速度比基于离散体素的方法快得多，因为它能够准确地分割器官而不受器官大小的影响，从而在不需要任何辅助技巧的情况下缓解大小不平衡问题。第三，由于其连续学习表示，IOSNet 自然支持超分辨率（即在推理过程中以任意分辨率采样）。此外，尽管使用了一个简单的轻量级解码器，IOSNet 始终优于离散专业分割架构 UNet。因此，我们的方法表明隐式神经表示非常适合医学成像应用，尤其是处理高分辨率 3D 医学扫描。
## Sep11 - Sep17, 2022
## Sep4 - Sep10, 2022
  - [神经特征融合领域：自监督 2D 图像表示的 3D 蒸馏, 3DV2022(oral)](https://arxiv.org/abs/2209.03494) | [***``[code]``***](https://github.com/dichotomies/N3F)
    > 我们提出了神经特征融合场 (N3F)，这是一种在将密集 2D 图像特征提取器应用于可重构为 3D 场景的多张图像分析时改进密集 2D 图像特征提取器的方法。给定一个图像特征提取器，例如使用自我监督进行预训练，N3F 使用它作为教师来学习在 3D 空间中定义的学生网络。 3D 学生网络类似于提取所述特征的神经辐射场，并且可以使用通常的可微渲染机器进行训练。因此，N3F 很容易适用于大多数神经渲染公式，包括 vanilla NeRF 及其对复杂动态场景的扩展。我们表明，我们的方法不仅能够在不使用手动标签的情况下在特定场景的神经领域的上下文中实现语义理解，而且在自我监督的 2D 基线上持续改进。这通过考虑不同序列中的各种任务（例如 2D 对象检索、3D 分割和场景编辑）来证明，包括 EPIC-KITCHENS 基准测试中的以自我为中心的长视频。
## Aug28 - Sep3, 2022
## Aug21 - Aug27, 2022
  - [DreamBooth：为主题驱动生成微调文本到图像的扩散模型](https://dreambooth.github.io/) | [code]
    > 大型文本到图像模型在人工智能的演进中实现了显着的飞跃，能够从给定的文本提示中对图像进行高质量和多样化的合成。然而，这些模型缺乏模仿给定参考集中对象的外观并在不同上下文中合成它们的新颖再现的能力。在这项工作中，我们提出了一种“个性化”文本到图像扩散模型的新方法（专门针对用户的需求）。给定主题的几张图像作为输入，我们微调预训练的文本到图像模型（Imagen，尽管我们的方法不限于特定模型），以便它学会将唯一标识符与该特定主题绑定.一旦对象被嵌入模型的输出域中，唯一标识符就可以用于合成在不同场景中情境化的对象的完全新颖的真实感图像。通过利用嵌入在模型中的语义先验和新的自生类特定先验保存损失，我们的技术能够在参考图像中没有出现的不同场景、姿势、视图和照明条件下合成主体。我们将我们的技术应用于几个以前无懈可击的任务，包括主题重新上下文化、文本引导视图合成、外观修改和艺术渲染（同时保留主题的关键特征）。项目页面：此 https 网址
## Aug14 - Aug20, 2022
## Aug7 - Aug13, 2022
## Jul31 - Aug6, 2022
  - [NeSF: 用于 3D 场景的可概括语义分割的神经语义场](https://research.google/pubs/pub51563/) | [code]
    > 我们提出了 NeSF，一种从预训练的密度场和稀疏的 2D 语义监督产生 3D 语义场的方法。我们的方法通过利用将 3D 信息存储在神经域中的神经表示来避开传统的场景表示。尽管仅由 2D 信号监督，我们的方法能够从新颖的相机姿势生成 3D 一致的语义图，并且可以在任意 3D 点进行查询。值得注意的是，NeSF 与任何产生密度场的方法兼容，并且随着预训练密度场质量的提高，其准确性也会提高。我们的实证分析证明了在令人信服的合成场景上与竞争性 2D 和 3D 语义分割基线相当的质量，同时还提供了现有方法无法提供的功能。
## Jul24 - Jul30, 2022
## Previous weeks
  - [节食 NeRF：语义一致的 Few-Shot 视图合成, ICCV2021](https://www.ajayj.com/dietnerf) | [***``[code]``***](https://github.com/ajayjain/DietNeRF)
    > 我们提出了 DietNeRF，一种从几张图像估计的 3D 神经场景表示。神经辐射场 (NeRF) 通过多视图一致性学习场景的连续体积表示，并且可以通过光线投射从新颖的视点进行渲染。虽然 NeRF 在给定许多图像的情况下具有令人印象深刻的重建几何和精细细节的能力，对于具有挑战性的 360° 场景最多可重建 100 个，但当只有少数输入视图可用时，它通常会为其图像重建目标找到退化的解决方案。为了提高few-shot质量，我们提出了DietNeRF。我们引入了一种辅助语义一致性损失，它鼓励以新颖的姿势进行逼真的渲染。 DietNeRF 在单个场景上进行训练，以 (1) 从相同的姿势正确渲染给定的输入视图，以及 (2) 在不同的随机姿势中匹配高级语义属性。我们的语义损失使我们能够从任意姿势监督 DietNeRF。我们使用预训练的视觉编码器提取这些语义，例如 CLIP，这是一种视觉转换器，通过自然语言监督从网络挖掘出的数亿张不同的单视图 2D 照片进行训练。在实验中，DietNeRF 在从头开始学习时提高了少镜头视图合成的感知质量，在多视图数据集上进行预训练时，可以用少至一张观察到的图像渲染新视图，并生成完全未观察到的区域的合理完成。
  - [物体辐射场的无监督发现, ICLR2022](https://arxiv.org/abs/2107.07905) | [code]
    > 我们研究从单个图像推断以对象为中心的场景表示的问题，旨在推导出解释图像形成过程的表示，捕捉场景的 3D 性质，并且在没有监督的情况下学习。由于将复杂的 3D 到 2D 图像形成过程集成到强大的推理方案（如深度网络）中存在根本性挑战，大多数现有的场景分解方法都缺乏这些特征中的一个或多个。在本文中，我们提出了对象辐射场 (uORF) 的无监督发现，将神经 3D 场景表示和渲染的最新进展与深度推理网络相结合，用于无监督 3D 场景分解。在没有注释的多视图 RGB 图像上进行训练，uORF 学习从单个图像分解具有不同纹理背景的复杂场景。我们展示了 uORF 在无监督 3D 场景分割、新视图合成和三个数据集上的场景编辑方面表现良好。
  - [使用隐式场景表示进行就地场景标记和理解, ICCV2021(oral)](https://shuaifengzhi.com/Semantic-NeRF/) | [***``[code]``***](https://github.com/Harry-Zhi/semantic_nerf/)
    > 语义标签与几何和辐射重建高度相关，因为具有相似形状和外观的场景实体更有可能来自相似的类别。最近的隐式神经重建技术很有吸引力，因为它们不需要事先的训练数据，但同样的完全自我监督的方法对于语义来说是不可能的，因为标签是人类定义的属性。
