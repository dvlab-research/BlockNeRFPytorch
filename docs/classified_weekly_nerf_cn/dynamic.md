
每周分类神经辐射场 - dynamic ![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)
====================================================================================================================================
## 按类别筛选: 
 [全部](../weekly_nerf_cn.md) | [动态](./dynamic.md) | [编辑](./editing.md) | [快速](./fast.md) | [泛化](./generalization.md) | [人体](./human.md) | [视频](./video.md) | [光照](./lighting.md) | [重建](./reconstruction.md) | [纹理](./texture.md) | [语义](./semantic.md) | [姿态-SLAM](./pose-slam.md) | [其他](./others.md) 
## Aug14 - Aug20, 2022
## Aug7 - Aug13, 2022
## Jul31 - Aug6, 2022
  - [NFOMP：具有非完整约束的差动驱动机器人最优运动规划器的神经场, IEEE Robotics and Automation Letters](https://ieeexplore.ieee.org/abstract/document/9851532/) | [code]
    > 摘要：最优运动规划是移动机器人中最关键的问题之一。一方面，经典的基于采样的方法为这个问题提出了渐近最优的解决方案。然而，这些规划器无法在合理的计算时间内实现平滑和短的轨迹。另一方面，基于优化的方法能够在各种场景中生成平滑而平坦的轨迹，包括密集的人群。然而，现代基于优化的方法使用预先计算的有符号距离函数进行碰撞损失估计，它限制了这些方法在一般配置空间中的应用，包括具有非完整约束的差分驱动非圆形机器人。此外，基于优化的方法缺乏准确处理 U 形或薄障碍物的能力。我们建议从两个方面改进优化方法。首先，我们开发了一个障碍物神经场模型来估计碰撞损失；将此模型与轨迹优化一起训练可以持续改善碰撞损失，同时实现更可行和更平滑的轨迹。其次，我们通过将拉格朗日乘数添加到轨迹损失函数中来强制轨迹考虑非完整约束。我们应用我们的方法解决了具有非完整约束的差动驱动机器人的最优运动规划问题，对我们的解决方案进行了基准测试，并证明了新的规划器生成了非常适合机器人跟随的平滑、短而平坦的轨迹，并且优于最先进的方法在归一化曲率上提高了 25%，在 MovingAI 环境中的尖点数量上提高了 75%。
  - [基于神经辐射场和运动图的可控自由视点视频重建, IEEE Transactions on Visualization and Computer Graphics](https://ieeexplore.ieee.org/abstract/document/9845414) | [code]
    > 在本文中，我们提出了一种基于运动图和神经辐射场（NeRF）的可控高质量自由视点视频生成方法。与现有的姿势驱动 NeRF 或时间/结构条件的 NeRF 工作不同，我们建议首先构建捕获序列的有向运动图。这种序列-运动-参数化策略不仅能够灵活地控制自由视点视频渲染的姿态，而且避免了相似姿态的冗余计算，从而提高了整体重建效率。此外，为了支持身体形状控制而不损失逼真的自由视点渲染性能，我们通过结合显式表面变形和隐式神经场景表示来改进 vanilla NeRF。具体来说，我们为运动图上的每个有效帧训练一个局部表面引导的 NeRF，并且体积渲染仅在真实表面周围的局部空间中执行，从而实现了合理的形状控制能力。据我们所知，我们的方法是第一个同时支持逼真的自由视点视频重建和基于运动图的用户引导运动遍历的方法。结果和比较进一步证明了所提出方法的有效性。
  - [基于神经描述符字段的鲁棒变化检测, IROS2022](https://ieeexplore.ieee.org/abstract/document/9845414) | [code]
    > 推理环境变化的能力对于长时间运行的机器人至关重要。代理应在操作期间捕获更改，以便可以遵循操作以确保工作会话的顺利进行。然而，不同的视角和累积的定位误差使得机器人很容易由于低观察重叠和漂移的对象关联而错误地检测到周围世界的变化。在本文中，基于最近提出的类别级神经描述符字段 (NDF)，我们开发了一种对象级在线变化检测方法，该方法对部分重叠的观察和嘈杂的定位结果具有鲁棒性。利用 NDF 的形状补全能力和 SE(3) 等效性，我们表示具有紧凑形状代码的对象，该代码编码来自部分观察的完整对象形状。然后基于从 NDF 恢复的对象中心将对象组织在空间树结构中，以便快速查询对象邻域。通过形状代码相似性关联对象并比较局部对象-邻居空间布局，我们提出的方法证明了对低观测重叠和定位噪声的鲁棒性。我们对合成序列和真实世界序列进行了实验，与多种基线方法相比，实现了改进的变化检测结果。
## Jul24 - Jul30, 2022
  - [用笼子变形辐射场, ECCV2022](https://arxiv.org/abs/2207.12298) | [code]
    > 辐射场的最新进展可以实现静态或动态 3D 场景的逼真渲染，但仍不支持用于场景操作或动画的显式变形。在本文中，我们提出了一种新的辐射场变形方法：自由形式的辐射场变形。我们使用一个三角形网格来包围称为笼子的前景对象作为界面，通过操纵笼子顶点，我们的方法可以实现辐射场的自由变形。我们方法的核心是网格变形中常用的基于笼的变形。我们提出了一种将其扩展到辐射场的新公式，该公式将采样点的位置和视图方向从变形空间映射到规范空间，从而实现变形场景的渲染。合成数据集和真实世界数据集的变形结果证明了我们方法的有效性。
## Previous weeks
  - [D-NeRF：动态场景的神经辐射场, CVPR2021](https://arxiv.org/abs/2011.13961) | [***``[code]``***](https://github.com/albertpumarola/D-NeRF)
    > 将机器学习与几何推理相结合的神经渲染技术已成为从一组稀疏图像中合成场景新视图的最有前途的方法之一。其中，神经辐射场 (NeRF) 尤为突出，它训练深度网络将 5D 输入坐标（表示空间位置和观察方向）映射为体积密度和与视图相关的发射辐射。然而，尽管在生成的图像上实现了前所未有的真实感水平，但 NeRF 仅适用于静态场景，其中可以从不同的图像中查询相同的空间位置。在本文中，我们介绍了 D-NeRF，这是一种将神经辐射场扩展到动态域的方法，允许在场景中移动的 \emph{single} 相机的刚性和非刚性运动下重建和渲染物体的新图像。为此，我们将时间视为系统的附加输入，并将学习过程分为两个主要阶段：一个将场景编码为规范空间，另一个将这个规范表示映射到特定时间的变形场景。两种映射都是使用全连接网络同时学习的。一旦网络经过训练，D-NeRF 就可以渲染新颖的图像，同时控制相机视图和时间变量，从而控制对象的移动。我们展示了我们的方法在物体处​​于刚性、关节和非刚性运动的场景中的有效性。代码、模型权重和动态场景数据集将发布。
  - [用于单目 4D 面部头像重建的动态神经辐射场, CVPR2021](https://gafniguy.github.io/4D-Facial-Avatars/) | [***``[code]``***](https://github.com/gafniguy/4D-Facial-Avatars)
    > 我们提出了用于模拟人脸外观和动态的动态神经辐射场。对说话的人进行数字建模和重建是各种应用程序的关键组成部分。特别是对于 AR 或 VR 中的远程呈现应用，需要忠实再现外观，包括新颖的视点或头部姿势。与显式建模几何和材料属性或纯粹基于图像的最先进方法相比，我们引入了基于场景表示网络的头部隐式表示。为了处理面部的动态，我们将场景表示网络与低维可变形模型相结合，该模型提供对姿势和表情的显式控制。我们使用体积渲染从这种混合表示中生成图像，并证明这种动态神经场景表示只能从单目输入数据中学习，而不需要专门的捕获设置。在我们的实验中，我们表明这种学习的体积表示允许生成照片般逼真的图像，其质量超过了基于视频的最先进的重演方法的质量。
  - [非刚性神经辐射场：单目视频变形场景的重建和新视图合成，, ICCV2021](https://vcai.mpi-inf.mpg.de/projects/nonrigid_nerf/) | [***``[code]``***](https://github.com/facebookresearch/nonrigid_nerf)
    > 我们提出了非刚性神经辐射场 (NR-NeRF)，这是一种用于一般非刚性动态场景的重建和新颖的视图合成方法。我们的方法将动态场景的 RGB 图像作为输入（例如，来自单目视频记录），并创建高质量的时空几何和外观表示。我们表明，单个手持消费级相机足以从新颖的虚拟相机视图合成动态场景的复杂渲染，例如一个“子弹时间”的视频效果。 NR-NeRF 将动态场景分解为规范体积及其变形。场景变形被实现为光线弯曲，其中直线光线被非刚性变形。我们还提出了一种新的刚性网络来更好地约束场景的刚性区域，从而获得更稳定的结果。射线弯曲和刚性网络在没有明确监督的情况下进行训练。我们的公式可以实现跨视图和时间的密集对应估计，以及引人注目的视频编辑应用程序，例如运动夸张。我们的代码将是开源的。
  - [神经体：具有结构化潜在代码的隐式神经表示，用于动态人类的新视图合成, CVPR2021](https://zju3dv.github.io/neuralbody/) | [***``[code]``***](https://github.com/zju3dv/neuralbody)
    > 本文解决了人类表演者从一组非常稀疏的摄像机视图中合成新颖视图的挑战。最近的一些工作表明，在给定密集输入视图的情况下，学习 3D 场景的隐式神经表示可以实现显着的视图合成质量。但是，如果视图高度稀疏，则表示学习将是不适定的。为了解决这个不适定问题，我们的关键思想是整合对视频帧的观察。为此，我们提出了神经体，这是一种新的人体表示，它假设在不同帧上学习到的神经表示共享同一组锚定到可变形网格的潜在代码，以便可以自然地整合跨帧的观察结果。可变形网格还为网络提供几何指导，以更有效地学习 3D 表示。为了评估我们的方法，我们创建了一个名为 ZJU-MoCap 的多视图数据集，用于捕捉具有复杂动作的表演者。 ZJU-MoCap 的实验表明，我们的方法在新颖的视图合成质量方面大大优于先前的工作。我们还展示了我们的方法从 People-Snapshot 数据集上的单目视频重建移动人物的能力。
  - [动态单目视频的动态视图合成, ICCV2021](https://free-view-video.github.io/) | [***``[code]``***](https://github.com/gaochen315/DynamicNeRF)
    > 我们提出了一种算法，用于在给定动态场景的单目视频的任意视点和任何输入时间步长处生成新视图。我们的工作建立在神经隐式表示的最新进展的基础上，并使用连续和可微的函数来建模时变结构和场景的外观。我们联合训练一个时不变的静态 NeRF 和一个时变的动态 NeRF，并学习如何以无监督的方式混合结果。然而，从单个视频中学习这个隐式函数是非常不适定的（与输入视频匹配的解决方案有无限多）。为了解决歧义，我们引入了正则化损失以鼓励更合理的解决方案。我们展示了从随意捕获的视频中进行动态视图合成的广泛定量和定性结果。
  - [TöRF：动态场景视图合成的飞行时间辐射场, NeurIPS2021](https://imaging.cs.cmu.edu/torf/) | [***``[code]``***](https://github.com/breuckelen/torf)
    > 神经网络可以表示和准确重建静态 3D 场景（例如 NeRF）的辐射场。一些作品将这些扩展到用单目视频捕获的动态场景，并具有可观的性能。然而，众所周知，单眼设置是一个约束不足的问题，因此方法依赖于数据驱动的先验来重建动态内容。我们用飞行时间 (ToF) 相机的测量值替换这些先验，并引入基于连续波 ToF 相机图像形成模型的神经表示。我们不使用处理过的深度图，而是对原始 ToF 传感器测量进行建模，以提高重建质量并避免低反射率区域、多路径干扰和传感器有限的明确深度范围等问题。我们展示了这种方法提高了动态场景重建对错误校准和大运动的鲁棒性，并讨论了集成现代智能手机上现在可用的 RGB+ToF 传感器的好处和局限性。
  - [以对象为中心的神经场景渲染](https://shellguo.com/osf/) | [***``[code]``***](https://shellguo.com/osf/)
    > 我们提出了一种从捕获的对象图像中合成逼真场景的方法。我们的工作建立在神经辐射场 (NeRFs) 之上，它隐含地模拟了场景的体积密度和定向发射的辐射。虽然 NeRF 可以合成逼真的图片，但它们只对静态场景进行建模，并且与特定的成像条件密切相关。这个属性使得 NeRFs 难以泛化到新场景，包括新的光照或对象的新排列。我们建议学习以对象为中心的神经散射函数 (OSF)，而不是像 NeRF 那样学习场景辐射场，这是一种使用与光照和视图相关的神经网络隐式模拟每个对象的光传输的表示。即使物体或灯光移动，这也可以渲染场景，而无需重新训练。结合体积路径跟踪程序，我们的框架能够渲染对象内和对象间的光传输效果，包括遮挡、镜面反射、阴影和间接照明。我们评估了我们的场景合成方法，并表明它可以推广到新的照明条件，产生逼真的、物理上精确的多对象场景渲染。
  - [学习动态人头的组成辐射场, CVPR2021(oral)](https://ziyanw1.github.io/hybrid_nerf/) | [code]
    > 动态人体的逼真渲染是远程呈现系统、虚拟购物、合成数据生成等的重要能力。最近，结合计算机图形学和机器学习技术的神经渲染方法已经创建了人类和物体的高保真模型。其中一些方法不会为可驱动的人体模型（神经体积）产生足够高保真度的结果，而其他方法则具有极长的渲染时间（NeRF）。我们提出了一种新颖的组合 3D 表示，它结合了以前最好的方法来产生更高分辨率和更快的结果。我们的表示通过将粗略的 3D 结构感知动画代码网格与连续学习的场景函数相结合，弥合了离散和连续体积表示之间的差距，该函数将每个位置及其相应的局部动画代码映射到其与视图相关的发射辐射和局部体积密度。可微分体渲染用于计算人头和上身的照片般逼真的新颖视图，并仅使用 2D 监督来端到端训练我们的新颖表示。此外，我们表明，学习到的动态辐射场可用于基于全局动画代码合成新的看不见的表情。我们的方法在合成动态人头和上半身的新视图方面取得了最先进的结果。
  - [动态场景的神经场景图, CVPR2021(oral)](https://arxiv.org/abs/2011.10379) | [***``[code]``***](https://github.com/princeton-computational-imaging/neural-scene-graphs)
    > 最近的隐式神经渲染方法表明，可以通过仅由一组 RGB 图像监督的预测其体积密度和颜色来学习复杂场景的准确视图合成。然而，现有方法仅限于学习将所有场景对象编码为单个神经网络的静态场景的有效表示，并且缺乏将动态场景表示和分解为单个场景对象的能力。在这项工作中，我们提出了第一个将动态场景分解为场景图的神经渲染方法。我们提出了一种学习的场景图表示，它对对象变换和辐射进行编码，以有效地渲染场景的新颖排列和视图。为此，我们学习隐式编码的场景，并结合联合学习的潜在表示来描述具有单个隐式函数的对象。我们在合成和真实汽车数据上评估所提出的方法，验证我们的方法学习动态场景 - 仅通过观察该场景的视频 - 并允许渲染具有看不见的对象集的新颖场景组合的新颖照片般逼真的视图看不见的姿势。
  - [用于视觉运动控制的 3D 神经场景表示, CoRL2021(oral)](https://3d-representation-learning.github.io/nerf-dy/) | [code]
    > 人类对我们周围的 3D 环境有着强烈的直觉理解。我们大脑中的物理心智模型适用于不同材料的物体，使我们能够执行远远超出当前机器人范围的广泛操作任务。在这项工作中，我们希望纯粹从 2D 视觉观察中学习动态 3D 场景的模型。我们的模型结合了神经弧度
  - [神经辐射世界中的仅视觉机器人导航](https://arxiv.org/abs/2110.00168) | [code]
    > 神经辐射场 (NeRFs) 最近已成为表示自然、复杂 3D 场景的强大范例。 NeRF 表示神经网络中的连续体积密度和 RGB 值，并通过光线追踪从看不见的相机视点生成照片般逼真的图像。我们提出了一种算法，用于在表示为 NeRF 的 3D 环境中导航机器人，仅使用板载 RGB 相机进行定位。我们假设场景的 NeRF 已经离线预训练，机器人的目标是在 NeRF 中的未占用空间中导航以达到目标姿势。我们引入了一种轨迹优化算法，该算法基于离散时间版本的差分平坦度避免与 NeRF 中的高密度区域发生碰撞，该版本可以约束机器人的完整姿势和控制输入。我们还引入了一种基于优化的过滤方法来估计 NeRF 中机器人的 6DoF 姿势和速度，仅给定一个板载 RGB 相机。我们将轨迹规划器与位姿过滤器结合在一个在线重新规划循环中，以提供基于视觉的机器人导航管道。我们展示了一个四旋翼机器人仅使用 RGB 相机在丛林健身房环境、教堂内部和巨石阵中导航的模拟结果。我们还演示了一个在教堂中导航的全向地面机器人，要求它重新定向以适应狭窄的缝隙。可以在此 https 网址上找到这项工作的视频。
