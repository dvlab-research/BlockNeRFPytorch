
每周分类神经辐射场 - dynamic ![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)
====================================================================================================================================
## 按类别筛选: 
 [全部](../weekly_nerf_cn.md) | [动态](./dynamic.md) | [编辑](./editing.md) | [快速](./fast.md) | [泛化](./generalization.md) | [人体](./human.md) | [视频](./video.md) | [光照](./lighting.md) | [重建](./reconstruction.md) | [纹理](./texture.md) | [语义](./semantic.md) | [姿态-SLAM](./pose-slam.md) | [其他](./others.md) 
## Dec27 - Jan3, 2023
## Dec25 - Dec31, 2022
## Dec18 - Dec24, 2022
## Dec11 - Dec17, 2022
## Dec4 - Dec10, 2022
## Nov27 - Dec3, 2022
## Nov20 - Nov26, 2022
  - [Tensor4D：用于高保真动态重建和渲染的高效神经 4D 分解](https://arxiv.org/abs/2211.11610) | [code]
    > 我们介绍了 Tensor4D，这是一种高效而有效的动态场景建模方法。我们解决方案的关键是一种高效的 4D 张量分解方法，使动态场景可以直接表示为 4D 时空张量。为了解决伴随的内存问题，我们首先将 4D 张量投影到三个时间感知体积，然后是九个紧凑的特征平面，从而分层分解 4D 张量。通过这种方式，可以以紧凑且高效的方式同时捕获随时间变化的空间信息。当应用 Tensor4D 进行动态场景重建和渲染时，我们进一步将 4D 场分解为不同的尺度，以便从粗到细学习结构运动和动态细节变化。我们的方法的有效性在合成场景和真实场景中都得到了验证。大量实验表明，我们的方法能够从稀疏视图摄像机装置甚至单目摄像机实现高质量的动态重建和渲染。代码和数据集将在此 https URL 上发布。
  - [DynIBaR：基于神经动态图像的渲染, -](https://arxiv.org/abs/2211.11082) | [code]
    > 我们解决了从描述复杂动态场景的单目视频中合成新视图的问题。基于随时间变化的神经辐射场（又名动态 NeRF）的最先进方法已在该任务上显示出令人印象深刻的结果。然而，对于具有复杂物体运动和不受控制的摄像机轨迹的长视频，这些方法可能会产生模糊或不准确的渲染，从而阻碍它们在现实世界中的应用。我们提出了一种新方法来解决这些限制，而不是在 MLP 的权重内对整个动态场景进行编码，方法是采用基于体积图像的渲染框架，该框架通过以场景运动感知方式聚合附近视图的特征来合成新视点.我们的系统保留了先前方法在建模复杂场景和视图相关效果方面的优势，而且还能够从具有复杂场景动态和不受约束的相机轨迹的长视频中合成照片般逼真的新颖视图。我们展示了对动态场景数据集的最先进方法的显着改进，并将我们的方法应用于具有挑战性相机和物体运动的野外视频，在这些视频中，先前的方法无法产生高质量的渲染。我们的项目网页位于此 http URL。
## Nov13 - Nov19, 2022
## Nov6 - Nov12, 2022
  - [ParticleNeRF：动态场景中在线神经辐射场的基于粒子的编码](https://arxiv.org/abs/2211.04041) | [code]
    > 神经辐射场 (NeRFs) 从图像中学习隐式表示（通常是静态的）环境。我们的论文扩展了 NeRFs 以在线方式处理动态场景。我们建议 ParticleNeRF 适应环境几何形状的变化，每 350 毫秒学习一个新的最新表示。与其他 NeRF 框架相比，ParticleNeRF 可以以更高的保真度表示动态环境的当前状态。为实现这一目标，我们引入了一种新的基于粒子的参数编码，它允许中间 NeRF 特征——现在耦合到空间中的粒子——随动态几何移动。这可以通过将光度重建损失反向传播到粒子的位置来实现。位置梯度被解释为粒子速度，并使用基于位置的动力学 (PBS) 物理系统集成到位置中。将 PBS 引入 NeRF 公式使我们能够为粒子运动添加碰撞约束，并创造未来机会将其他运动先验添加到系统中，例如刚体和可变形体
## Oct30 - Nov5, 2022
## Oct23 - Oct29, 2022
## Oct16 - Oct22, 2022
  - [神经辐射场场景重建探索：合成、真实世界和动态场景](https://arxiv.org/abs/2210.12268) | [code]
    > 该项目介绍了使用神经辐射场 (NeRF) 方法对合成和真实世界场景进行 3D 场景重建的探索。我们主要利用神经图形基元多分辨率哈希编码的训练和渲染时间的减少，来重建静态视频游戏场景和现实世界场景，比较和观察重建细节和局限性。此外，我们使用动态场景的神经辐射场 (D-NeRF) 探索动态场景重建。最后，我们扩展了 D-NeRF 的实现，最初仅限于处理合成场景，也可以处理真实世界的动态场景。
## Oct9 - Oct15, 2022
  - [通过学习一致性场实现高效的神经场景图, BMVC2022](https://arxiv.org/abs/2210.04127) | [***``[code]``***](https://github.com/ldynx/CF-NSG)
    > 神经辐射场 (NeRF) 从新颖的视图实现照片般逼真的图像渲染，神经场景图 (NSG) \cite{ost2021neural} 将其扩展到具有多个对象的动态场景（视频）。然而，为每个图像帧计算繁重的光线行进成为一个巨大的负担。在本文中，利用视频中相邻帧之间的显着冗余，我们提出了一个特征重用框架。然而，从天真地重用 NSG 特征的第一次尝试中，我们了解到，将跨帧一致的对象内在属性与瞬态属性分开是至关重要的。我们提出的方法，\textit{基于一致性场的 NSG (CF-NSG)}，重新定义了神经辐射场以额外考虑 \textit{一致性场}。通过解开表示，CF-NSG 充分利用了特征重用方案，并以更可控的方式执行扩展程度的场景操作。我们凭经验验证，CF-NSG 通过使用比 NSG 少 85% 的查询大大提高了推理效率，而渲染质量没有显着下降。代码将在以下位置提供：此 https 网址
## Oct2 - Oct8, 2022
## Sep25 - Oct1, 2022
## Sep18 - Sep24, 2022
  - [PREF：可预测性正则化神经运动场, ECCV2022(oral)](https://arxiv.org/abs/2209.10691) | [code]
    > 了解动态场景中的 3D 运动对于许多视觉应用至关重要。最近的进展主要集中在估计一些特定元素的活动，如人类。在本文中，我们利用神经运动场来估计多视图设置中所有点的运动。由于颜色相似的点和颜色随时间变化的点的模糊性，使用多视图数据对动态场景的运动进行建模具有挑战性。我们建议将估计的运动规范化为可预测的。如果先前帧的运动是已知的，那么不久的将来的运动应该是可预测的。因此，我们通过首先调节潜在嵌入的估计运动，然后通过采用预测器网络来强制嵌入的可预测性来引入可预测性正则化。与最先进的基于神经运动场的动态场景表示方法相比，所提出的框架 PREF（Predictability REgularized Fields）实现了同等或更好的结果，同时不需要场景的先验知识。
## Sep11 - Sep17, 2022
## Sep4 - Sep10, 2022
  - [神经特征融合领域：自监督 2D 图像表示的 3D 蒸馏, 3DV2022(oral)](https://arxiv.org/abs/2209.03494) | [***``[code]``***](https://github.com/dichotomies/N3F)
    > 我们提出了神经特征融合场 (N3F)，这是一种在将密集 2D 图像特征提取器应用于可重构为 3D 场景的多张图像分析时改进密集 2D 图像特征提取器的方法。给定一个图像特征提取器，例如使用自我监督进行预训练，N3F 使用它作为教师来学习在 3D 空间中定义的学生网络。 3D 学生网络类似于提取所述特征的神经辐射场，并且可以使用通常的可微渲染机器进行训练。因此，N3F 很容易适用于大多数神经渲染公式，包括 vanilla NeRF 及其对复杂动态场景的扩展。我们表明，我们的方法不仅能够在不使用手动标签的情况下在特定场景的神经领域的上下文中实现语义理解，而且在自我监督的 2D 基线上持续改进。这通过考虑不同序列中的各种任务（例如 2D 对象检索、3D 分割和场景编辑）来证明，包括 EPIC-KITCHENS 基准测试中的以自我为中心的长视频。
  - [MotionDiffuse：使用扩散模型的文本驱动人体运动生成](https://arxiv.org/abs/2208.15001) | [***``[code]``***](https://github.com/mingyuan-zhang/MotionDiffuse)
    > 人体运动建模对于许多现代图形应用程序很重要，这些应用程序通常需要专业技能。为了消除外行的技能障碍，最近的动作生成方法可以直接生成以自然语言为条件的人体动作。然而，通过各种文本输入实现多样化和细粒度的运动生成仍然具有挑战性。为了解决这个问题，我们提出了 MotionDiffuse，这是第一个基于扩散模型的文本驱动的运动生成框架，它展示了现有方法的几个所需属性。 1）概率映射。 MotionDiffuse 不是确定性的语言-运动映射，而是通过一系列注入变化的去噪步骤生成运动。 2）现实综合。 MotionDiffuse 擅长对复杂的数据分布进行建模并生成生动的运动序列。 3) 多级操作。 MotionDiffuse 响应身体部位的细粒度指令，以及带有时变文本提示的任意长度运动合成。我们的实验表明，MotionDiffuse 在文本驱动的运动生成和动作条件的运动生成方面具有令人信服的优势，从而优于现有的 SoTA 方法。定性分析进一步证明了 MotionDiffuse 对综合运动生成的可控性。主页：此 https 网址
## Aug28 - Sep3, 2022
## Aug21 - Aug27, 2022
  - [E-NeRF：来自移动事件相机的神经辐射场](https://arxiv.org/abs/2208.11300) | [code]
    > 从理想图像估计神经辐射场 (NeRFs) 已在计算机视觉领域得到广泛研究。大多数方法假设最佳照明和缓慢的相机运动。这些假设在机器人应用中经常被违反，其中图像包含运动模糊并且场景可能没有合适的照明。这可能会导致下游任务（例如场景的导航、检查或可视化）出现重大问题。为了缓解这些问题，我们提出了 E-NeRF，这是第一种从快速移动的事件摄像机中以 NeRF 形式估计体积场景表示的方法。我们的方法可以在非常快速的运动和高动态范围条件下恢复 NeRF，在这种情况下，基于帧的方法会失败。我们展示了仅通过提供事件流作为输入来渲染高质量帧是可能的。此外，通过结合事件和帧，我们可以估计在严重运动模糊下比最先进的方法质量更高的 NeRF。我们还表明，在只有很少的输入视图可用的情况下，结合事件和帧可以克服 NeRF 估计的失败情况，而无需额外的正则化。
## Aug14 - Aug20, 2022
  - [从单目视频中对动画 3D 人体进行神经捕获, ECCV2022](https://arxiv.org/abs/2208.08728) | [code]
    > 我们提出了一种从单目视频输入构建可动画 3D 人体表示的新颖范例，这样它就可以以任何看不见的姿势和视图进行渲染。我们的方法基于动态神经辐射场 (NeRF)，该动态神经辐射场 (NeRF) 由作为几何代理的基于网格的参数化 3D 人体模型装配。以前的方法通常依赖多视图视频或准确的 3D 几何信息作为附加输入；此外，大多数方法在推广到看不见的姿势时质量会下降。我们认为，泛化的关键是用于查询动态 NeRF 的良好输入嵌入：良好的输入嵌入应该定义全体积空间中的单射映射，由姿态变化下的表面网格变形引导。基于这一观察，我们建议嵌入输入查询及其与网格顶点上一组测地最近邻所跨越的局部表面区域的关系。通过包含位置和相对距离信息，我们的嵌入定义了距离保留的变形映射，并很好地推广到看不见的姿势。为了减少对额外输入的依赖，我们首先使用现成的工具初始化每帧 3D 网格，然后提出一个管道来联合优化 NeRF 并细化初始网格。大量实验表明，我们的方法可以在看不见的姿势和视图下合成合理的人类渲染结果。
## Aug7 - Aug13, 2022
## Jul31 - Aug6, 2022
  - [NFOMP：具有非完整约束的差动驱动机器人最优运动规划器的神经场, IEEE Robotics and Automation Letters](https://ieeexplore.ieee.org/abstract/document/9851532/) | [code]
    > 摘要：最优运动规划是移动机器人中最关键的问题之一。一方面，经典的基于采样的方法为这个问题提出了渐近最优的解决方案。然而，这些规划器无法在合理的计算时间内实现平滑和短的轨迹。另一方面，基于优化的方法能够在各种场景中生成平滑而平坦的轨迹，包括密集的人群。然而，现代基于优化的方法使用预先计算的有符号距离函数进行碰撞损失估计，它限制了这些方法在一般配置空间中的应用，包括具有非完整约束的差分驱动非圆形机器人。此外，基于优化的方法缺乏准确处理 U 形或薄障碍物的能力。我们建议从两个方面改进优化方法。首先，我们开发了一个障碍物神经场模型来估计碰撞损失；将此模型与轨迹优化一起训练可以持续改善碰撞损失，同时实现更可行和更平滑的轨迹。其次，我们通过将拉格朗日乘数添加到轨迹损失函数中来强制轨迹考虑非完整约束。我们应用我们的方法解决了具有非完整约束的差动驱动机器人的最优运动规划问题，对我们的解决方案进行了基准测试，并证明了新的规划器生成了非常适合机器人跟随的平滑、短而平坦的轨迹，并且优于最先进的方法在归一化曲率上提高了 25%，在 MovingAI 环境中的尖点数量上提高了 75%。
  - [基于神经辐射场和运动图的可控自由视点视频重建, IEEE Transactions on Visualization and Computer Graphics](https://ieeexplore.ieee.org/abstract/document/9845414) | [code]
    > 在本文中，我们提出了一种基于运动图和神经辐射场（NeRF）的可控高质量自由视点视频生成方法。与现有的姿势驱动 NeRF 或时间/结构条件的 NeRF 工作不同，我们建议首先构建捕获序列的有向运动图。这种序列-运动-参数化策略不仅能够灵活地控制自由视点视频渲染的姿态，而且避免了相似姿态的冗余计算，从而提高了整体重建效率。此外，为了支持身体形状控制而不损失逼真的自由视点渲染性能，我们通过结合显式表面变形和隐式神经场景表示来改进 vanilla NeRF。具体来说，我们为运动图上的每个有效帧训练一个局部表面引导的 NeRF，并且体积渲染仅在真实表面周围的局部空间中执行，从而实现了合理的形状控制能力。据我们所知，我们的方法是第一个同时支持逼真的自由视点视频重建和基于运动图的用户引导运动遍历的方法。结果和比较进一步证明了所提出方法的有效性。
  - [基于神经描述符字段的鲁棒变化检测, IROS2022](https://ieeexplore.ieee.org/abstract/document/9845414) | [code]
    > 推理环境变化的能力对于长时间运行的机器人至关重要。代理应在操作期间捕获更改，以便可以遵循操作以确保工作会话的顺利进行。然而，不同的视角和累积的定位误差使得机器人很容易由于低观察重叠和漂移的对象关联而错误地检测到周围世界的变化。在本文中，基于最近提出的类别级神经描述符字段 (NDF)，我们开发了一种对象级在线变化检测方法，该方法对部分重叠的观察和嘈杂的定位结果具有鲁棒性。利用 NDF 的形状补全能力和 SE(3) 等效性，我们表示具有紧凑形状代码的对象，该代码编码来自部分观察的完整对象形状。然后基于从 NDF 恢复的对象中心将对象组织在空间树结构中，以便快速查询对象邻域。通过形状代码相似性关联对象并比较局部对象-邻居空间布局，我们提出的方法证明了对低观测重叠和定位噪声的鲁棒性。我们对合成序列和真实世界序列进行了实验，与多种基线方法相比，实现了改进的变化检测结果。
## Jul24 - Jul30, 2022
  - [用笼子变形辐射场, ECCV2022](https://arxiv.org/abs/2207.12298) | [code]
    > 辐射场的最新进展可以实现静态或动态 3D 场景的逼真渲染，但仍不支持用于场景操作或动画的显式变形。在本文中，我们提出了一种新的辐射场变形方法：自由形式的辐射场变形。我们使用一个三角形网格来包围称为笼子的前景对象作为界面，通过操纵笼子顶点，我们的方法可以实现辐射场的自由变形。我们方法的核心是网格变形中常用的基于笼的变形。我们提出了一种将其扩展到辐射场的新公式，该公式将采样点的位置和视图方向从变形空间映射到规范空间，从而实现变形场景的渲染。合成数据集和真实世界数据集的变形结果证明了我们方法的有效性。
## Previous weeks
  - [D-NeRF：动态场景的神经辐射场, CVPR2021](https://arxiv.org/abs/2011.13961) | [***``[code]``***](https://github.com/albertpumarola/D-NeRF)
    > 将机器学习与几何推理相结合的神经渲染技术已成为从一组稀疏图像中合成场景新视图的最有前途的方法之一。其中，神经辐射场 (NeRF) 尤为突出，它训练深度网络将 5D 输入坐标（表示空间位置和观察方向）映射为体积密度和与视图相关的发射辐射。然而，尽管在生成的图像上实现了前所未有的真实感水平，但 NeRF 仅适用于静态场景，其中可以从不同的图像中查询相同的空间位置。在本文中，我们介绍了 D-NeRF，这是一种将神经辐射场扩展到动态域的方法，允许在场景中移动的 \emph{single} 相机的刚性和非刚性运动下重建和渲染物体的新图像。为此，我们将时间视为系统的附加输入，并将学习过程分为两个主要阶段：一个将场景编码为规范空间，另一个将这个规范表示映射到特定时间的变形场景。两种映射都是使用全连接网络同时学习的。一旦网络经过训练，D-NeRF 就可以渲染新颖的图像，同时控制相机视图和时间变量，从而控制对象的移动。我们展示了我们的方法在物体处​​于刚性、关节和非刚性运动的场景中的有效性。代码、模型权重和动态场景数据集将发布。
  - [用于单目 4D 面部头像重建的动态神经辐射场, CVPR2021](https://gafniguy.github.io/4D-Facial-Avatars/) | [***``[code]``***](https://github.com/gafniguy/4D-Facial-Avatars)
    > 我们提出了用于模拟人脸外观和动态的动态神经辐射场。对说话的人进行数字建模和重建是各种应用程序的关键组成部分。特别是对于 AR 或 VR 中的远程呈现应用，需要忠实再现外观，包括新颖的视点或头部姿势。与显式建模几何和材料属性或纯粹基于图像的最先进方法相比，我们引入了基于场景表示网络的头部隐式表示。为了处理面部的动态，我们将场景表示网络与低维可变形模型相结合，该模型提供对姿势和表情的显式控制。我们使用体积渲染从这种混合表示中生成图像，并证明这种动态神经场景表示只能从单目输入数据中学习，而不需要专门的捕获设置。在我们的实验中，我们表明这种学习的体积表示允许生成照片般逼真的图像，其质量超过了基于视频的最先进的重演方法的质量。
  - [非刚性神经辐射场：单目视频变形场景的重建和新视图合成，, ICCV2021](https://vcai.mpi-inf.mpg.de/projects/nonrigid_nerf/) | [***``[code]``***](https://github.com/facebookresearch/nonrigid_nerf)
    > 我们提出了非刚性神经辐射场 (NR-NeRF)，这是一种用于一般非刚性动态场景的重建和新颖的视图合成方法。我们的方法将动态场景的 RGB 图像作为输入（例如，来自单目视频记录），并创建高质量的时空几何和外观表示。我们表明，单个手持消费级相机足以从新颖的虚拟相机视图合成动态场景的复杂渲染，例如一个“子弹时间”的视频效果。 NR-NeRF 将动态场景分解为规范体积及其变形。场景变形被实现为光线弯曲，其中直线光线被非刚性变形。我们还提出了一种新的刚性网络来更好地约束场景的刚性区域，从而获得更稳定的结果。射线弯曲和刚性网络在没有明确监督的情况下进行训练。我们的公式可以实现跨视图和时间的密集对应估计，以及引人注目的视频编辑应用程序，例如运动夸张。我们的代码将是开源的。
  - [神经体：具有结构化潜在代码的隐式神经表示，用于动态人类的新视图合成, CVPR2021](https://zju3dv.github.io/neuralbody/) | [***``[code]``***](https://github.com/zju3dv/neuralbody)
    > 本文解决了人类表演者从一组非常稀疏的摄像机视图中合成新颖视图的挑战。最近的一些工作表明，在给定密集输入视图的情况下，学习 3D 场景的隐式神经表示可以实现显着的视图合成质量。但是，如果视图高度稀疏，则表示学习将是不适定的。为了解决这个不适定问题，我们的关键思想是整合对视频帧的观察。为此，我们提出了神经体，这是一种新的人体表示，它假设在不同帧上学习到的神经表示共享同一组锚定到可变形网格的潜在代码，以便可以自然地整合跨帧的观察结果。可变形网格还为网络提供几何指导，以更有效地学习 3D 表示。为了评估我们的方法，我们创建了一个名为 ZJU-MoCap 的多视图数据集，用于捕捉具有复杂动作的表演者。 ZJU-MoCap 的实验表明，我们的方法在新颖的视图合成质量方面大大优于先前的工作。我们还展示了我们的方法从 People-Snapshot 数据集上的单目视频重建移动人物的能力。
  - [动态单目视频的动态视图合成, ICCV2021](https://free-view-video.github.io/) | [***``[code]``***](https://github.com/gaochen315/DynamicNeRF)
    > 我们提出了一种算法，用于在给定动态场景的单目视频的任意视点和任何输入时间步长处生成新视图。我们的工作建立在神经隐式表示的最新进展的基础上，并使用连续和可微的函数来建模时变结构和场景的外观。我们联合训练一个时不变的静态 NeRF 和一个时变的动态 NeRF，并学习如何以无监督的方式混合结果。然而，从单个视频中学习这个隐式函数是非常不适定的（与输入视频匹配的解决方案有无限多）。为了解决歧义，我们引入了正则化损失以鼓励更合理的解决方案。我们展示了从随意捕获的视频中进行动态视图合成的广泛定量和定性结果。
  - [TöRF：动态场景视图合成的飞行时间辐射场, NeurIPS2021](https://imaging.cs.cmu.edu/torf/) | [***``[code]``***](https://github.com/breuckelen/torf)
    > 神经网络可以表示和准确重建静态 3D 场景（例如 NeRF）的辐射场。一些作品将这些扩展到用单目视频捕获的动态场景，并具有可观的性能。然而，众所周知，单眼设置是一个约束不足的问题，因此方法依赖于数据驱动的先验来重建动态内容。我们用飞行时间 (ToF) 相机的测量值替换这些先验，并引入基于连续波 ToF 相机图像形成模型的神经表示。我们不使用处理过的深度图，而是对原始 ToF 传感器测量进行建模，以提高重建质量并避免低反射率区域、多路径干扰和传感器有限的明确深度范围等问题。我们展示了这种方法提高了动态场景重建对错误校准和大运动的鲁棒性，并讨论了集成现代智能手机上现在可用的 RGB+ToF 传感器的好处和局限性。
  - [以对象为中心的神经场景渲染](https://shellguo.com/osf/) | [***``[code]``***](https://shellguo.com/osf/)
    > 我们提出了一种从捕获的对象图像中合成逼真场景的方法。我们的工作建立在神经辐射场 (NeRFs) 之上，它隐含地模拟了场景的体积密度和定向发射的辐射。虽然 NeRF 可以合成逼真的图片，但它们只对静态场景进行建模，并且与特定的成像条件密切相关。这个属性使得 NeRFs 难以泛化到新场景，包括新的光照或对象的新排列。我们建议学习以对象为中心的神经散射函数 (OSF)，而不是像 NeRF 那样学习场景辐射场，这是一种使用与光照和视图相关的神经网络隐式模拟每个对象的光传输的表示。即使物体或灯光移动，这也可以渲染场景，而无需重新训练。结合体积路径跟踪程序，我们的框架能够渲染对象内和对象间的光传输效果，包括遮挡、镜面反射、阴影和间接照明。我们评估了我们的场景合成方法，并表明它可以推广到新的照明条件，产生逼真的、物理上精确的多对象场景渲染。
  - [学习动态人头的组成辐射场, CVPR2021(oral)](https://ziyanw1.github.io/hybrid_nerf/) | [code]
    > 动态人体的逼真渲染是远程呈现系统、虚拟购物、合成数据生成等的重要能力。最近，结合计算机图形学和机器学习技术的神经渲染方法已经创建了人类和物体的高保真模型。其中一些方法不会为可驱动的人体模型（神经体积）产生足够高保真度的结果，而其他方法则具有极长的渲染时间（NeRF）。我们提出了一种新颖的组合 3D 表示，它结合了以前最好的方法来产生更高分辨率和更快的结果。我们的表示通过将粗略的 3D 结构感知动画代码网格与连续学习的场景函数相结合，弥合了离散和连续体积表示之间的差距，该函数将每个位置及其相应的局部动画代码映射到其与视图相关的发射辐射和局部体积密度。可微分体渲染用于计算人头和上身的照片般逼真的新颖视图，并仅使用 2D 监督来端到端训练我们的新颖表示。此外，我们表明，学习到的动态辐射场可用于基于全局动画代码合成新的看不见的表情。我们的方法在合成动态人头和上半身的新视图方面取得了最先进的结果。
  - [动态场景的神经场景图, CVPR2021(oral)](https://arxiv.org/abs/2011.10379) | [***``[code]``***](https://github.com/princeton-computational-imaging/neural-scene-graphs)
    > 最近的隐式神经渲染方法表明，可以通过仅由一组 RGB 图像监督的预测其体积密度和颜色来学习复杂场景的准确视图合成。然而，现有方法仅限于学习将所有场景对象编码为单个神经网络的静态场景的有效表示，并且缺乏将动态场景表示和分解为单个场景对象的能力。在这项工作中，我们提出了第一个将动态场景分解为场景图的神经渲染方法。我们提出了一种学习的场景图表示，它对对象变换和辐射进行编码，以有效地渲染场景的新颖排列和视图。为此，我们学习隐式编码的场景，并结合联合学习的潜在表示来描述具有单个隐式函数的对象。我们在合成和真实汽车数据上评估所提出的方法，验证我们的方法学习动态场景 - 仅通过观察该场景的视频 - 并允许渲染具有看不见的对象集的新颖场景组合的新颖照片般逼真的视图看不见的姿势。
  - [用于视觉运动控制的 3D 神经场景表示, CoRL2021(oral)](https://3d-representation-learning.github.io/nerf-dy/) | [code]
    > 人类对我们周围的 3D 环境有着强烈的直觉理解。我们大脑中的物理心智模型适用于不同材料的物体，使我们能够执行远远超出当前机器人范围的广泛操作任务。在这项工作中，我们希望纯粹从 2D 视觉观察中学习动态 3D 场景的模型。我们的模型结合了神经弧度
  - [神经辐射世界中的仅视觉机器人导航](https://arxiv.org/abs/2110.00168) | [code]
    > 神经辐射场 (NeRFs) 最近已成为表示自然、复杂 3D 场景的强大范例。 NeRF 表示神经网络中的连续体积密度和 RGB 值，并通过光线追踪从看不见的相机视点生成照片般逼真的图像。我们提出了一种算法，用于在表示为 NeRF 的 3D 环境中导航机器人，仅使用板载 RGB 相机进行定位。我们假设场景的 NeRF 已经离线预训练，机器人的目标是在 NeRF 中的未占用空间中导航以达到目标姿势。我们引入了一种轨迹优化算法，该算法基于离散时间版本的差分平坦度避免与 NeRF 中的高密度区域发生碰撞，该版本可以约束机器人的完整姿势和控制输入。我们还引入了一种基于优化的过滤方法来估计 NeRF 中机器人的 6DoF 姿势和速度，仅给定一个板载 RGB 相机。我们将轨迹规划器与位姿过滤器结合在一个在线重新规划循环中，以提供基于视觉的机器人导航管道。我们展示了一个四旋翼机器人仅使用 RGB 相机在丛林健身房环境、教堂内部和巨石阵中导航的模拟结果。我们还演示了一个在教堂中导航的全向地面机器人，要求它重新定向以适应狭窄的缝隙。可以在此 https 网址上找到这项工作的视频。
