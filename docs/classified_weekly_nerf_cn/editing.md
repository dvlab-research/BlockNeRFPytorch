
每周分类神经辐射场 - editing ![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)
====================================================================================================================================
## 按类别筛选: 
 [全部](../weekly_nerf_cn.md) | [动态](./dynamic.md) | [编辑](./editing.md) | [快速](./fast.md) | [泛化](./generalization.md) | [人体](./human.md) | [视频](./video.md) | [光照](./lighting.md) | [重建](./reconstruction.md) | [纹理](./texture.md) | [语义](./semantic.md) | [姿态-SLAM](./pose-slam.md) | [其他](./others.md) 
## Dec27 - Jan3, 2023
## Dec25 - Dec31, 2022
## Dec18 - Dec24, 2022
  - [从神经辐射场中移除对象](https://arxiv.org/abs/2212.11966) | [code]
    > 神经辐射场 (NeRFs) 正在成为一种无处不在的场景表示，可实现新颖的视图合成。 NeRF 将越来越多地与其他人共享。 不过，在共享 NeRF 之前，可能需要删除个人信息或难看的物体。 使用当前的 NeRF 编辑框架不容易实现这种删除。 我们提出了一个框架，用于从 RGB-D 序列创建的 NeRF 表示中删除对象。 我们的 NeRF 修复方法利用了最近在 2D 图像修复方面的工作，并以用户提供的掩码为指导。 我们的算法以基于置信度的视图选择程序为基础。 它选择在创建 NeRF 时使用哪些单独的 2D 修复图像，以便生成的修复 NeRF 是 3D 一致的。 我们表明我们的 NeRF 编辑方法对于以多视图连贯方式合成合理的修复是有效的。 我们使用一个新的且仍然具有挑战性的数据集来验证我们的方法来完成 NeRF 修复任务。
## Dec11 - Dec17, 2022
  - [NeRF-Art：文本驱动的神经辐射场程式化](https://arxiv.org/abs/2212.08070) | [***``[code]``***](https://cassiepython.github.io/nerfart/)
    > 作为 3D 场景的强大表示，神经辐射场 (NeRF) 可以从多视图图像中合成高质量的新视图。 然而，对 NeRF 进行样式化仍然具有挑战性，尤其是在模拟外观和几何形状同时发生变化的文本引导样式时。 在本文中，我们介绍了 NeRF-Art，这是一种文本引导的 NeRF 风格化方法，它通过简单的文本提示来操纵预训练的 NeRF 模型的风格。 与以前缺乏足够的几何变形和纹理细节或需要网格来指导风格化的方法不同，我们的方法可以将 3D 场景转换为以所需几何形状和外观变化为特征的目标样式，而无需任何网格引导。 这是通过引入一种新颖的全局-局部对比学习策略，结合方向约束来同时控制目标风格的轨迹和强度来实现的。 此外，我们采用权重正则化方法来有效抑制在几何样式化过程中转换密度场时容易出现的混浊伪影和几何噪声。 通过对各种风格的广泛实验，我们证明了我们的方法在单视图风格化质量和跨视图一致性方面是有效且稳健的。 代码和更多结果可以在我们的项目页面中找到：这个 https URL。
## Dec4 - Dec10, 2022
  - [Ref-NPR：基于参考的非真实感辐射场](https://arxiv.org/abs/2212.02766) | [code]
    > 现有的 3D 场景风格化方法采用任意风格参考来将纹理和颜色作为风格进行传输，而无需建立有意义的语义对应关系。 我们提出了基于参考的非真实感辐射场，即 Ref-NPR。 它是一种可控的场景风格化方法，利用辐射场对 3D 场景进行风格化，并以单个风格化的 2D 视图作为参考。 为了获得不错的结果，我们提出了一种基于程式化参考视图的光线配准过程，以在新颖的视图中获得伪光线监督，并利用内容图像中的语义对应来填充具有感知相似风格的遮挡区域。 结合这些操作，Ref-NPR 使用单个参考生成非真实感和连续的新颖视图序列，同时在遮挡区域获得合理的程式化。 实验表明，Ref-NPR 在视觉质量和语义对应方面明显优于其他场景和视频风格化方法。 代码和数据将公开。
## Nov27 - Dec3, 2022
## Nov20 - Nov26, 2022
## Nov13 - Nov19, 2022
## Nov6 - Nov12, 2022
  - [基于学习的复杂室内场景逆渲染与可微蒙特卡洛光线追踪, SIGGRAPH-Asia2022](https://jingsenzhu.github.io/invrend/) | [code]
    > 我们提出了一种基于学习的方法，用于使用可区分的蒙特卡洛光线追踪对复杂的室内场景进行逆向渲染。我们的方法将单个室内场景 RGB 图像作为输入，并自动推断其底层表面反射率、几何形状和空间变化的照明。这使我们能够对场景进行逼真的编辑，例如插入多个复杂的虚拟对象并使用全局照明忠实地编辑表面材质。
## Oct30 - Nov5, 2022
  - [gCoRF：生成合成辐射场, 3DV2022](https://vcai.mpi-inf.mpg.de/projects/gCoRF/) | [code]
    > 对象的 3D 生成模型可通过 3D 控制实现逼真的图像合成。现有方法将场景建模为全局场景表示，忽略了场景的组成方面。除了支持可概括的 3D 推理之外，组合推理还可以支持各种编辑应用程序。在本文中，我们提出了一个组合生成模型，其中对象的每个语义部分都表示为仅从野外 2D 数据中学习的独立 3D 表示。我们从全局生成模型 (GAN) 开始，学习使用 2D 分割掩码的监督将其分解为不同的语义部分。然后，我们学习合成独立采样的部分，以创建连贯的全局场景。不同的部分可以独立采样，同时保持物体的其余部分固定。我们在各种对象和部件上评估我们的方法，并演示编辑应用程序。
## Oct23 - Oct29, 2022
  - [通过辐射贴图提升点云渲染](https://arxiv.org/abs/2210.15107) | [code]
    > 近年来，由于其高质量，我们见证了基于 NeRF 的图像渲染的快速发展。然而，点云渲染在某种程度上较少被探索。与遭受密集空间采样的基于 NeRF 的渲染相比，点云渲染自然计算密集度较低，这使其能够部署在移动计算设备中。在这项工作中，我们专注于通过紧凑的模型设计提高点云渲染的图像质量。我们首先分析体绘制公式在点云上的适应性。基于分析，我们将 NeRF 表示简化为空间映射函数，每个像素只需要一次评估。此外，受光线行进的启发，我们将嘈杂的原始点云校正为光线与表面之间的估计交点作为查询坐标，这可以避免空间频率崩溃和邻点干扰。由光栅化、空间映射和细化阶段组成，我们的方法在点云渲染上实现了最先进的性能，以显着的优势优于之前的工作，模型尺寸更小。我们在 NeRF-Synthetic 上获得了 31.74 的 PSNR，在 ScanNet 上获得了 25.88，在 DTU 上获得了 30.81。代码和数据将很快发布。
## Oct16 - Oct22, 2022
## Oct9 - Oct15, 2022
  - [LB-NERF：用于透明介质的光弯曲神经辐射场, ICIP2022](https://ieeexplore.ieee.org/abstract/document/9897642) | [code]
    > 神经辐射场 (NeRFs) 已被提出作为新颖的视图合成方法，并且由于其多功能性已被用于解决各种问题。 NeRF 可以使用假设直线光路的神经渲染来表示 3D 空间中的颜色和密度。但是，场景中具有不同折射率的介质，例如透明介质，会引起光的折射，打破了光路直线的假设。因此，不能在多视图图像中一致地学习 NeRF。为了解决这个问题，本研究提出了一种方法，通过引入光折射效应作为与源自相机中心的直线的偏移量来学习跨多个视点的一致辐射场。实验结果定量和定性地验证了在考虑透明物体的折射时，我们的方法可以比传统的 NeRF 方法更好地插入视点。
  - [通过隐式神经表示的测试时间训练实现可控风格迁移](https://arxiv.org/abs/2210.07762) | [code]
    > 我们提出了一个基于隐式神经表示的可控风格迁移框架，该框架通过测试时训练以像素方式控制风格化输出。与传统的图像优化方法经常遇到不稳定的收敛和需要密集训练且泛化能力有限的基于学习的方法不同，我们提出了一个模型优化框架，该框架在测试时通过显式损失函数来优化神经网络以进行风格迁移。在经过一次测试时间训练后，由于基于 INR 的模型的灵活性，我们的框架可以以像素方式精确控制风格化图像，并自由调整图像分辨率，无需进一步优化或训练。我们演示了几个应用程序。
  - [神经形状变形先验, NeurIPS2022](https://arxiv.org/abs/2210.05616) | [code]
    > 我们提出了神经形状变形先验，这是一种新的形状操作方法，可以根据用户提供的手柄运动来预测非刚性物体的网格变形。最先进的方法将此问题视为优化任务，其中输入源网格被迭代变形以根据手工制作的正则化器（如 ARAP）最小化目标函数。在这项工作中，我们基于形状的基本几何特性来学习变形行为，同时利用包含各种非刚性变形的大规模数据集。具体来说，给定源网格和描述部分表面变形的手柄的所需目标位置，我们预测在 3D 空间中定义的连续变形场以描述空间变形。为此，我们引入了基于变压器的变形网络，将形状变形表示为局部表面变形的组合。它学习一组锚定在 3D 空间中的局部潜在代码，从中我们可以学习一组局部表面的连续变形函数。我们的方法可以应用于具有挑战性的变形，并且可以很好地推广到看不见的变形。我们使用 DeformingThing4D 数据集在实验中验证了我们的方法，并与经典的基于优化的方法和最近的基于神经网络的方法进行了比较。
## Oct2 - Oct8, 2022
  - [使用辐射场传播的无监督多视图对象分割, NeurIPS2022](https://arxiv.org/abs/2210.00489) | [code]
    > 我们提出了辐射场传播 (RFP)，这是一种在重建过程中分割 3D 对象的新方法，仅给出场景的未标记多视图图像。 RFP 源自新兴的基于神经辐射场的技术，该技术将语义与外观和几何形状联合编码。我们方法的核心是一种新颖的传播策略，用于具有双向光度损失的单个对象的辐射场，能够将场景无监督地划分为对应于不同对象实例的显着或有意义的区域。为了更好地处理具有多个对象和遮挡的复杂场景，我们进一步提出了一种迭代期望最大化算法来细化对象掩码。据我们所知，RFP 是第一个在没有任何监督、注释或其他线索（如 3D 边界框和对象类别的先验知识）的情况下处理神经辐射场 (NeRF) 的 3D 场景对象分割的无监督方法。实验表明，RFP 实现了可行的分割结果，比以前的无监督图像/场景分割方法更准确，并且可与现有的基于 NeRF 监督的方法相媲美。分段对象表示支持单独的 3D 对象编辑操作。
## Sep25 - Oct1, 2022
## Sep18 - Sep24, 2022
## Sep11 - Sep17, 2022
  - [3DMM-RF：用于 3D 人脸建模的卷积辐射场](https://arxiv.org/abs/2209.07366) | [code]
    > 面部 3D 可变形模型是具有无数应用的主要计算机视觉主题，并且在过去二十年中得到了高度优化。深度生成网络的巨大改进为改进此类模型创造了各种可能性，并引起了广泛的兴趣。此外，神经辐射领域的最新进展正在彻底改变已知场景的新视图合成。在这项工作中，我们提出了一个面部 3D 可变形模型，它利用了上述两者，并且可以准确地建模对象的身份、姿势和表情，并在任意光照下渲染它。这是通过利用强大的基于深度样式的生成器来克服神经辐射场的两个主要弱点，即它们的刚性和渲染速度来实现的。我们引入了一种基于样式的生成网络，它一次性合成所有且仅合成神经辐射场所需的渲染样本。我们创建了一个巨大的面部渲染标记合成数据集，并在这些数据上训练网络，以便它可以准确地建模和概括面部身份、姿势和外观。最后，我们证明该模型可以准确地拟合任意姿势和光照的“in-the-wild”人脸图像，提取人脸特征，并用于在可控条件下重新渲染人脸。
## Sep4 - Sep10, 2022
  - [SIRA：来自单个图像的可重新点亮的头像](https://arxiv.org/abs/2209.03027) | [code]
    > 从单个图像中恢复人头的几何形状，同时分解材料和照明是一个严重不适定的问题，需要解决先验信息。基于 3D 可变形模型 (3DMM) 的方法，以及它们与可微渲染器的组合，已显示出可喜的结果。然而，3DMM 的表现力是有限的，它们通常会产生过度平滑且与身份无关的 3D 形状，仅限于面部区域。最近已经通过使用多层感知器参数化几何形状的神经场获得了高度准确的全头重建。这些表示的多功能性也被证明对于解开几何、材料和照明是有效的。然而，这些方法需要几十个输入图像。在本文中，我们介绍了 SIRA，这是一种从单个图像重建具有高保真几何形状和分解光和表面材料的人头头像的方法。我们的关键成分是两个基于神经场的数据驱动统计模型，可解决单视图 3D 表面重建和外观分解的模糊性。实验表明，SIRA 在 3D 头部重建中获得了最先进的结果，同时它成功地解开了全局照明、漫反射和镜面反射率。此外，我们的重建适用于基于物理的外观编辑和头部模型重新照明。
## Aug28 - Sep3, 2022
  - [NerfCap：使用动态神经辐射场捕获人类表现, TVCG2022](https://ieeexplore.ieee.org/abstract/document/9870173) | [code]
    > 本文解决了从稀疏的多视图或单目视频中捕捉人类表演的挑战。给定表演者的模板网格，以前的方法通过将模板网格非刚性地注册到具有 2D 轮廓或密集光度对齐的图像来捕获人体运动。然而，详细的表面变形无法从轮廓中恢复，而光度对齐则受到视频外观变化引起的不稳定性的影响。为了解决这些问题，我们提出了 NerfCap，这是一种基于表演者动态神经辐射场 (NeRF) 表示的新型表演捕捉方法。具体来说，通过优化变形场和规范 NeRF 的外观模型，从模板几何初始化规范 NeRF 并注册到视频帧。为了捕捉大型身体运动和详细的表面变形，NerfCap 将线性混合蒙皮与嵌入式图形变形相结合。与受限于固定拓扑和纹理的基于网格的方法相比，NerfCap 能够灵活地捕捉视频中复杂的几何形状和外观变化，并合成更逼真的图像。此外，NerfCap 可以通过将合成视频与输入视频进行匹配，以自我监督的方式进行端到端的预训练。各种数据集的实验结果表明，NerfCap 在表面重建精度和新视图合成质量方面都优于先前的工作。
## Aug21 - Aug27, 2022
  - [训练和调整生成神经辐射场以进行属性条件 3D 感知人脸生成](https://arxiv.org/abs/2208.12550) | [***``[code]``***](https://github.com/zhangqianhui/TT-GNeRF)
    > 基于生成神经辐射场 (GNeRF) 的 3D 感知 GAN 已经实现了令人印象深刻的高质量图像生成，同时保持了强大的 3D 一致性。最显着的成就是在人脸生成领域。然而，这些模型中的大多数都专注于提高视图一致性而忽略了解耦方面，因此这些模型无法提供对生成的高质量语义/属性控制。为此，我们引入了一个使用特定属性标签作为输入的条件 GNeRF 模型，以提高 3D 感知生成模型的可控性和解开能力。我们利用预训练的 3D 感知模型作为基础，并集成了一个双分支属性编辑模块 (DAEM)，该模块利用属性标签来提供对生成的控制。此外，我们提出了一种 TRIOT (TRAining as Init, and Optimizing for Tuning) 方法来优化潜在向量，以进一步提高属性编辑的精度。在广泛使用的 FFHQ 上进行的大量实验表明，我们的模型在保留非目标区域的同时，可以产生具有更好视图一致性的高质量编辑。该代码可在此 https 网址上找到。
  - [DreamBooth：为主题驱动生成微调文本到图像的扩散模型](https://dreambooth.github.io/) | [code]
    > 大型文本到图像模型在人工智能的演进中实现了显着的飞跃，能够从给定的文本提示中对图像进行高质量和多样化的合成。然而，这些模型缺乏模仿给定参考集中对象的外观并在不同上下文中合成它们的新颖再现的能力。在这项工作中，我们提出了一种“个性化”文本到图像扩散模型的新方法（专门针对用户的需求）。给定主题的几张图像作为输入，我们微调预训练的文本到图像模型（Imagen，尽管我们的方法不限于特定模型），以便它学会将唯一标识符与该特定主题绑定.一旦对象被嵌入模型的输出域中，唯一标识符就可以用于合成在不同场景中情境化的对象的完全新颖的真实感图像。通过利用嵌入在模型中的语义先验和新的自生类特定先验保存损失，我们的技术能够在参考图像中没有出现的不同场景、姿势、视图和照明条件下合成主体。我们将我们的技术应用于几个以前无懈可击的任务，包括主题重新上下文化、文本引导视图合成、外观修改和艺术渲染（同时保留主题的关键特征）。项目页面：此 https 网址
  - [FurryGAN：高质量的前景感知图像合成, ECCV2022](https://jeongminb.github.io/FurryGAN/) | [***``[code]``***](https://jeongminb.github.io/FurryGAN/)
    > 前景感知图像合成旨在生成图像及其前景蒙版。一种常见的方法是将图像公式化为前景图像和背景图像的蒙版混合。这是一个具有挑战性的问题，因为它很容易达到一个简单的解决方案，即任一图像压倒另一个图像，即蒙版完全满或空，前景和背景没有有意义地分离。我们展示了 FurryGAN 的三个关键组件：1）将前景图像和合成图像都强加为逼真，2）将掩码设计为粗略和精细掩码的组合，以及 3）通过辅助掩码预测器引导生成器鉴别器。我们的方法使用非常详细的 alpha 蒙版生成逼真的图像，这些蒙版以完全无人监督的方式覆盖头发、毛皮和胡须。
## Aug14 - Aug20, 2022
  - [Vox-Surf：基于体素的隐式表面表示](https://arxiv.org/abs/2208.10925) | [code]
    > 虚拟内容创建和交互在 AR 和 VR 等现代 3D 应用中发挥着重要作用。从真实场景中恢复详细的 3D 模型可以显着扩展其应用范围，并且已经在计算机视觉和计算机图形学界进行了数十年的研究。我们提出了 Vox-Surf，一种基于体素的隐式表面表示。我们的 Vox-Surf 将空间划分为有限的有界体素。每个体素在其角顶点中存储几何和外观信息。由于从体素表示继承而来的稀疏性，Vox-Surf 几乎适用于任何场景，并且可以从多个视图图像中轻松训练。我们利用渐进式训练过程逐步提取重要体素进行进一步优化，从而只保留有效体素，这大大减少了采样点的数量并提高了渲染速度。精细体素也可以视为碰撞检测的边界体积。实验表明，与其他方法相比，Vox-Surf 表示可以以更少的内存和更快的渲染速度学习精细的表面细节和准确的颜色。我们还表明，Vox-Surf 在场景编辑和 AR 应用中可以更实用。
  - [DM-NeRF：2D 图像的 3D 场景几何分解和操作](https://arxiv.org/abs/2208.07227) | [***``[code]``***](https://github.com/vLAR-group/DM-NeRF)
    > 在本文中，我们从 2D 视图研究 3D 场景几何分解和操纵问题。通过利用最近的隐式神经表示技术，特别是吸引人的神经辐射场，我们引入了一个对象场组件，仅从 2D 监督中学习 3D 空间中所有单个对象的唯一代码。该组件的关键是一系列精心设计的损失函数，以使每个 3D 点，尤其是在非占用空间中，即使没有 3D 标签也能得到有效优化。此外，我们引入了一种逆查询算法，可以在学习的场景表示中自由操作任何指定的 3D 对象形状。值得注意的是，我们的操作算法可以明确地解决关键问题，例如对象碰撞和视觉遮挡。我们的方法称为 DM-NeRF，是最早在单个管道中同时重建、分解、操作和渲染复杂 3D 场景的方法之一。在三个数据集上的大量实验清楚地表明，我们的方法可以准确地从 2D 视图中分解所有 3D 对象，允许在 3D 空间中自由操作任何感兴趣的对象，例如平移、旋转、大小调整和变形。
## Aug7 - Aug13, 2022
## Jul31 - Aug6, 2022
  - [VolTeMorph：体积表示的实时、可控和可泛化动画](https://arxiv.org/pdf/2208.00949) | [code]
    > 最近，用于场景重建和新颖视图合成的体积表示越来越受欢迎，这使人们重新关注在高可见度下对体积内容进行动画处理质量和实时性。虽然基于学习函数的隐式变形方法可以产生令人印象深刻的结果，但它们对于艺术家和内容创作者来说是“黑匣子”，它们需要大量的训练数据才能进行有意义的概括，而且它们不会在训练数据之外产生现实的外推。在这项工作中，我们通过引入一种实时、易于使用现成软件进行编辑并且可以令人信服地推断的体积变形方法来解决这些问题。为了展示我们方法的多功能性，我们将其应用于两个场景：基于物理的对象变形和远程呈现，其中化身使用混合形状进行控制。我们还进行了彻底的实验，表明我们的方法优于结合隐式变形的体积方法和基于网格变形的方法。
  - [基于神经辐射场和运动图的可控自由视点视频重建, IEEE Transactions on Visualization and Computer Graphics](https://ieeexplore.ieee.org/abstract/document/9845414) | [code]
    > 在本文中，我们提出了一种基于运动图和神经辐射场（NeRF）的可控高质量自由视点视频生成方法。与现有的姿势驱动 NeRF 或时间/结构条件的 NeRF 工作不同，我们建议首先构建捕获序列的有向运动图。这种序列-运动-参数化策略不仅能够灵活地控制自由视点视频渲染的姿态，而且避免了相似姿态的冗余计算，从而提高了整体重建效率。此外，为了支持身体形状控制而不损失逼真的自由视点渲染性能，我们通过结合显式表面变形和隐式神经场景表示来改进 vanilla NeRF。具体来说，我们为运动图上的每个有效帧训练一个局部表面引导的 NeRF，并且体积渲染仅在真实表面周围的局部空间中执行，从而实现了合理的形状控制能力。据我们所知，我们的方法是第一个同时支持逼真的自由视点视频重建和基于运动图的用户引导运动遍历的方法。结果和比较进一步证明了所提出方法的有效性。
  - [基于神经描述符字段的鲁棒变化检测, IROS2022](https://ieeexplore.ieee.org/abstract/document/9845414) | [code]
    > 推理环境变化的能力对于长时间运行的机器人至关重要。代理应在操作期间捕获更改，以便可以遵循操作以确保工作会话的顺利进行。然而，不同的视角和累积的定位误差使得机器人很容易由于低观察重叠和漂移的对象关联而错误地检测到周围世界的变化。在本文中，基于最近提出的类别级神经描述符字段 (NDF)，我们开发了一种对象级在线变化检测方法，该方法对部分重叠的观察和嘈杂的定位结果具有鲁棒性。利用 NDF 的形状补全能力和 SE(3) 等效性，我们表示具有紧凑形状代码的对象，该代码编码来自部分观察的完整对象形状。然后基于从 NDF 恢复的对象中心将对象组织在空间树结构中，以便快速查询对象邻域。通过形状代码相似性关联对象并比较局部对象-邻居空间布局，我们提出的方法证明了对低观测重叠和定位噪声的鲁棒性。我们对合成序列和真实世界序列进行了实验，与多种基线方法相比，实现了改进的变化检测结果。
## Jul24 - Jul30, 2022
  - [MobileNeRF：利用多边形光栅化管道在移动架构上进行高效的神经场渲染](https://arxiv.org/abs/2208.00277) | [***``[code]``***](https://github.com/google-research/jax3d/tree/main/jax3d/projects/mobilenerf)
    > 神经辐射场 (NeRFs) 展示了从新颖视图合成 3D 场景图像的惊人能力。但是，它们依赖于基于光线行进的专用体积渲染算法，这些算法与广泛部署的 g 的功能不匹配图形硬件。本文介绍了一种基于纹理多边形的新 NeRF 表示，它可以使用标准渲染管道有效地合成新图像。 NeRF 表示为一组多边形，其纹理表示二进制不透明度和特征向量。使用 z 缓冲区对多边形进行传统渲染会生成每个像素都有特征的图像，这些图像由在片段着色器中运行的小型、依赖于视图的 MLP 进行解释，以产生最终的像素颜色。这种方法使 NeRF 能够使用传统的多边形光栅化管道进行渲染，该管道提供大规模的像素级并行性，在包括手机在内的各种计算平台上实现交互式帧速率。
  - [用笼子变形辐射场, ECCV2022](https://arxiv.org/abs/2207.12298) | [code]
    > 辐射场的最新进展可以实现静态或动态 3D 场景的逼真渲染，但仍不支持用于场景操作或动画的显式变形。在本文中，我们提出了一种新的辐射场变形方法：自由形式的辐射场变形。我们使用一个三角形网格来包围称为笼子的前景对象作为界面，通过操纵笼子顶点，我们的方法可以实现辐射场的自由变形。我们方法的核心是网格变形中常用的基于笼的变形。我们提出了一种将其扩展到辐射场的新公式，该公式将采样点的位置和视图方向从变形空间映射到规范空间，从而实现变形场景的渲染。合成数据集和真实世界数据集的变形结果证明了我们方法的有效性。
  - [NeuMesh：学习基于解缠结神经网格的隐式场，用于几何和纹理编辑, ECCV2022(oral)](https://arxiv.org/abs/2207.11911) | [code]
    > 最近，神经隐式渲染技术得到了迅速发展，并在新颖的视图合成和 3D 场景重建中显示出巨大的优势。然而，现有的用于编辑目的的神经渲染方法提供的功能有限，例如，刚性变换，或者不适用于日常生活中一般对象的细粒度编辑。在本文中，我们提出了一种新颖的基于网格的表示，通过在网格顶点上使用解开几何和纹理代码对神经隐场进行编码，这促进了一组编辑功能，包括网格引导的几何编辑、带有纹理交换的指定纹理编辑、填充和绘画操作。为此，我们开发了几种技术包括可学习的符号指标以放大基于网格的表示的空间可区分性，蒸馏和微调机制以实现稳定收敛，以及空间感知优化策略以实现精确的纹理编辑。对真实数据和合成数据的大量实验和编辑示例证明了我们的方法在表示质量和编辑能力方面的优越性。代码可在项目网页上找到：此 https URL。
## Previous weeks
  - [神经稀疏体素场, NeurIPS2020](https://lingjie0206.github.io/papers/NSVF/) | [***``[code]``***](https://github.com/facebookresearch/NSVF)
    > 我们介绍了神经稀疏体素场 (NSVF)，这是一种用于快速和高质量自由视点渲染的新神经场景表示。 NSVF 定义了一组以稀疏体素八叉树组织的体素有界隐式字段，以对每个单元中的局部属性进行建模。 我们仅从一组姿势的 RGB 图像中通过可区分的光线行进操作逐步学习底层体素结构。 使用稀疏体素八叉树结构，可以通过跳过不包含相关场景内容的体素来加速渲染新颖的视图。 我们的方法在推理时比最先进的方法（即 NeRF (Mildenhall et al., 2020)）快 10 倍以上，同时获得更高质量的结果。 此外，通过利用显式稀疏体素表示，我们的方法可以很容易地应用于场景编辑和场景合成。 我们还展示了几个具有挑战性的任务，包括多场景学习、移动人体的自由视点渲染和大规模场景渲染。
  - [CAMPARI：相机感知分解生成神经辐射场](https://arxiv.org/pdf/2103.17269.pdf) | [code]
    > 深度生成模型的巨大进步导致了逼真的图像合成。在取得令人信服的结果的同时，大多数方法都在二维图像域中运行，而忽略了我们世界的三维性质。因此，最近的几项工作提出了具有 3D 感知能力的生成模型，即场景以 3D 建模，然后可微分地渲染到图像平面。这导致了令人印象深刻的 3D 一致性，但纳入这种偏差是有代价的：相机也需要建模。当前的方法假定固定的内在函数和预先定义的相机姿势范围。因此，实际数据通常需要参数调整，如果数据分布不匹配，结果会下降。我们的关键假设是，与图像生成器一起学习相机生成器会导致更原则性的 3D 感知图像合成方法。此外，我们建议将场景分解为背景和前景模型，从而实现更有效和更清晰的场景表示。在从原始的、未定型的图像集合中进行训练时，我们学习了一个 3D 和相机感知的生成模型，它不仅忠实地恢复了图像，而且还忠实地恢复了相机数据分布。在测试时，我们的模型生成的图像可以显式控制相机以及场景的形状和外观。
  - [NeRFactor：未知光照下形状和反射率的神经分解, TOG 2021 (Proc. SIGGRAPH Asia)](https://xiuming.info/projects/nerfactor/) | [code]
    > 我们解决了从由一种未知光照条件照射的物体的多视图图像（及其相机姿势）中恢复物体的形状和空间变化反射率的问题。这使得能够在任意环境照明下渲染对象的新颖视图并编辑对象的材质属性。我们方法的关键，我们称之为神经辐射分解（NeRFactor），是提取神经辐射场（NeRF）的体积几何[Mildenhall et al。 2020] 将对象表示为表面表示，然后在解决空间变化的反射率和环境照明的同时联合细化几何。具体来说，NeRFactor 在没有任何监督的情况下恢复表面法线、光能见度、反照率和双向反射分布函数 (BRDF) 的 3D 神经场，仅使用重新渲染损失、简单的平滑先验和从真实数据中学习的数据驱动的 BRDF 先验-世界BRDF测量。通过显式建模光可见性，NeRFactor 能够从反照率中分离出阴影，并在任意光照条件下合成逼真的软阴影或硬阴影。 NeRFactor 能够恢复令人信服的 3D 模型，用于在合成场景和真实场景的这种具有挑战性且约束不足的捕获设置中进行自由视点重新照明。定性和定量实验表明，NeRFactor 在各种任务中都优于经典和基于深度学习的最新技术。我们的视频、代码和数据可在 people.csail.mit.edu/xiuming/projects/nerfactor/ 上找到。
  - [以对象为中心的神经场景渲染](https://shellguo.com/osf/) | [***``[code]``***](https://shellguo.com/osf/)
    > 我们提出了一种从捕获的对象图像中合成逼真场景的方法。我们的工作建立在神经辐射场 (NeRFs) 之上，它隐含地模拟了场景的体积密度和定向发射的辐射。虽然 NeRF 可以合成逼真的图片，但它们只对静态场景进行建模，并且与特定的成像条件密切相关。这个属性使得 NeRFs 难以泛化到新场景，包括新的光照或对象的新排列。我们建议学习以对象为中心的神经散射函数 (OSF)，而不是像 NeRF 那样学习场景辐射场，这是一种使用与光照和视图相关的神经网络隐式模拟每个对象的光传输的表示。即使物体或灯光移动，这也可以渲染场景，而无需重新训练。结合体积路径跟踪程序，我们的框架能够渲染对象内和对象间的光传输效果，包括遮挡、镜面反射、阴影和间接照明。我们评估了我们的场景合成方法，并表明它可以推广到新的照明条件，产生逼真的、物理上精确的多对象场景渲染。
  - [物体辐射场的无监督发现, ICLR2022](https://arxiv.org/abs/2107.07905) | [code]
    > 我们研究从单个图像推断以对象为中心的场景表示的问题，旨在推导出解释图像形成过程的表示，捕捉场景的 3D 性质，并且在没有监督的情况下学习。由于将复杂的 3D 到 2D 图像形成过程集成到强大的推理方案（如深度网络）中存在根本性挑战，大多数现有的场景分解方法都缺乏这些特征中的一个或多个。在本文中，我们提出了对象辐射场 (uORF) 的无监督发现，将神经 3D 场景表示和渲染的最新进展与深度推理网络相结合，用于无监督 3D 场景分解。在没有注释的多视图 RGB 图像上进行训练，uORF 学习从单个图像分解具有不同纹理背景的复杂场景。我们展示了 uORF 在无监督 3D 场景分割、新视图合成和三个数据集上的场景编辑方面表现良好。
  - [学习用于可编辑场景渲染的对象组合神经辐射场, ICCV2021](https://zju3dv.github.io/object_nerf/) | [***``[code]``***](https://github.com/zju3dv/object_nerf)
    > 隐式神经渲染技术已经显示出用于新视图合成的有希望的结果。然而，现有方法通常将整个场景编码为一个整体，这通常不知道对象身份，并且限制了移动或添加家具等高级编辑任务的能力。在本文中，我们提出了一种新颖的神经场景渲染系统，该系统学习对象组成的神经辐射场，并为集群和真实世界场景生成具有编辑能力的逼真渲染。具体来说，我们设计了一种新颖的双路径架构，其中场景分支对场景几何和外观进行编码，对象分支根据可学习的对象激活码对每个独立对象进行编码。为了在严重混乱的场景中进行训练，我们提出了一种场景引导的训练策略来解决遮挡区域中的 3D 空间模糊性并学习每个对象的清晰边界。大量实验表明，我们的系统不仅在静态场景新视图合成方面取得了有竞争力的性能，而且为对象级编辑产生了逼真的渲染。
  - [编辑条件辐射场, ICCV2021](http://editnerf.csail.mit.edu/) | [***``[code]``***](https://github.com/stevliu/editnerf)
    > 神经辐射场 (NeRF) 是支持高质量视图合成的场景模型，针对每个场景进行了优化。在本文中，我们探索启用用户编辑类别级 NeRF - 也称为条件辐射场 - 在形状类别上训练。具体来说，我们介绍了一种将粗略的 2D 用户涂鸦传播到 3D 空间的方法，以修改局部区域的颜色或形状。首先，我们提出了一个条件辐射场，它结合了新的模块化网络组件，包括一个跨对象实例共享的形状分支。观察同一类别的多个实例，我们的模型在没有任何监督的情况下学习底层部分语义，从而允许将粗略的 2D 用户涂鸦传播到整个 3D 区域（例如，椅子座位）。接下来，我们提出了一种针对特定网络组件的混合网络更新策略，该策略平衡了效率和准确性。在用户交互过程中，我们制定了一个既满足用户约束又保留原始对象结构的优化问题。我们在三个形状数据集上展示了我们在各种编辑任务上的方法，并表明它优于以前的神经编辑方法。最后，我们编辑真实照片的外观和形状，并显示编辑传播到外推的新视图。
  - [使用分层神经表示的可编辑自由视点视频, SIGGRAPH2021](https://jiakai-zhang.github.io/st-nerf/) | [***``[code]``***](https://jiakai-zhang.github.io/st-nerf/#code)
    > 生成自由视点视频对于沉浸式 VR/AR 体验至关重要，但最近的神经学进展仍然缺乏编辑能力来操纵大型动态场景的视觉感知。为了填补这一空白，在本文中，我们提出了第一种仅使用稀疏的 16 个摄像头为大规模动态场景生成可编辑照片般逼真的自由视点视频的方法。我们方法的核心是一种新的分层神经表示，其中包括环境本身的每个动态实体都被制定为称为 ST-NeRF 的时空相干神经分层辐射表示。这种分层表示支持对动态场景的完全感知和真实操作，同时仍支持大范围的自由观看体验。在我们的 ST-NeRF 中，动态实体/层被表示为连续函数，以连续和自监督的方式实现动态实体的位置、变形以及外观的解耦。我们提出了一个场景解析 4D 标签映射跟踪来显式地解开空间信息，以及一个连续变形模块来隐式地解开时间运动。进一步引入了一种对象感知体绘制方案，用于重新组装所有神经层。我们采用了一种新颖的分层损失和运动感知光线采样策略，以实现对具有多个表演者的大型动态场景的有效训练，我们的框架进一步实现了各种编辑功能，即操纵规模和位置，复制或重新定时单个神经层在保持高度真实感的同时创造众多视觉效果。大量实验证明了我们的方法在为动态场景生成高质量、照片般逼真和可编辑的自由视点视频方面的有效性。
